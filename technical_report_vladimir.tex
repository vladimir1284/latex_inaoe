%-------------------------------------------------------------------------
\documentclass[11pt]{article}   % el estilo de documento

\usepackage{elsarticle}             % la clase
\usepackage{times}              % la letra
\usepackage{graphicx}           % para manejar imagenes
\usepackage{subfigure}          % para manejar subfiguras
\usepackage{tabularx}		   % para ajustar el ancho de las columnas
%-------------------------------------------------------------------------
% Se definen los margenes para el documento
\topmargin=0.5in \oddsidemargin=0in \evensidemargin=0in
\textwidth=6.5in \textheight=8.5in
%-------------------------------------------------------------------------


\begin{document}

\title{Efficient algorithm for reduct computation}
\author{Vlad\'imir Rodr\'iguez Diez \and Jos\'e Francisco Mart\'inez Trinidad
\affiliation{Computer Science Department\\National Institute of
Astrophysics, Optics and Electronics\\
Luis Enrique Erro \# 1, Santa Mar\'{\i}a Tonantzintla, Puebla,
72840, M\'{e}xico} \email{\{vladimir.rodriguez,fmartine\}@inaoep.mx}}
\maketitle

%\thispagestyle{empty}

\begin{abstract}
    Rough Set Theory reducts are minimal subsets of attributes preserving the semantics of an 
    information system. The reduct computation problem has exponential complexity regarding the number of 
    attributes in a dataset. Parallel acceleration of efficient algorithms have been made to
    reduce the runtime for large datasets. The development of a new algorithm designed from scratch
    for an efficient parallel implementation must improve the performance of existing alternatives. 
    Throughout our research, most efficient reported algorithm for reduct computation will be 
    implemented in a parallel acceleration fashion for benchmarking. Experiences and difficulties 
    encountered in these experiments will constitute the basis for the development of our resulting algorithm. 
    We expect this new approach would reduce the runtime, and hence make the reduct computation viable for 
    larger datasets than it is today.
\end{abstract}

\textbf{keywords.--} Rough Sets, Dimensionality Reduction, reduct computation, Parallel Acceleration.

\pagebreak 
\tableofcontents
\pagebreak 

%-------------------------------------------------------------------------------
% your earth-shattering contribution

\section{Introduction}
  Rough set theory (RST), proposed by Professor Z. Pawlak in 1982 (\cite{Pawlak81},\cite{Pawlak81-2},
  \cite{Pawlak82},\cite{Pawlak91}), is a relatively new mathematical theory 
  to deal with imperfect knowledge, in particular with vague concepts. Information systems in RST 
  are tables of objects described by some attributes (columns). 
  When data is collected or recorded, every single aspect of the object under study is considered 
  to have a complete representation and to ensure that no potentially useful information is lost.
  As a result, information systems are usually characterized by a large number of attributes,
  degrading the performance of machine learning tools \cite{Parthalain08}.
  One of the main concepts in RST is the notion of reduct, which is a minimal subset of attributes 
  preserving the required classification features \cite{Pawlak91}. A new information system using 
  only those features in a reduct, is a reduced representation of the original data with the same 
  classification quality. 
  However, the main restriction in practical use of RST is that computing all reducts has been proven 
  as an NP-hard problem~\cite{Skowron92}.
  It is therefore of high importance the development of efficient algorithms for reduct computation.
  
  Several attempts to speedup computing of reducts are reported. Many of the presented algorithms are 
  based on some heuristics. Main drawback of this approach is that these algorithm do not necessarily 
  return the complete set of attributes and may obtain super-reducts (non minimal subsets). Another 
  way to speedup computation of reducts is the parallelization \cite{Strakowski08}. There are also 
  interesting alternatives such as the use of a parallel version of genetic algorithms \cite{Wroblewski98}
  and the transformation of reduct computation to the well known problem SAT \cite{Jensen14}.
  
  Testor Theory (TT) was created by Yablonskii and Chegis in the middle of fifties 
  of the last century as a tool for analysis of problems connected with control and 
  diagnosis of faults in circuits. TT can be used for feature selection as shown in~\cite{Ruiz08}
  and \cite{Martinez01}. The relation between the rough set reducts and typical testors from the
  logical combinatorial approach to pattern recognition (TT) is exposed in \cite{Lazo15}. Algorithm for
  typical testors computation:~\cite{Ruiz85},~\cite{Santiesteban03},~\cite{Sanchez07} and~\cite{Lias09},
  may be applied to reduct computation because of the similarity between these two concepts. One strength 
  of our research is that we will be testing, for the first time, these two families of algorithm in the 
  same arena.
  
%\subsection{Main Research Teams in Rough Set Theory}\label{mainGroups}{
%  Here we include an overview of main research teams in RST. Researchers working on TT will be 
%  considered as well due to its close relation to RST reducts.
%  
%\subsubsection{Poland}
%  Andrzej Skowron, professor at the University of Warsaw; and Roman Slowinski from Institute of Computing
%  Science at the Poznan University of Technology were co-authors of professor Zdzislaw Pawlak. Their works
%  constitute the basis of RST and have been expanding its capabilities and applications.
%  
%  Maciej Kopczynski,  Tomasz Grzes and Jaroslaw Stepaniuk from the Faculty of Computer Science at the Bialystok
%  University of Technology work on FPGA-based acceleration of RST algorithm.
%  
%\subsubsection{India}
%  Kanchan Tiwari from E{\&}TC Department at MESCOE, Pune and Ashwin Kothari from the Electronics \& Computer 
%  Sceince Department at VNIT, Nagpur; both from India, have been recently working on the development of FPGA 
%  architectures for reduct computations of informations systems.
%   
%\subsubsection{United Kingdom} 
%  Richard Jensen and Qiang Shen from Department of Computer Science at Aberystwyth University, Wales, United 
%  Kingdom work in the study of non-classic approaches to RST. Most of their works are on Fuzzy-rough sets and 
%  the application of optimization techniques to the computation of reducts in informations systems.
%  
%\subsubsection{Testors Theory} 
%  Jos\'e F. Mart\'inez Trinidad, Jes\'us A. Carrasco Ochoa and Manuel S. Lazo Cort\'es from 
%  Instituto Nacional de Astrof\'isica \'Optica y Electr\'onica (INAOE) work on theoretical aspects of TT 
%  and the development of new algorithms for reduct computation in collaboration with Guillermo Sanchez 
%  Diaz from Universidad Autonoma de San Luis Potosi. They have been also working on hardware implementations
%  of algorithms for reduct computation in collaboration with Ren\'e Cumplido Parra and Claudia Feregrino 
%  Uribe from INAOE.}
  
\subsection{Justification and Motivation}\label{Justification}{
  RST can be used to reduce the number of attributes in a dataset without relevant information lost. 
  Therefore, there has been much research in the area of finding reducts, particularly, reducts with 
  minimal cardinality~\cite{Jensen14}. 
  
  Heuristic methods such as~\cite{Chouchoulas01,Jensen04,Zhong01} are fast alternatives for finding 
  reducts but they do not guarantee minimal reductions. Stochastic approaches~\cite{Wroblewski95,Jensen03,
  Chen10,Wang07} still do not guarantee finding the smallest reducts, as we will see further. Techniques 
  for finding the complete reducts set~\cite{Ruiz85,Santiesteban03,Sanchez07,Lias09} can, of course, find 
  minimal cardinality reducts but with a higher computational effort.
  
  The motivation of this work is the development of an algorithm for finding reducts with minimal length 
  in an information system. This is an NP-Hard problem which makes every attempt for reducing its execution
  time, a challenging task. Our proposal must be competitive with the state of the art algorithm in all 
  cases and will be faster in most datasets. The main arena for comparison will be a large set of synthetic,
  randomly generated datasets and benchmarking datasets from~\cite{Bache13}. We will be dealing only with
  classical reducts definition. The practical applications (or relevance) of shortest reducts in supervised
  classification is beyond our goals.
  
  The results of this research will impact the supervised classification methods specially in large datasets.
  Nowadays, data is automatically collected, thus generating huge databases in almost every field. The 
  current growth of the size of data and the number of existing databases, is the justification for our
  research on efficient algorithms for dimensionality reduction without semantics lost.}  
    
  Throughout our research we will conduct a comparative study between the most relevant algorithms for reduct 
  computation. Although main focus will be on algorithm obtaining minimal cardinality reducts, experiences 
  form heuristics approaches will be considered. The rest of this document is structured as follows. In 
  section~\ref{basicConcepts}, some basic concepts of rough set theory are introduced. A thorough revision of 
  the state of the art in classic reduct computation is presented in section \ref{relatedWork}. Our research
  questions are exposed in section~\ref{ResearchQuestions}; and our research goals in section~\ref{Goals}.
  In section~\ref{DOE} we present the design of experiments to be conducted throughout our research and;
  finally, the publication plan is exposed in section~\ref{PubPlan}.
  

\section{Basic Concepts}\label{basicConcepts}
  RST philosophy is based on the assumption that every object in the universe of discourse is described by 
  some information associated to it. This information constitutes the basis through which classification of 
  objects can be achieved. RST motto is "Let the data speak for themselves". Moreover, RST is a formal 
  framework to deal with imprecise and incomplete data with no need of additional information.
  
  From the RST point of view, two objects are indistinguishable (indiscernible) if they have an equivalent 
  value for each attribute in their description. Indiscernibility relations arising this way constitute the
  mathematical foundations of RST. Any partition of a dataset in which every pair of indiscernible objects
  belongs to the same subdivision is called a crisp (precise) set; otherwise, the set is rough (imprecise, 
  vague). Some basic concepts of RST are presented bellow. Although we will be following the explanation 
  in~\cite{Polkowski00}, some modifications are introduced to provide sufficient basis for the further 
  discussion.
  
\subsection{Information System}
  The basic representation of data in RST is an \emph{Information System} (IS). An IS is a table with rows
  representing objects while columns specify its attributes or features. Formally, an IS can be defined as 
  $IS=(U,A)$ where $U$ is a finite non-empty set of objects $U=\lbrace x_1,x_2,...,x_n\rbrace$ and $A$ is a 
  finite non-empty set
  of attributes (features, variables). Every attribute in $A$ is a map: $a: U \rightarrow V_a$. The set $V_a$ is
  called the \textit{value set} of $A$. Attributes in $A$ are further classified as condition attributes $C$ and 
  decision attributes $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. We can of course define two 
  value sets $V_c$ and $V_d$, for condition and decision attributes respectively, such that 
  $V_a=V_c \cup V_d$. Table~\ref{tab_IS} shows a typical IS.
  
  
 \begin{table}[htb]
		\caption{An Information System.} \label{tab_IS}
		\centering
 	\begin{tabular}{c||c|c||c}
 			  & $c_1$ & $c_2$ &  $d$ \\
 		\hline \hline
		$x_1$ &   1   &    3  &   0   \\
		$x_2$ &   1   &    0  &   0   \\
		$x_3$ &   3   &    1  &   1   \\
		$x_4$ &   3   &    1  &   1   \\
		$x_5$ &   4   &    2  &   1   \\
		$x_6$ &   1   &    2  &   0   \\
		$x_7$ &   4   &    2  &   1   \\
 	\end{tabular}             
 \end{table}
 
   
  \textit{Decision attributes} absolutely decide to which class the object belongs. In the IS of
  table~\ref{tab_IS}, $d$ is the decision attribute column. For this example $V_d = \lbrace 0,1 \rbrace$;
  hence this is a 2 classes system. \textit{Condition attributes} do not absolutely decide the class for 
  an object but help to decide. In supervised classification condition attributes are the only information
  available during classification of new objects while decision attributes are only present in the training set. 
  IS with distinguished decision and condition attributes are called decision tables. In table~\ref{tab_IS},
  $c_1$ and $c_2$ are condition attributes.
 
\subsection{Indiscernibility Relations}
  For an object $x \in U$, the information about $x$ with respect to a set $B \subseteq A$ may be defined as
  \textit{the B-information set} 
  
  \begin{equation}
  	Inf_B(x)=\lbrace (a,a(x)):a \in B \rbrace
  \end{equation}  
  
  of $x$.
  
  \emph{Indiscernibility relation} of $B$ is defined as follows:
  
  \begin{equation}
  	(x,y) \in IND_B \Longleftrightarrow Inf_B(x)=Inf_B(y)
  \end{equation} 
  
  Equivalent classes $[x]_B$ of the relation $IND_B$ represent therefore elementary (atomic) portions
  of knowledge represented by the subsystem $IS_B=(U,B)$.
  
  For a \textit{concept} (set of objects), $X \subseteq U$, we say that $X$ is \textit{B-exact} if and 
  only if
  	
  \begin{equation}
  	X=\cup_{i=1}^{k} [x_i]_B
  \end{equation} 
  
  for some $x_1, x_2,...,x_k \in U$ i.e. where $X$ is the union of some \textit{B-indiscernibility} classes.
  
  For example, in table~\ref{tab_IS} the three possible subset of conditional attributes are 
  $\lbrace c_1 \rbrace$, $\lbrace c_2 \rbrace$ and $\lbrace c_1, c_2 \rbrace$. The indiscernibility relation 
  for these sets defines three partitions of the universe:
  
  $$\begin{array}{lcc}
  IND_{\lbrace c_1 \rbrace} &=& \lbrace \lbrace x_1, x_2, x_6 \rbrace, 
  								\lbrace x_3, x_4 \rbrace, 
  								\lbrace x_5, x_7 \rbrace \rbrace \\
  IND_{\lbrace c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
  								\lbrace x_2 \rbrace, 
  								\lbrace x_3, x_4 \rbrace,
  								\lbrace x_5, x_6, x_7 \rbrace \rbrace \\
  IND_{\lbrace c_1, c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
  									\lbrace x_2 \rbrace, 
  									\lbrace x_3, x_4 \rbrace,
  									\lbrace x_5, x_7 \rbrace,
  									\lbrace x_6 \rbrace \rbrace 
  \end{array}$$

\subsection{Concept Approximations}
  RST capability of handling non-exact (rough) concepts arises from the approximation of a rough concept 
  by means of two crisp concepts: the \textit{lower} and the \textit{upper approximations of} $X$. 
  Denoted by $\underline{B}X$ and $\overline{B}X$, respectively as follows:
  
  \begin{equation}
  	\begin{array}{lcc}
  	\underline{B}X &=& \lbrace x \in U : [x]_B \subseteq X \rbrace\\
  	\overline{B}X  &=& \lbrace x:[x]_B \cap X \neq \emptyset \rbrace
  	\end{array}
  \end{equation}
  
  We would say that objects in $\underline{B}X$ can be certainly classified as elements of $X$ on the 
  basis of knowledge in $IS_B$, while objects in $\overline{B}X$ can only be possibly classified as 
  elements of $X$ on the basis of knowledge in $IS_B$. In other words, $\underline{B}X$ is 
  composed by those objects in $X$ having no indiscernible objects outside $X$. $\overline{B}X$ on the 
  other hand, is composed by all objects in $X$ plus all objects outside $X$ having an indiscernible 
  object in $X$.
  
  The set 
  
  \begin{equation}
  	BN_B(X)=\overline{B}X-\underline{B}X
  \end{equation}
  
  is called the \textit{B-boundary region of X} and it contains the objects which neither are certainly 
  members of $X$ nor they are certainly member of $U-X$. The presence of a non-empty boundary region
  indicates that the concept in question is rough (\textit{B-rough}).
  
  From the previous example we can see that e.g. concept $X=\lbrace x_1,x_2,x_3 \rbrace$ is rough for the 
  three attributes sets while e.g. the concept $Y=\lbrace x_1,x_2 \rbrace$ is both $\lbrace c_1 \rbrace$-
  and $\lbrace c_1, c_2 \rbrace$-exact. Notice that
  
  $$\begin{array}{lcc}
  \underline{c_1}X &=& \emptyset\\
  \overline{c_1}X  &=& \lbrace x_1,x_2,x_3,x_4,x_6 \rbrace\\
  \underline{c_2}X &=& \lbrace x_1,x_2 \rbrace
  \end{array}$$
  
\subsection{Positive Region}\label{subsect_Pos}
  The decision attribute $d$ induces a partition of the universe $U$ into equivalence classes 
  (\textit{decision classes}) of the relation $IND_d$. Each decision class $X_1,X_2,...,X_k$, where 
  $k=|V_d|$, may be approximated by its lower and upper approximations over a set $B \subseteq A$ of 
  attributes. Since we will be trying to associate a decision class to an object, based on the 
  knowledge in attributes belonging to $B$, we are interested in those $B-classes$ $[x]_B$ which 
  satisfy the condition $[x]_B \subseteq X_i$ for some $X_i$. This idea leads to the notion of the 
  \textit{positive region of the decision}.
  
  \begin{equation}
  	POS_B(d) = \lbrace x \in U: \exists i \in \lbrace 1,2,...,k \rbrace : [x]_B \subseteq X_i \rbrace
  \end{equation}
  
  The set $POS_B(d)$ is called the \textit{B-positive region of d}.
  
  Taking for example the IS in table~\ref{tab_IS}, we can see that
  
  $$\begin{array}{lcc}
  POS_{\lbrace c_1 \rbrace}(d)&=&U\\
  POS_{\lbrace c_2 \rbrace}(d)&=& \lbrace x_1,x_2,x_3,x_4 \rbrace\\
  POS_{\lbrace c_1, c_2 \rbrace}(d)&=&U
  \end{array}$$
 
\subsection{Reducts and Core}
  Given an information system $IS=(U,A)$ with condition attributes set $C$ and decision attributes set
  $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. A subset $B \subseteq C$ is a \textit{reduct} 
  of $IS$ relative to $D$ if
  \begin{enumerate}
  	\item $POS_B(D)=POS_C(D)$. \label{cond_1}
  	\item $B$ is a minimal subset (with respect to inclusion) satisfying condition~\ref{cond_1}.
  \end{enumerate}
  
  The intersection of all the reducts in an IS is called the \textit{core}, the elements of which cannot be
  eliminated without introducing more contradictions to the representation of the dataset.
  
\subsection{Discernibility Matrix and Discernibility Function}
  The discernibility knowledge of the information system is commonly recorded in a symmetric $|U| \times |U|$
  matrix called the \textit{discernibility matrix}, and each element $m_{ij}$ in the discernibility matrix 
  $M_{IS}$ is defined as 
  
  \begin{equation}
  	m_{ij}=\left\lbrace\begin{array}{cl}
  			\lbrace c \in C: c(x_i) \neq c(x_j) \rbrace & \mathrm{for~~}D(x_i) \neq D(x_j)\\
  			\emptyset 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation}
  
  Table~\ref{tab_DM} shows the discernibility matrix for the IS in table~\ref{tab_IS} as a lower triangular 
  matrix.
  
   \begin{table}[htb]
		\caption{Discernibility Matrix Example.} \label{tab_DM}
		\centering
 	\begin{tabular}{c|ccccccc}
 		$x \in U$ & 1 & 2 &  3 & 4 & 5 &  6 & 7\\
 		\hline
		1 &&&&&&&\\
		2 &&&&&&&\\
		3 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		4 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		5 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		6 &&& $c_1,c_2$ & $c_1,c_2$ &&&\\
		7 & $c_1,c_2$ & $c_1,c_2$ &&&& $c_1$ &\\
 	\end{tabular}             
 \end{table}
  
  Once the discernibility matrix $M_{IS}$ is found, we can define the \textit{discernibility function} $f_{IS}$.
  This is a boolean function of $n$ boolean variables $c_1^*, c_2^*,...,c_n^*$, representing the presence of
  the corresponding attribute (True) or its absence (False) in $M_{IS}$.
  
  \begin{equation}
  	f_{IS}(c_1^*, c_2^*,...,c_n^*)=\wedge \lbrace \vee c_{ij}^* : 1 \leq j \leq i \leq |U|, 
  									c_{ij} \neq \emptyset \rbrace
  \end{equation}

  where $c_{ij}^*=\lbrace c^* : c \in c_{ij} \rbrace$. Only the lower triangular matrix from $M_{IS}$ is
  taken into consideration since $M_{IS}$ is symmetric. An equivalence between the prime implicants of
  $f_{IS}$ and all the reducts of $IS$ has been found~\cite{Pawlak07}.
  
  The discernibility function for the discernibility matrix in table~\ref{tab_DM} is, without repetitions  
  $f_{IS}(c_1^*,c_2^*)=(c_1^* \vee c_2^*) \wedge c_1^*$
  
  From this we can easily see that the only reduct (also the core) for this IS is $c_1$.
  
\subsection{Attributes Dependency and Significance}
  One important aspect of data analysis is the study of dependencies between attributes describing the 
  objects. Intuitively, a subset of attributes $D$ depends totally on a set of attributes $B$, denoted 
  $B \Rightarrow D$ if all attribute values from $D$ are uniquely determine by values of attributes
  in $B$. Formally in RST we say that for $B,D \subset A$, $D$ depends on $B$ in a degree 
  $k(0 \leq k \leq 1)$, denoted $B \Rightarrow _{k}D$ if
  
  \begin{equation}
  	k=\gamma _B (D)=\frac{|POS_B(D)|}{|U|}
  \end{equation}
    
  This is called the \textit{Positive Dependency Degree}, and it is the ratio of the number of objects belonging
  to the positive region to the number of all objects in universe $U$. If $k=1$, $D$ depends totally on $B$, if
  $0 < k < 1$, $D$ depends partially (in a degree $k$) on $B$ and if $k=0$, $D$ does not depends on $B$.
  
  Taking for example the positive regions from subsection~\ref{subsect_Pos}, we can see that
  
  $$\begin{array}{lcccc}  
  \gamma _{\lbrace c_1 \rbrace} (d)&=&\frac{|U|}{|U|}&=&1\\
  \gamma _{\lbrace c_2 \rbrace}(d)&=& \frac{|\lbrace x_1,x_2,x_3,x_4\rbrace|}{|U|}&=&\frac{4}{7} \\
  \gamma _{\lbrace c_1, c_2 \rbrace}(d)&=&\frac{|U|}{|U|}&=&1
  \end{array}$$
  
  We can express the \textit{significance} of feature $c \in B$ upon $D$ as
  
  \begin{equation}
  	SIG(c,B,D)=\gamma _B (D)-\gamma _{B-\lbrace c \rbrace} (D)
  \end{equation}
  
  From  the previous example:
  
  $$\begin{array}{lcccccc}
  SIG(c_1,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
  										-\gamma _{\lbrace c_2 \rbrace}(d)
  										&=&1-\frac{4}{7}&=&\frac{3}{7}\\
  SIG(c_2,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
  										-\gamma _{\lbrace c_1 \rbrace}(d)
  										&=&1-1&=&0
  \end{array}$$
  
  From this it follows that attribute $c_1$ is indispensable, but attribute $c_2$ can be dispense when 
  considering the dependency between the decision attribute $d$ on condition attributes $c_1$ and $c_2$.
  
  Finally, we would like to say that an alternative definition of reduct from dependency exist. We say 
  that a subset $B \in C$ is a reduct of $C$ if it is a minimal set (with respect to inclusion) satisfying 
  the condition $\gamma _B(D)=\gamma _C(D)$.

\section{Related Work}\label{relatedWork}
  In this section, we will be first discussing heuristic algorithms for reduct computation. Some of these 
  algorithm are capable of finding several reducts and others are intended to obtain a single \textit{minimal} 
  reduct. Then, two kind of algorithm for computing the complete set of reducts will be exposed: those 
  from RST and those from TT. Researches in TT have been focused in algorithm for finding all typical testors.
  Finally, we will make a survey of parallel accelerations reported in literature.  
  
  Figure~\ref{fig_Tax} shows a taxonomy of the reported algorithms for reduct computation. This classification
  corresponds to the sequence that we will be following throughout our survey of the state of the art. 
  Table~\ref{tab_Alg} makes a correspondence between the codename used in figure~\ref{fig_Tax} and the 
  publication reporting the algorithm. Some parallel approaches are implementations of existing sequential 
  algorithm; for those cases, we use a color association in figure~\ref{fig_Tax}.

  \begin{figure}[htb] 
    \centering
    \includegraphics[width=\textwidth]{Reduct_Computation_Algorithms.png}
	\caption{Taxonomy of reduct computation algorithms.}
	\label{fig_Tax}
 \end{figure}
 
 \newcolumntype{L}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 \begin{table}[t]
	\caption{reduct computation algorithms.} \label{tab_Alg}
	\centering
 	\begin{tabular}{l||L{13cm}}
 		\multicolumn{1}{c||}{Codename} &  \multicolumn{1}{c}{Publication} \\
 		\hline \hline
		\textsc{quickreduct} 	&  Rough set-aided keyword reduction for text 
								   categorization~\cite{Chouchoulas01} \\
		YangLiHuang 				&  An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility
				 				   Matrix~\cite{Yang08}\\
		EBR						&  A rough set--aided system for sorting WWW bookmarks~\cite{Jensen01}\\
		RSFSACO					&  A rough set approach to feature selection based on ant colony 
								   optimization~\cite{Chen10}\\
		AntRSAR					&  Finding rough set reducts with ant colony optimization~\cite{Jensen03}\\
		GenRSAR					&  Finding rough set reducts with ant colony optimization~\cite{Jensen03}\\
		BjorvandKomorowski		&  Practical applications of genetic algorithms for efficient reduct 
								   computation~\cite{Bjorvand97}\\
		\hline		   
		BTHW						&  FPGA Based Architecture for Computing Testors~\cite{Rojas07}\\
		BruteForce				&  On the Design and Implementation of a High Performance Configurable
								   Architecture for Testor Identification~\cite{Cumplido06}\\
		TiwariKothariShah		&  FPGA Implementation of a Reduct Generation Algorithm based on Rough 
								   Set Theory~\cite{Tiwari13}\\
		TiwariKothari			&  Architecture and Implementation of Attribute Reduction Algorithm Using 
								   Binary Discernibility Matrix~\cite{Tiwari11}\\
		DM-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\cite{Kopczynski14}\\
		DT-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\cite{Kopczynski14}\\
		SRGonCRS					&  Highly Scalable Rough Set Reducts Generation~\cite{WangP07}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\cite{Jiao10}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\cite{Jiao10}\\
		\hline
		TB \& BT					&  BT and TB algorithms for computing all irreducible testors~\cite{Ruiz85}\\
		CT--EXT					&  CT--EXT: an algorithm for computing typical testor set~\cite{Sanchez07}\\
		LEX						&  LEX: a new algorithm for the calculus of typical
								   testors~\cite{Santiesteban03}\\
		BR						&  BR: A new method for computing all typical testors~\cite{Lias09}\\
		YYC						&  YYC: A Fast Performance Incremental Algorithm for Finding Typical
								   Testors~\cite{Alba14}\\
		RSAR	--SAT				&  Finding rough and fuzzy--rough set reducts with SAT~\cite{Jensen14}\\
		Expansion Algorithm		&  Reduct generation in information systems~\cite{Starzyk99}\\
		GonCRS					&  Highly Scalable Rough Set Reducts Generation~\cite{WangP07}\\	
 	\end{tabular}             
 \end{table}
 
\subsection{Heuristics Approaches}
  The algorithm presented in~\cite{Chouchoulas01} (\textsc{quickreduct}) starts with an empty set of 
  attributes and adds, one each iteration, the attribute having the highest significance. This greedy algorithm 
  uses the maximal dependency criterion for finding one optimal reduct.  
  A similar approach is the Johnson Reducer~\cite{Johnson74}, first introduced in RST in \cite{Ohrn00}.
  This simple greedy algorithm begins with an empty set of attributes evaluating each conditional attribute in the
  discernibility function according to a heuristic measure. In the simplest case, those attributes with highest 
  appearance frequency within the logical context termed clauses, are considered to be more relevant. Works 
  in~\cite{Nguyen97} and~\cite{Wang01} use alternative heuristic functions guiding the search down better paths. 
  Variations of this algorithm~\cite{Wang01} and~\cite{Yang08} use the discernibility matrix instead of the
  discernibility function.
  Algorithm presented in \cite{Zhong01} starts from the core (since it must be contained in every reduct) and
  follows a similar procedure adding selected attributes. This optimization may be impractical for large datasets
  \cite{Jensen14} since the core must be computed a priori.
  
  The work presented in~\cite{Jiao10} improves the efficiency of computing reducts by means of subdivision. The 
  original dataset is broken down into a master-table and several sub-tables that are simpler, more manageable 
  and more solvable. Results are then joined together in order to solve the original dataset. Different variants
  for decomposition of reduct computation problem are discussed and proposed in~\cite{Strakowski08}.
  
  Special attention deserve the approaches using genetic algorithms to discover sub-optimal reducts. Although 
  these algorithms does not guarantee finding optimal reducts, many reducts may be found in a determined time.
  A good point in this approach is the use of the fitness function to guide the search down to a set of 
  reducts with the desired properties. The work reported in~\cite{Wroblewski95} encodes candidates as bit 
  strings with a positional representation of attributes presence in the candidate set. The fitness function
  depends on the number of attributes in the subset, penalizing strings with a large number of bits set. The 
  second optimization parameter is the number of classifiable objects by the given candidate. The reduct should 
  discern between as many objects as possible.
  
  Other evolutionary approaches to reduct computation include Ant Colony Optimization~\cite{Jensen03}
  and~\cite{Chen10}; and Particle Swarm Optimization~\cite{Wang07}.
    
\subsection{Algorithm for Complete Reducts Set computation}
  A method for the generation of all reducts in an Information System is proposed in \cite{Starzyk99,Starzyk00}.
  Although its computational cost is high, this method provides all the reducts by means of manipulations of 
  the clauses in the discernibility function. In addition to standard simplifications laws, the concept of 
  strong compressibility is introduced and applied along with an expansion algorithm.
  
  Although originally intended for computing a single minimal reduct, algorithm proposed in~\cite{Jensen14} may be
  modified in order to obtain all reducts in an Information System. The method introduced in this work reduces
  the problem of finding a reduct from the discernibility function to the SAT problem~\cite{Davis62}. The boolean
  function generated this way is always satisfy since the complete set of attributes is a trivial solution.
  In~\cite{Lin04}, a heuristic is followed to find a short reduct. This first reduct is used to limit the search
  space to attributes combinations with lower cardinality.
  
  A special mention deserves the work presented in~\cite{WangP07}. This approach avoids the computation of the
  discernibility matrix, which is a expensive computational task, and works directly over the information system. 
 
  The first work developed in the reduction of the search space for typical testors~\cite{Ruiz85}, follows 
  the natural order of attributes codified as a binary number. Unnecessary candidates (attributes subset)
  evaluation are avoided taking into account properties of typical testors and results from previous 
  evaluations. Subsequent algorithm~\cite{Santiesteban03},~\cite{Sanchez07} and~\cite{Lias09} propose
  more sophisticated evaluation orders. Recently, a new internal typical testor--finding algorithm was
  proposed~\cite{Alba14}. In our experience, non of this algorithm is the fastest for all dataset.
  
\subsection{Parallel Accelerations}

  A parallel acceleration of the algorithm presented in~\cite{Yang08} for reduct generation from binary
  discernibility matrix was developed in~\cite{Tiwari11,Tiwari12}. This FPGA implementation computes a 
  single reduct. A real application of object identification system by an intelligent robot is presented.
  In~\cite{Tiwari13} a quick algorithm, similar to those presented in~\cite{Chouchoulas01}, is proposed
  and implemented in a hardware fashion. A recent work from this authors~\cite{Tiwari14}, shows a thorough
  survey of FPGA applications in rough sets reduct computation.

  From the Typical Testors theory, several attempts have been made to overcome the problem 
  complexity by means of FPGA implementations of algorithms. In a first work~\cite{Cumplido06}, an 
  FPGA-based brute force approach for computing testors was proposed. This first approach did 
  not take advantage of dataset characteristics to reduce the number of candidates to be tested; 
  thus all $2^n$ combinations of $n$ features have to be tested. Then, in \cite{Rojas07} a hardware 
  architecture of the BT algorithm for computing typical testors was implemented. 
  This algorithm uses a candidate pruning process for avoiding many unnecessary candidate evaluation, 
  reducing the number of verifications of the typical testor condition. These two previous works computed 
  a set of testors on the FPGA device whilst typical condition was evaluated afterwards by the 
  software component in the hosting PC. Thus, in~\cite{Rojas12} a hardware-software platform for 
  computing typical testors that implemented the BT algorithm, as in \cite{Rojas07}, was proposed; but it also 
  included a new module that eliminates most of the non typical testors before transferring them to 
  a host software application for final filtering. 
	
	%TODO estos trabajos de Wroblewski hay q revisarlos bien pq parece falso lo de los GA paralelos
  In~\cite{Wroblewski98}, a parallel variant of the algorithm proposed in~\cite{Wroblewski95} is presented.
  Developments in genetic algorithms are exploited to provide a speedup for the problem of finding reducts.
  This line of thinking brought us an unexplored acceleration idea from~\cite{Jensen14}. We can combine 
  available architectures for SAT solving on FPGA~\cite{Safar07,Kanazawa11} with the transformation
  presented in~\cite{Jensen14} to obtain a new hardware platform for computing all reducts of an information 
  system.
  
  In~\cite{Grzes13,Kopczynski14}, an FPGA application for a single reduct computation is presented. Although
  authors claim that a huge acceleration is achieved, some drawbacks have to be mentioned. Experiments presented 
  in~\cite{Kopczynski14} to validate their results is performed over a small dataset which in our experience 
  does not implies its applicability to larger cases where such acceleration are needed. On the other hand, 
  runtime estimations for FPGA component executions are made by means of a oscilloscope without taking into 
  account communication overhead, which cannot be neglected in the presented dataset.

\section{The Research Questions}\label{ResearchQuestions} 
  Throughout our preliminary work, we noticed that there is no faster algorithm for finding all reducts in 
  all datasets. Different algorithms use different strategies for traversing and pruning the complete search 
  space. These strategies are better suited for some datasets and are time consuming for others. This leads 
  us to our first scientific question:
  
\begin{quote}
  \emph{Is there a correlational relationship between the external properties of the discernibility matrix and the time 
  		complexity of traversing strategies for finding minimal cardinality reducts of an information
  		system?}
\end{quote}
  		
  In this context, external properties are those characteristics that we can extract from the discernibility
  matrix by traversing their cells just one time. Lets take for instance, the minimum and maximum number of
  attributes in a cell, the core or the mean number of attributes per cell. Internal properties require, on 
  the other hand, more complex operations. Internal properties are, for instance, the number of reducts, the
  cardinality of the shortest and largest reducts, etc.
  
  From this research question we formulate the following hypothesis:
  
\begin{quote}  
  \emph{There is a correlational relationship between the external properties of the discernibility matrix and the time 
  		complexity of traversing strategies for finding minimal cardinality reducts of an information
  		system}
\end{quote}
  		
  Several attempts for the decomposition of the original problem of the complete reducts set computation have 
  been made~\cite{Strakowski08,Jiao10,Kopczynski14}. The main disadvantage of the problem decomposition or
  parallelization is the strong dependency between the speed--up of the method used and the particularities of 
  the dataset~\cite{Strakowski08}. This leads us to our second scientific question:
  
\begin{quote}
  \emph{Is there a correlational relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding minimal cardinality reducts of an information
  		system in a parallel environment?}
\end{quote}

  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{There is a correlational relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding minimal cardinality reducts of an information
  		system in a parallel environment}
\end{quote}

  Based on these two scientific questions we can formulate the main hypothesis for our research proposal:
  
\begin{quote}
  \emph{Using some external properties of the discernibility matrix, we can design a parallel algorithm 
  		for computing minimal cardinality reducts of an information system; which is comparable to the 
  		state of the art alternatives in all datasets, and faster in most of them}
\end{quote}  

\section{The Research Goals}\label{Goals} 
  The main goal in our research is the \emph{development of  a parallel algorithm for computing minimal
  cardinality reducts of an information system; which is comparable to the state of the art alternatives 
  in all datasets, and faster in most of them}. This algorithm will use some external properties of the
  discernibility matrix to conveniently select the traversing strategy for the search space and the 
  decomposition method for paralellization.
  
  Our specific goals are:
  \begin{enumerate}
  \item Find a correlational relationship between some external properties of the discernibility matrix and the fastest 
  		traversing strategy for finding shortest reducts.
  		
  		Main tasks for this goal are:
  		\begin{itemize}
  		\item Generate a set of random datasets, systematically covering the space of possible combinations in 
  			  the values of the external properties of discernibility matrices.
  		\item Implement the main traversing strategies reported for the computation of minimal length reducts.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing minimal cardinality reducts using the classifier found in
  			  the previous task.
  		\item Evaluate the proposed algorithm over the synthetic datasets and over benchmarking datasets.
  		\end{itemize}
  	
  \item Find a correlational relationship between some external properties of the discernibility matrix and the fastest 
  		decomposition method for finding shortest reducts.
  		
  		Main tasks for this goal are:
  		\begin{itemize}
  		\item Implement the main decomposition methods reported for the computation of minimal length reducts
  			  in a parallel fashion.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest decomposition method as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  decomposition method for a given dataset; and the rules governing this relation.
  		\item Design a new parallel algorithm for computing minimal cardinality reducts using the classifier 
  			  found in the previous task.
  		\item Evaluate the proposed algorithm over the synthetic datasets and over benchmarking datasets.
  		\end{itemize}
  
  \item Develop a parallel algorithm for computing minimal cardinality reducts of an information system, based 
  		on the two previously designed algorithms.
  		
  		Main tasks for this goal are:
  		\begin{itemize}
  		\item Design a new parallel algorithm for computing minimal cardinality reducts using some external
  		      properties of the discernibility matrix.
  		\item Implement the proposed algorithm using the optimal acceleration platform.
  		\item Evaluate the proposed algorithm over the synthetic datasets and over benchmarking datasets.
  		\end{itemize}
  \end{enumerate}
%
%\section{Design of Experiments}\label{DOE}
%  Throughout our research, we will be conducting four experiments: one experiment for each of the two first
%  specific goals and two experiments for the third one.
%  
%\subsection{First Experiment}\label{exprimet1}
%  \begin{description}
%  \item[Scientific hypothesis] There is a correlational relationship between the external properties of the discernibility 
%  							   matrix and the time complexity of traversing strategies for finding minimal
%  							   cardinality reducts of an information	system.
%  \item[Experiment hypothesis] There can be extracted some rules by means of Rough Sets Theory for explaining
%  							   the relationship between the external properties of discernibility matrices
%  							   with 30 attributes and 100 rows and the fastest traversing strategies for finding
%  							   their minimal cardinality reducts.  
%  \end{description}
%  \paragraph{Experiment Design} 
%	  \begin{description}
%	  	\item[Experimental Unit:] 10000 synthetically generated discernibility matrices with 30 attributes and
%	  							  100 rows.
%	  	\item[Traversing Strategies:] The concepts of Acceptance, Gap, Compatibility~\cite{Lias13} and the 
%	  								  Jhonson's rule~\cite{Ohrn00} are explored individually and combined with 
%	  								  each other. We would like to note that the concept of Acceptance is basic
%	  								  and will be present in every combination although not mentioned.
%	  	\item[Experimental Design:] Factorial design.
%	  \end{description}
%  \paragraph{Independent variables}  
%  	\begin{itemize}
%  	   \item \textbf{Ones Density:} The number of ones in the discernibility matrix divided by its number of
%  	   								 cells.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 1
%  	   			\item[Levels:] steps = 0.067
%  	   		\end{description}
%  	   \item \textbf{MinRow:} The minimum number of ones in a row.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 29
%  	   			\item[Levels:] steps = 1
%  	   		\end{description}
%  	   \item \textbf{MaxRow:} The maximum number of ones in a row.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 29
%  	   			\item[Levels:] steps = 1
%  	   		\end{description}
%  	   \item \textbf{Strategy:} The traversing strategy used for computing the minimal cardinality reducts.
%  	   		\begin{description}
%  	   			\item[Type:] Categorical
%  	   			\item[Levels:] Acceptance, Compatibility, Johnson, Compatibility+Johnson, Gap, Compatibility+Gap,
%  	   						   Gap+Johnson, Compatibility+Gap+Johnson
%  	   		\end{description}
%    \end{itemize}	
%    \paragraph{Cofactors}  
%  	\begin{itemize}
%  	   \item \textbf{StdRow:} The standard deviation of the density of ones in a row for the matrix.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 0.5
%  	   		\end{description}
%  	   \item \textbf{MinCol:} The minimum number of ones in a column.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 100
%  	   		\end{description}
%  	   \item \textbf{MaxCol:} The maximum number of ones in a column.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 100
%  	   		\end{description}
%  	   \item \textbf{StdCol:} The standard deviation of the density of ones in a column for the matrix.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 0.5
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Controlled variable}  
%  	\begin{itemize}
%  	   \item \textbf{Matrix Dimensions:} Dimensions of the synthetically generated discernibility matrices.
%  	   		\begin{description}
%  	   			\item[Type:] (Integer,Integer)
%  	   			\item[Value:] (30,100)
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Dependent variable}  
%  	\begin{itemize}
%  	   \item \textbf{Fastest Strategy:} The fastest traversing strategy used for computing the minimal cardinality
%  	   								    reducts.
%  	   		\begin{description}
%  	   			\item[Type:] Categorical
%  	   			\item[Levels:] Acceptance, Compatibility, Johnson, Compatibility+Johnson, Gap, Compatibility+Gap,
%  	   						   Gap+Johnson, Compatibility+Gap+Johnson
%  	   		\end{description}
%    \end{itemize}
%  
%  \begin{description}
%    \item[Bias Sources] Our main source of bias is the runtime variability due to the dynamic behaviour of
%    					   	operating systems. We will be doing three execution for each treatment and using 
%    					   	the minimum as the runtime estimator. In order to obtain the fastest strategy, we 
%    					   	will use the strategy with the lowest runtime as the base to compare with the rest of 
%    					   	them. A 10\% of this lowest runtime value is used as the minimum difference indicating
%    					   	statistical significance. This process will ensure us a 95\% confidence
%    					   	interval~\cite{Haveraaen01}.
%    					   	For practical purposes, a runtime difference lower than a 10\% is, in our opinion, 
%    					   	meaningless. This procedure will lead to 248 combinations in which we do not have a
%    					   	fastest strategy. Nevertheless, these combinations provide useful information as well.
%    					   	
%    					   	Another source of bias may be the differences in the implementation of these strategies.
%    					   	To prevent this, we will write all the code for this experiment under the same conditions
%    					   	(e.g. programming language, code style, platform, libraries, etc.).  					   	
%    	\item[Proposed Analysis] We are going to use the Rough Sets Theory in order to extract meaningful rules 
%    							 representing the relationship between the external properties of the discernibility 
%  							 matrix and the time complexity of traversing strategies for finding minimal
%  							 cardinality reducts of an information system. A new information system will be
%  							 generated using the independent and controlled variables as condition attributes and
%  							 the fastest strategy(ies) for each matrix as decision attribute. 
%  							 Notice that cofactors do not constitute a bias source since their value
%  							 is used for the rules extraction precess.
%    	\item[Validation Efforts] We are going to evaluate our result over 30 benchmarking datasets~\cite{Bache13}.
%    							 The kind of validity we look for is conclusion validity. 
%    							 For each treatment application, 30 random synthetic matrices will be generated with
%    							 approximately the same external properties. This process ensures us internal validity
%    							 in the fastest strategy identification.
%  \end{description}
%
%
%
%
%\subsection{Second Experiment}\label{exprimet2}
%  \begin{description}
%  \item[Scientific hypothesis] There is a correlational relationship between the external properties of the discernibility
%  							   matrix and the speed--up of the decomposition method used for finding minimal
%  							   cardinality reducts of an information	system in a parallel environment.
%  \item[Experiment hypothesis] There can be extracted some rules by means of Rough Sets Theory for explaining
%  							   the relationship between the external properties of discernibility matrices
%  							   with 30 attributes and 1000 rows and the best decomposition method used for finding
%  							   minimal cardinality reducts of an information	system in a parallel environment.
%  \end{description}
%  \paragraph{Experiment Design} 
%	  \begin{description}
%	  	\item[Experimental unit:] 1000 synthetically generated discernibility matrices with 30 attributes and
%	  							  1000 rows.
%	  	\item[Decomposition Methods:] Horizontal decomposition over the information system (ISD), horizontal
%	  								 decomposition over the discernibility matrix (DMD) and vertical decomposition 
%	  								 over the attributes (AVD)~\cite{Strakowski08}. By horizontal decomposition 
%	  								 we imply a partition of rows and by vertical decomposition, a partition of
%	  								 attributes (columns).
%	  	\item[Experimental Design:] Factorial design.
%	  \end{description}
%  \paragraph{Independent variables}  
%  	\begin{itemize}
%  	   \item \textbf{Ones Density:} The number of ones in the discernibility matrix divided by its number of
%  	   								 cells.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 1
%  	   			\item[Levels:] steps = 0.067
%  	   		\end{description}
%  	   \item \textbf{MinRow:} The minimum number of ones in a row.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 29
%  	   			\item[Levels:] steps = 1
%  	   		\end{description}
%  	   \item \textbf{MaxRow:} The maximum number of ones in a row.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 29
%  	   			\item[Levels:] steps = 1
%  	   		\end{description}
%  	   \item \textbf{Decomposition Method:} The decomposition method used for computing the minimal cardinality
%  	   										reducts in large datasets.
%  	   		\begin{description}
%  	   			\item[Type:] Categorical
%  	   			\item[Levels:] ISD, DMD, AVD
%  	   		\end{description}
%    \end{itemize}	
%    \paragraph{Cofactors}  
%  	\begin{itemize}
%  	   \item \textbf{StdRow:} The standard deviation of the density of ones in a row for the matrix.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 0.5
%  	   		\end{description}
%  	   \item \textbf{MinCol:} The minimum number of ones in a column.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 1000
%  	   		\end{description}
%  	   \item \textbf{MaxCol:} The maximum number of ones in a column.
%  	   		\begin{description}
%  	   			\item[Type:] Integer
%  	   			\item[Range:] 1 -- 1000
%  	   		\end{description}
%  	   \item \textbf{StdCol:} The standard deviation of the density of ones in a column for the matrix.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Range:] 0 -- 0.5
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Controlled variable}  
%  	\begin{itemize}
%  	   \item \textbf{Matrix Dimensions:} Dimensions of the synthetically generated discernibility matrices.
%  	   		\begin{description}
%  	   			\item[Type:] (Integer,Integer)
%  	   			\item[Value:] (30,1000)
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Dependent variable}  
%  	\begin{itemize}
%  	   \item \textbf{Best Decomposition Method:} The best decomposition method used for computing the minimal
%  	   											cardinality	reducts in large datasets.
%  	   		\begin{description}
%  	   			\item[Type:] Categorical
%  	   			\item[Levels:] ISD, DMD, AVD
%  	   		\end{description}
%    \end{itemize}
%    
%    Notice that when we talk about the best decomposition method, we refer to the method achieving the largest
%    speed--up over the sequential algorithm. This way, we are going to be measuring the runtime as we proposed 
%    in the first experiment. Thus, the bias sources, the proposed analysis and the validation efforts are the 
%    same as in the first experiment.
%
%\subsection{Third Experiment}\label{exprimet3}
%  \begin{description}
%  \item[Scientific hypothesis] Using some external properties of the discernibility matrix, we can design a
%  							   parallel algorithm for computing minimal cardinality reducts of an information
%  							   system; which is comparable to the state of the art alternatives in all datasets,
%  							   and faster in most of them.
%  \item[Null hypothesis] There is no statistically significant improvement in using our proposed
%  						 algorithm instead of the state of the art alternatives.
%  \end{description}
%  \paragraph{Experiment Design} 
%	  \begin{description}
%	  	\item[Experimental unit:] 30 synthetically randomly generated discernibility matrices with 30 attributes
%	  							  and 1000 rows.
%	  	\item[Treatments:] The best performing alternatives from the two previous experiments and our external 
%	  					   properties aware proposed algorithm.
%	  	\item[Statistical Test:] One sided t--test. Notice that we want to evaluate the significance of the
%	  							 runtime reduction achieved by our proposed algorithm.
%	  \end{description}
%  \paragraph{Independent variable}  
%  	\begin{itemize}
%  		\item \textbf{Algorithm:} The algorithm for computing the minimal cardinality reducts in large datasets.
%  	   		\begin{description}
%  	   			\item[Type:] Categorical
%  	   			\item[Levels:] The best performing alternatives from the two previous experiments and our external 
%	  					   	   properties aware proposed algorithm.
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Controlled variable}  
%  	\begin{itemize}
%  	   \item \textbf{Matrix Dimensions:} Dimensions of the synthetically generated discernibility matrices.
%  	   		\begin{description}
%  	   			\item[Type:] (Integer,Integer)
%  	   			\item[Value:] (30,1000)
%  	   		\end{description}
%    \end{itemize}
%    \paragraph{Dependent variable}  
%  	\begin{itemize}
%  	   \item \textbf{Runtime:} The algorithm execution runtime.
%  	   		\begin{description}
%  	   			\item[Type:] Real
%  	   			\item[Unit:] seconds
%  	   		\end{description}
%    \end{itemize}
%    \begin{description}
%    \item[Bias Sources] Our main source of bias is the runtime variability due to the dynamic behaviour of
%    					   	operating systems. We will be doing five execution for each treatment and using 
%    					   	the minimum as the runtime estimator. This process will ensure us a 99\% confidence
%    					   	interval~\cite{Haveraaen01} in the runtime estimation.			   	
%    	\item[Proposed Analysis] We are going to make a one sided t--test comparing our proposed algorithm against
%    							every alternative.
%    	\item[Validation Efforts] Same as in the first experiment but we also look for statistical conclusion
%    							 validity since we will have a confidence interval of our conclusion.
%  \end{description}
% 
% \subsection{Fourth Experiment}\label{exprimet4}
%  This experiment is similar to experiment in~\ref{exprimet3} but we will be evaluating our algorithm over 
%  randomly selected large benchmarking datasets from \cite{Bache13}. All the experiment elements are the
%  same as in experiment in~\ref{exprimet3} except for those stated bellow.
%  \paragraph{Experiment Design} 
%	  \begin{description}
%	  	\item[Experimental unit:] 30 randomly selected large benchmarking datasets from \cite{Bache13}.
%	  	\item[Bias Sources] This time, the matrices dimensions is another source of bias since we cannot 
%	  						control them. The only resource we can use to control this bias source is the
%	  						randomization of treatments application order.
%	  \end{description}
%	  	
\section{Publication Plan}\label{PubPlan}
  In this section we present our publication plan. Results from this research are to be submitted indistinctly 
  to the following journals or congresses, according to the advisors recommendations.
  
  Main journals:
  \begin{itemize}
 	\item Information Sciences\footnote{http://www.journals.elsevier.com/information-sciences/}. IF: 3.893
 	\item Expert Systems with 
 		  Applications\footnote{http://www.journals.elsevier.com/expert-systems-with-applications/}. IF: 1.965
 	\item IEEE Transactions on Knowledge and Data
 		  Engineering\footnote{http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69}. IF: 1.815
 	\item International Journal of Advanced Computer Science and
 		  Applications\footnote{http://thesai.org/Publications/IJACSA}. IF: 1.32
  \end{itemize}
 
  Main congresses:
  \begin{itemize}
 	\item Rough Sets and Current Trends in Computing
 	\item Rough Sets and Intelligent Systems Paradigms
 	\item CIARP: Iberoamerican Congress on Pattern Recognition
 	\item Mexican Conference on Pattern Recognition
  \end{itemize}
  
  In table~\ref{tab_PP} we present the estimated dates for our proposed publications. The first publication will
  discuss the results of the first experiment. In our second publication we will make a review of the state of 
  the art in rough sets reduct computation. Third and fourth publications will present the results from the last
  three experiments as shown in table~\ref{tab_PP}.
  
     \begin{table}[htb]
		\caption{Publication plan.} \label{tab_PP}
		\centering
 	\begin{tabular}{c||l|l}
 		\# & \multicolumn{1}{c|}{Date} & \multicolumn{1}{c}{Results}\\
 		\hline \hline
		1 & July 2015 & First Experiment (\ref{exprimet1})\\
		2 & December 2015 & State of the art survey \\
		3 & July 2016 & Second Experiment (\ref{exprimet2})\\
		4 & July 2017 & Third and fourth Experiments (\ref{exprimet3} \& \ref{exprimet4})\\
 	\end{tabular}             
 	\end{table}

%-------------------------------------------------------------------------------
% Bibliography
%-------------------------------------------------------------------------------
\newpage 
\begin{thebibliography}{}

\bibitem{Alba14}
	Alba--Cabrera, E., Ibarra--Fiallo, J., Godoy--Calderon, S., Cervantes--Alonso, F.:
 	YYC: A Fast Performance Incremental Algorithm for Finding Typical Testors. 
 	In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications.
 	(2014) 416--423.

\bibitem{Bache13}
	Bache K., Lichman M.:  
	UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. 
	Irvine, CA: University of California, School of Information and Computer Science. (2013).

\bibitem{Bjorvand97}	
	Bjorvand, A. T., Komorowski, J.:
	Practical applications of genetic algorithms for efficient reduct computation. 
	Wissenschaft \& Technik Verlag. (1997) 4 601--606.
	
\bibitem{Chen10}
	Chen Y., Duoqian M., Ruizhi W.:
	A rough set approach to feature selection based on ant colony optimization.
	Pattern Recognition Letters. 31(3) (2010) 226--233.

\bibitem{Chouchoulas01}
	Chouchoulas A., Shen Q.:
	Rough set-aided keyword reduction for text categorization.
	Applied Artificial Intelligence. 15(9) (2001) 843--873.
	
\bibitem{Cumplido06} 
	Cumplido R., Carrasco A., Feregrino C.:
	On the Design and Implementation of a High Performance Configurable Architecture for Testor Identification.
	Lectures Notes on Computer Science. 4225 (2006) 665--673.
	
\bibitem{Davis62} 
	Davis M., Logemann G., Loveland D.:
	A machine program for theorem proving.
	Communication of the ACM. 5 (1962) 394--397.

\bibitem {Grzes13}	
	Grzes T., Kopczynski M., Stepaniuk J.:
	FPGA in Rough Set Based Core and Reduct Computation. 
	Rough Sets and Knowledge Technology.	(2013) 263--270.

\bibitem {Haveraaen01}
	Haveraaen M., Informatikk I.,  Bergen U.:
	Some Statistical Performance Estimation Techniques for Dynamic Machines. 
	Norsk Informatikkonferanse. (2001) 176--185.

\bibitem {Hsieh05}
	Hsieh, H. F., Shannon S. E.:
	Three approaches to qualitative content analysis.
	Qualitative health research. 15(9) (2005) 1277--1288.

\bibitem {Jensen01}
	Jensen R., Shen Q.:
	A Rough Set-Aided System for Sorting WWW Bookmarks. 
	In N. Zhong et al. (Eds.), Web Intelligence: Research and Development. (2001) 95--105.	
	
\bibitem {Jensen03}
	Jensen R., Shen Q.:
	Finding rough set reducts with ant colony optimization.
	Proceedings of the 2003 UK workshop on computational intelligence. 1(2) (2003).

\bibitem {Jensen04}
	Jensen R., Shen Q.:
	Semantics-preserving dimensionality reduction: rough and fuzzy-rough based approaches.
	IEEE Transactions on Knowledge and Data Engineering. 16(12) (2004) 1457--1471.
	
\bibitem {Jensen14}
	Jensen R., Andrew T., Shen Q.:
	Finding rough and fuzzy--rough set reducts with SAT.
	Information Sciences. 255 (2014) 100--120.
	
\bibitem {Jiao10}	
	Jiao N., Miao D., Zhou J.:
 	Two novel feature selection methods based on decomposition and composition.
 	Expert Systems with Applications, 37(12) (2010) 741--7426.
 	
\bibitem {Johnson74}	
	Johnson D. S.:
	Approximation algorithms for combinatorial problems. 
	Journal of Computer and System Sciences. 9 (1974) 256--278.

\bibitem {Kanazawa11}	
	Kanazawa K., Maruyama T.
	An FPGA solver for SAT-encoded formal verification problems. 
	Proceedings - 21st International Conference on Field Programmable Logic and Applications. (2011) 38--43.

\bibitem {Kopczynski14}	
	Kopczynski M., Grzes T., Stepaniuk J.:
	FPGA in Rough--Granular Computing : Reduct Generation.
	ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technologies. (2014) 364--370. 

\bibitem {Lazo15}
	Lazo, M. S., Martnez J. F., Carrasco J. A., Sanchez G.:
	On the relation between rough set reducts and typical testors.
	Information Sciences. 294 (2015) 152--163.

\bibitem {Lias09}	
	 Lias A., Pons A.:
	 BR: A new method for computing all typical testors.
	 Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 
	 Springer Berlin Heidelberg. (2009) 433--440.

\bibitem {Lias13}	
	 Lias A., Sanchez G.:
	 An algorithm for computing typical testors based on elimination of gaps and reduction of columns.
	 International Journal of Pattern Recognition and Artificial Intelligence. 
	 Springer Berlin Heidelberg. 27(8) (2013).

\bibitem {Lin04}
	Lin T. Y., Yin, P.:
	Heuristically Fast Finding of the Shortest Reducts. 
	In Rough Sets and Current Trends in Computing(2004) 465--470.

\bibitem{Martinez01}
	Mart\'inez J. F., Guzm\'an A.:
	The Logical Combinatorial Approach to Pattern Recognition an Overview through Selected Works. 
	Pattern Recognition. 34 (2001) 741--751.

\bibitem{Nguyen97}	
	Nguyen H. S., Skowron A.:
	Boolean reasoning for feature extraction problems.
	Foundations of Intelligent Systems. Springer Berlin Heidelberg, 1997. 117--126.	
	
\bibitem{Ohrn00}
	$\emptyset$hrn A.:
	Discernibility and rough sets in medicine: tools and applications. (2000).	
	 
\bibitem {Parthalain08}
	Parthalin, N. M., Jensen, R., Shen Q.:
	Finding Fuzzy-Rough Reducts with Fuzzy Entropy.
	IEEE International Conference on Fuzzy Systems. (2008) 1282--1288.
	
\bibitem {Pawlak81}
	Pawlak, Z.: 
	Classification of Objects by Means of Attributes.
	Reports, Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland. 429 (1981).

\bibitem {Pawlak81-2}
	Pawlak, Z.:
	Rough Relations.
	Reports, Institute of Computer Science, Polish Academy of Sciences, Warsaw, Poland. 429 (1981).

\bibitem {Pawlak82}
	Pawlak, Z.: 
	Rough sets.
	International Journal of Computer and Information Sciences. 11 (1982) 341--356.

\bibitem {Pawlak91}
	Pawlak, Z.: 
	Rough Sets: Theoretical Aspects of Reasoning about Data.
	System Theory, Knowledge Engineering and Problem Solving. 
	Kluwer Academic Publishers, Dordrecht. 9 (1991).
	
\bibitem {Pawlak07}
	Pawlak, Z. Skowron A.: 
	Rough Sets and Boolean Reasoning.
	Information Sciences. 177(1) (2007) 41--73.
	
\bibitem {Polkowski00}	
	Polkowski L., Tsumoto S., Lin T. Y.: 
	Rough Set Methods and Applications. 
	New Developments in Knowledge Discovery in Information Systems.
	PhysicaVerlag, Heidelberg. (2000).

\bibitem{Rojas07}
	Rojas A., Cumplido R., Carrasco J. A., Feregrino C., Mart\'inez J. f.:
	FPGA Based Architecture for Computing Testors. 
	Lectures Notes on Computer Science. 4881 (2007) 188--197.

\bibitem{Rojas12}	
	Rojas A., Cumplido R., Carrasco J. A., Feregrino C., Mart\'inez J. f.:
	Hardware-software platform for computing irreducible testors. 
	Expert Systems with Applications. 39 (2012) 2203--2210.

\bibitem{Ruiz85}
	Ruiz J., Aguila L., Bravo A.:
	BT and TB algorithms for computing all irreducible testors. 
	Revista Ciencias Matem\'aticas. 2 (1985) 11--18.
	
\bibitem{Ruiz08}
	Ruiz, J.:
 	Pattern recognition with mixed and incomplete data. 
 	Pattern Recognition and Image Analysis. 18(4) (2008) 563--576.
 	
\bibitem{Safar07}
	Safar M., El-Kharashi M. W., Salem A.:
	FPGA-based SAT solver. 
	Canadian Conference on Electrical and Computer Engineering. (2007) 1901--1904. 
	
\bibitem{Sanchez07}
 	Sanchez G., Lazo M.:
 	CT--EXT: an algorithm for computing typical testor set.
 	Progress in Pattern Recognition, Image Analysis and Applications. 
 	Springer Berlin Heidelberg. (2007) 506--514.
 
\bibitem{Santiesteban03}
	Santiesteban Y., Pons A.:
	LEX: a new algorithm for the calculus of typical testors.
	Mathematics Sciences Journal. 21(1) (2003) 85--95.
	
\bibitem {Skowron92}
	Skowron, A., Rauszer, C.:
	The discernibility matrices and functions in information systems. 
	Handbook of Applications and Advances of the Rough Sets Theory. (1992) 331--362.

\bibitem {Starzyk99}
	Starzyk J., Nelson D. E., Sturtz K.:
	Reduct generation in information systems.
	Bulletin of international rough set society. 3(1/2) (1999) 19--22.

\bibitem {Starzyk00}	
	Starzyk J., Dale E. N., Sturtz K.:
	A mathematical foundation for improved reduct generation in information systems.
	Knowledge and Information Systems. 2(2) (2000) 131--146.

\bibitem {Strakowski08}	
	Strakowski T.,  Rybi\'nski H.:
	A new approach to distributed algorithms for reduct calculation.
	Transactions on Rough Sets IX. Springer Berlin Heidelberg. (2008) 365--378.

\bibitem {Tiwari11}	
	Tiwari K., Kothari A.:
	Architecture and Implementation of Attribute Reduction Algorithm Using Binary Discernibility Matrix.
	International Conference on Computational Intelligence and Communication Networks. (2011) 212--216.

\bibitem {Tiwari12}	
	Tiwari K., Kothari A., Keskar A.  
	Reduct generation from binary discernibility matrix: an hardware approach. 
	International Journal of Future Computer. 1(3) (2012) 270--272.

\bibitem {Tiwari13}	
	Tiwari K., Kothari A., Shah, R.:
	FPGA Implementation of a Reduct Generation Algorithm based on Rough Set Theory. 
	International Journal of Advanced Electrical and Electronics Engineering. 2(6) (2013) 55--61.

\bibitem {Tiwari14}	
	Tiwari K., Kothari A.:
	Design and implementation of Rough Set Algorithms on FPGA: A Survey.
	International Journal of Advanced Research in Artificial Intelligence. 3(9) (2014) 14--23.
		
\bibitem {Wang01}
	Wang J., Wang J.:
	Reduction algorithms based on discernibility matrix: the ordered attributes method.
	Journal of computer science and technology 16(6) (2001) 489--504.
	
\bibitem {WangP07}	
	Wang, P. C.:
	Highly Scalable Rough Set Reducts Generation. 
	Journal of Information Science and Engineering. (2007) 4(23), 1281--1298.

\bibitem {Wang07}
	Wang X., Yang J., Teng X., Xia W., Jensen R.:
	Feature selection based on rough sets and particle swarm optimization.
	Pattern Recognition Letters. 28(4) (2007) 459--471.
	
\bibitem {Wroblewski95}
	Wr\'oblewski, J.: 
	Finding minimal reducts using genetic algorithms.
	Proceedings of the second annual join conference on infromation science. (1995) 186--189.

\bibitem {Wroblewski98}	
	Wr\'oblewski, J.:
	A parallel algorithm for knowledge discovery system.
	PARELEC. (1998) 228--30.

\bibitem {Yang08}		
	Yang P., Jisheng L., Yongxuan H.:
	An attribute reduction algorithm by rough set based on binary discernibility matrix.
	Fifth International Conference on Fuzzy Systems and Knowledge Discovery. 2 (2008) 276--280.
	
\bibitem {Zhong01}	
	Zhong N., Dong J., Ohsuga S.:
	Using rough sets with heuristics for feature selection.
	Journal of Intelligent Information Systems. 16(3) (2001) 199--214.


			
%\bibitem {Asa98}
%   Asada, M., Stone, P., Kitano, H., Werger, B., Kuniyoshi, Y., Drogoul, A., Duhaut, D., Veloso, M.:
%   The RoboCup Physical Agent Challenge: Phase-I.
%   Applied Artificial Intelligence. 12 (1998) 251--263.
%
%\bibitem {Bro86}
%   Brooks, R. A.:
%   A Robust Layered Control System for a Mobile Robot.
%   IEEE Journal of Robotics and Automation. RA-2 (1986) 14--23.
%
%\bibitem {Qui93}
%   Quinlan, J. R.:
%   C4.5: Programs for Machine Learning.
%   Morgan Kaufmann, San Mateo, C.A. (1993).

\end{thebibliography}


\end{document}

====================== END FILE: latex8.tex ======================
