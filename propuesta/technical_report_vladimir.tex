%-------------------------------------------------------------------------
%\documentclass[11pt,authoryear]{elsarticle}
\documentclass[authoryear,11pt]{elsarticle}

\setlength{\parskip}{1em}			% espaciar parrafos
\usepackage{hyperref}				% enlaces en el pdf
\hypersetup{backref,colorlinks=true}	% colores en vez de cajas en los enlaces
\usepackage{times}              		% la letra
\usepackage{graphicx}           		% para manejar imagenes
\usepackage{subfigure}          		% para manejar subfiguras
\usepackage{tabularx}		   		% para ajustar el ancho de las columnas
\usepackage[margin=2.5cm]{geometry}	% Change margins
%-------------------------------------------------------------------------
% Se definen los margenes para el documento
%\evensidemargin=0in
%\topmargin=0.5in \oddsidemargin=0in \evensidemargin=0in
%\textwidth=6.5in \textheight=8.5in
%-------------------------------------------------------------------------


\begin{document}
\begin{frontmatter}
	
	\title{Development of efficient algorithms for reduct computation}
	
	\author{Vlad\'imir Rodr\'iguez Diez}
	\author{Jos\'e Francisco Mart\'inez Trinidad}
	
	\address{Computer Science Department\\National Institute of
	Astrophysics, Optics and Electronics\\
	Luis Enrique Erro \# 1, Santa Mar\'{\i}a Tonantzintla, Puebla,
	72840, M\'{e}xico} 
	%\email{\{vladimir.rodriguez,fmartine\}@inaoep.mx}
	
	%\maketitle
	
	%\thispagestyle{empty}
	
	\begin{abstract}
	    Rough Set Theory reducts are minimal subsets of attributes preserving the semantics of an 
	    information system. The reduct computation problem has exponential complexity regarding the number of 
	    attributes in a dataset. The development of new efficient algorithms, which adapt to the external
	    properties of the dataset, must improve the performance of existing ones. 
	    Throughout our research, most efficient reported algorithm for reduct computation will be 
	    implemented in an homogeneous environment for benchmarking. Experiences and difficulties 
	    encountered in these experiments will constitute the basis for the development of our resulting algorithms. 
	    We will develop algorithms in two directions, the computation of all reducts and the computation of
	    globally shortest reducts.
	    We expect this new approach would reduce the runtime, and hence make the reduct computation viable for 
	    larger datasets than it is today.
	\end{abstract}
	
	\begin{keyword}
		Rough Sets\sep Dimensionality Reduction\sep Reduct Computation.
	\end{keyword}
	
\end{frontmatter}

\pagebreak 
\tableofcontents
\pagebreak 

%-------------------------------------------------------------------------------
% your earth-shattering contribution

\section{Introduction}
  Rough set theory (RST), proposed by Z. Pawlak in 1982 \citep{Pawlak81,Pawlak81-2,Pawlak82,Pawlak91}, 
  is a relatively new mathematical theory 
  to deal with imperfect knowledge, in particular with vague concepts. Information systems in RST 
  are tables of objects described by some attributes (columns). 
  When data is collected or recorded, every single aspect of the object under study is considered 
  to have a complete representation and to ensure that no potentially useful information is lost.
  As a result, information systems are usually characterized by a large number of attributes,
  degrading the performance of machine learning tools~\citep{Parthalain08}.
  One of the main concepts in RST is the notion of reduct, which is a minimal subset of attributes 
  preserving the required classification capabilities~\citep{Pawlak91}. A new information system using 
  only those features in a reduct, is a reduced representation of the original data with the same 
  classification quality. 
  However, the main restriction in practical use of RST is that computing all reducts has been proven 
  as an NP-hard problem~\citep{Skowron92}.
  It is therefore of high importance the development of efficient algorithms for reduct computation.
  
  Several attempts to speedup computing of reducts are reported. Many of the presented algorithms are 
  based on some heuristics. Main drawback of this approach is that these algorithm do not necessarily 
  return the complete set of attributes and may obtain super-reducts (non minimal subsets). Another 
  way to speedup computation of reducts is the parallelization~\citep{Strakowski08}. There are also 
  interesting alternatives such as the use of a parallel version of genetic algorithms \citep{Wroblewski98}
  and the transformation of reduct computation to the well known problem SAT~\citep{Jensen14}.
  
  Testor Theory was created by Yablonskii and Chegis in the middle of fifties 
  of the last century as a tool for analysis of problems connected with control and 
  diagnosis of faults in circuits. Testor Theory can be used for feature selection as shown in~\citep{Ruiz08}
  and~\citep{Martinez01}. The relation between the rough set reducts and the typical testors (TT) from the
  logical combinatorial approach to pattern recognition is exposed in~\citep{Lazo15}. Algorithm for
  typical testors computation:~\citep{Ruiz85},~\citep{Santiesteban03},~\citep{Sanchez07} and~\citep{Lias09},
  may be applied to reduct computation because of the similarity between these two concepts. One strength 
  of our research is that we will be testing, for the first time, these two families of algorithm in the 
  same arena.
     
  Throughout our research we will conduct a comparative study between the most relevant algorithms for reduct 
  computation. Although main focus will be on algorithm obtaining all the reducts and globally shortest reducts,
  experiences form heuristics approaches will be considered as well. We will be exploring the relationship 
  between algorithms performance and external properties of datasets. Another direction in our research is
  the evaluation of dynamically changing the algorithm strategy throughout the execution. Finally best
  performing algorithms will be redesign and implemented in a hardware fashion in order to improve their
  efficiency.  

\section{Basic Concepts}\label{basicConcepts}
%TODO hablar de la matriz basica
  RST philosophy is based on the assumption that every object in the universe of discourse is described by 
  some information associated to it. This information constitutes the basis through which classification of 
  objects can be achieved. RST motto is \textit{Let the data speak for themselves}. Moreover, RST is a formal 
  framework to deal with imprecise and incomplete data with no need of additional information.
  
  From the RST point of view, two objects are indistinguishable (indiscernible) if they have an equivalent 
  value for each attribute in their description. Indiscernibility relations arising this way constitute the
  mathematical foundations of RST. Any partition of a dataset in which every pair of indiscernible objects
  belongs to the same subdivision is called a crisp (precise) set; otherwise, the set is rough (imprecise, 
  vague). Some basic concepts of RST are presented bellow. Although we will be following the explanation 
  in~\citep{Polkowski00}, some modifications are introduced to provide sufficient basis for the further 
  discussion.
  
\subsection{Information System}
  The basic representation of data in RST is an \emph{Information System} (IS). An IS is a table with rows
  representing objects while columns specify its attributes or features. Formally, an IS can be defined as 
  $IS=(U,A)$ where $U$ is a finite non-empty set of objects $U=\lbrace x_1,x_2,...,x_n\rbrace$ and $A$ is a 
  finite non-empty set
  of attributes (features, variables). Every attribute in $A$ is a map: $a: U \rightarrow V_a$. The set $V_a$ is
  called the \textit{value set} of $A$. Attributes in $A$ are further classified as condition attributes $C$ and 
  decision attributes $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. 
%  We can of course define two value sets $V_c$ and $V_d$, for condition and decision attributes respectively, 
%  such that $V_a=V_c \cup V_d$. 
  Table~\ref{tab_IS} shows a typical IS.
  
  
 \begin{table}[htb]
		\caption{An Information System.} \label{tab_IS}
		\centering
 	\begin{tabular}{c||c|c||c}
 			  & $c_1$ & $c_2$ &  $d$ \\
 		\hline \hline
		$x_1$ &   1   &    3  &   0   \\
		$x_2$ &   1   &    0  &   0   \\
		$x_3$ &   3   &    1  &   1   \\
		$x_4$ &   3   &    1  &   1   \\
		$x_5$ &   4   &    2  &   1   \\
		$x_6$ &   1   &    2  &   0   \\
		$x_7$ &   4   &    2  &   1   \\
 	\end{tabular}             
 \end{table}
 
   
  \textit{Decision attributes} absolutely decide to which class the object belongs. In the IS of
  table~\ref{tab_IS}, $d$ is the decision attribute. 
  %For this example $V_d = \lbrace 0,1 \rbrace$; hence 
  This is a 2 classes system. \textit{Condition attributes} do not absolutely decide the class for 
  an object but help to decide. In supervised classification, condition attributes are the only information
  available during classification of new objects; while, decision attributes are only present in the training set. 
  IS with distinguished decision and condition attributes are called decision tables. In table~\ref{tab_IS},
  $c_1$ and $c_2$ are condition attributes.
 
%\subsection{Indiscernibility Relations}
%  For an object $x \in U$, the information about $x$ with respect to a set $B \subseteq A$ may be defined as
%  \textit{the B-information set} 
%  
%  \begin{equation}
%  	Inf_B(x)=\lbrace (a,a(x)):a \in B \rbrace
%  \end{equation}  
%  
%  of $x$.
%  
%  \emph{Indiscernibility relation} of $B$ is defined as follows:
%  
%  \begin{equation}
%  	(x,y) \in IND_B \Longleftrightarrow Inf_B(x)=Inf_B(y)
%  \end{equation} 
%  
%  Equivalent classes $[x]_B$ of the relation $IND_B$ represent therefore elementary (atomic) portions
%  of knowledge represented by the subsystem $IS_B=(U,B)$.
%  
%  For a \textit{concept} (set of objects), $X \subseteq U$, we say that $X$ is \textit{B-exact} if and 
%  only if
%  	
%  \begin{equation}
%  	X=\cup_{i=1}^{k} [x_i]_B
%  \end{equation} 
%  
%  for some $x_1, x_2,...,x_k \in U$ i.e. where $X$ is the union of some \textit{B-indiscernibility} classes.
%  
%  For example, in table~\ref{tab_IS} the three possible subset of conditional attributes are 
%  $\lbrace c_1 \rbrace$, $\lbrace c_2 \rbrace$ and $\lbrace c_1, c_2 \rbrace$. The indiscernibility relation 
%  for these sets defines three partitions of the universe:
%  
%  $$\begin{array}{lcc}
%  IND_{\lbrace c_1 \rbrace} &=& \lbrace \lbrace x_1, x_2, x_6 \rbrace, 
%  								\lbrace x_3, x_4 \rbrace, 
%  								\lbrace x_5, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
%  								\lbrace x_2 \rbrace, 
%  								\lbrace x_3, x_4 \rbrace,
%  								\lbrace x_5, x_6, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_1, c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
%  									\lbrace x_2 \rbrace, 
%  									\lbrace x_3, x_4 \rbrace,
%  									\lbrace x_5, x_7 \rbrace,
%  									\lbrace x_6 \rbrace \rbrace 
%  \end{array}$$
%
%\subsection{Concept Approximations}
%  RST capability of handling non-exact (rough) concepts arises from the approximation of a rough concept 
%  by means of two crisp concepts: the \textit{lower} and the \textit{upper approximations of} $X$. 
%  Denoted by $\underline{B}X$ and $\overline{B}X$, respectively as follows:
%  
%  \begin{equation}
%  	\begin{array}{lcc}
%  	\underline{B}X &=& \lbrace x \in U : [x]_B \subseteq X \rbrace\\
%  	\overline{B}X  &=& \lbrace x:[x]_B \cap X \neq \emptyset \rbrace
%  	\end{array}
%  \end{equation}
%  
%  We would say that objects in $\underline{B}X$ can be certainly classified as elements of $X$ on the 
%  basis of knowledge in $IS_B$, while objects in $\overline{B}X$ can only be possibly classified as 
%  elements of $X$ on the basis of knowledge in $IS_B$. In other words, $\underline{B}X$ is 
%  composed by those objects in $X$ having no indiscernible objects outside $X$. $\overline{B}X$ on the 
%  other hand, is composed by all objects in $X$ plus all objects outside $X$ having an indiscernible 
%  object in $X$.
%  
%  The set 
%  
%  \begin{equation}
%  	BN_B(X)=\overline{B}X-\underline{B}X
%  \end{equation}
%  
%  is called the \textit{B-boundary region of X} and it contains the objects which neither are certainly 
%  members of $X$ nor they are certainly member of $U-X$. The presence of a non-empty boundary region
%  indicates that the concept in question is rough (\textit{B-rough}).
%  
%  From the previous example we can see that e.g. concept $X=\lbrace x_1,x_2,x_3 \rbrace$ is rough for the 
%  three attributes sets while e.g. the concept $Y=\lbrace x_1,x_2 \rbrace$ is both $\lbrace c_2 \rbrace$-
%  and $\lbrace c_1, c_2 \rbrace$-exact. Notice that
%  
%  $$\begin{array}{lcc}
%  \underline{c_1}X &=& \emptyset\\
%  \overline{c_1}X  &=& \lbrace x_1,x_2,x_3,x_4,x_6 \rbrace\\
%  \underline{c_2}X &=& \lbrace x_1,x_2 \rbrace
%  \end{array}$$
  
\subsection{Positive Region}\label{subsect_Pos}
  The decision attribute $d$ induces a partition of the universe $U$ into equivalence classes 
  (\textit{decision classes}). Since we will be trying to associate a decision class to an object, 
  based on the knowledge in attributes belonging to $B \subseteq C$, we are interested in those 
  $B-classes$ (classes induced by $B$) which correspond to classes induced by $d$. 
  This idea leads to the notion of the  \textit{positive region of the decision}. The set $POS_B(d)$ 
  is called the \textit{B-positive region of d} and is the set of objects in $U$ such that all their
  indistinguishable objects (under the knowledge in $B$) belong to its same class induced by $d$.
  
  Taking for example the IS in table~\ref{tab_IS}, we can see that
  
  $$\begin{array}{lcc}
  POS_{\lbrace c_1 \rbrace}(d)&=&U\\
  POS_{\lbrace c_2 \rbrace}(d)&=& \lbrace x_1,x_2,x_3,x_4 \rbrace\\
  POS_{\lbrace c_1, c_2 \rbrace}(d)&=&U
  \end{array}$$
 
\subsection{Reducts and Core}
  Given an information system $IS=(U,A)$ with condition attributes set $C$ and decision attributes set
  $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. A subset $B \subseteq C$ is a \textit{reduct} 
  of $IS$ relative to $D$ if
  \begin{enumerate}
  	\item $POS_B(D)=POS_C(D)$. \label{cond_1}
  	\item $B$ is a minimal subset (with respect to inclusion) satisfying condition~\ref{cond_1}.
  \end{enumerate}
  
  The intersection of all the reducts in an IS is called the \textit{core}, the elements of which cannot be
  eliminated without introducing more contradictions to the representation of the dataset.
  
\subsection{Discernibility Matrix and Discernibility Function}
  The discernibility knowledge of the information system is commonly recorded in a symmetric $|U| \times |U|$
  matrix called the \textit{discernibility matrix}, and each element $m_{ij}$ in the discernibility matrix 
  $M_{IS}$ is defined as 
  
  \begin{equation}
  	m_{ij}=\left\lbrace\begin{array}{cl}
  			\lbrace c \in C: c(x_i) \neq c(x_j) \rbrace & \mathrm{for~~}D(x_i) \neq D(x_j)\\
  			\emptyset 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation}
  
  Table~\ref{tab_DM} shows the discernibility matrix for the IS in table~\ref{tab_IS} as a lower triangular 
  matrix.
  
   \begin{table}[htb]
		\caption{Discernibility Matrix Example.} \label{tab_DM}
		\centering
 	\begin{tabular}{c|ccccccc}
 		$x \in U$ & 1 & 2 &  3 & 4 & 5 &  6 & 7\\
 		\hline
		1 &&&&&&&\\
		2 &&&&&&&\\
		3 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		4 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		5 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		6 &&& $c_1,c_2$ & $c_1,c_2$ &&&\\
		7 & $c_1,c_2$ & $c_1,c_2$ &&&& $c_1$ &\\
 	\end{tabular}             
 \end{table}
  
  Once the discernibility matrix $M_{IS}$ is found, we can define the \textit{discernibility function} $f_{IS}$.
  This is a boolean function of $n$ boolean variables $c_1^*, c_2^*,...,c_n^*$, representing the presence of
  the corresponding attribute (True) or its absence (False) in $M_{IS}$.
  
  \begin{equation}
  	f_{IS}(c_1^*, c_2^*,...,c_n^*)=\wedge \lbrace \vee c_{ij}^* : 1 \leq j \leq i \leq |U|, 
  									c_{ij} \neq \emptyset \rbrace
  \end{equation}

  where $c_{ij}^*=\lbrace c^* : c \in c_{ij} \rbrace$. Only the lower triangular matrix from $M_{IS}$ is
  taken into consideration since $M_{IS}$ is symmetric. An equivalence between the prime implicants of
  $f_{IS}$ and all the reducts of $IS$ has been found~\citep{Pawlak07}.
  
  The discernibility function for the discernibility matrix in table~\ref{tab_DM} is, without repetitions  
  $f_{IS}(c_1^*,c_2^*)=(c_1^* \vee c_2^*) \wedge c_1^*$
  
  From this we can easily see that the only reduct (also the core) for this IS is $c_1$.
  
%\subsection{Attributes Dependency and Significance}
%  One important aspect of data analysis is the study of dependencies between attributes describing the 
%  objects. Intuitively, a subset of attributes $D$ depends totally on a set of attributes $B$, denoted 
%  $B \Rightarrow D$ if all attribute values from $D$ are uniquely determine by values of attributes
%  in $B$. Formally in RST we say that for $B,D \subset A$, $D$ depends on $B$ in a degree 
%  $k(0 \leq k \leq 1)$, denoted $B \Rightarrow _{k}D$ if
%  
%  \begin{equation}
%  	k=\gamma _B (D)=\frac{|POS_B(D)|}{|U|}
%  \end{equation}
%    
%  This is called the \textit{Positive Dependency Degree}, and it is the ratio of the number of objects belonging
%  to the positive region to the number of all objects in universe $U$. If $k=1$, $D$ depends totally on $B$, if
%  $0 < k < 1$, $D$ depends partially (in a degree $k$) on $B$ and if $k=0$, $D$ does not depends on $B$.
%  
%  Taking for example the positive regions from subsection~\ref{subsect_Pos}, we can see that
%  
%  $$\begin{array}{lcccc}  
%  \gamma _{\lbrace c_1 \rbrace} (d)&=&\frac{|U|}{|U|}&=&1\\
%  \gamma _{\lbrace c_2 \rbrace}(d)&=& \frac{|\lbrace x_1,x_2,x_3,x_4\rbrace|}{|U|}&=&\frac{4}{7} \\
%  \gamma _{\lbrace c_1, c_2 \rbrace}(d)&=&\frac{|U|}{|U|}&=&1
%  \end{array}$$
%  
%  We can express the \textit{significance} of feature $c \in B$ upon $D$ as
%  
%  \begin{equation}
%  	SIG(c,B,D)=\gamma _B (D)-\gamma _{B-\lbrace c \rbrace} (D)
%  \end{equation}
%  
%  From  the previous example:
%  
%  $$\begin{array}{lcccccc}
%  SIG(c_1,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_2 \rbrace}(d)
%  										&=&1-\frac{4}{7}&=&\frac{3}{7}\\
%  SIG(c_2,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_1 \rbrace}(d)
%  										&=&1-1&=&0
%  \end{array}$$
%  
%  From this it follows that attribute $c_1$ is indispensable, but attribute $c_2$ can be dispense when 
%  considering the dependency between the decision attribute $d$ on condition attributes $c_1$ and $c_2$.
%  
%  Finally, we would like to say that an alternative definition of reduct from dependency exist. We say 
%  that a subset $B \in C$ is a reduct of $C$ if it is a minimal set (with respect to inclusion) satisfying 
%  the condition $\gamma _B(D)=\gamma _C(D)$.
\subsection{Simplified Discernibility Matrix}
  The \textit{Simplified Discernibility Matrix} is a reduced version of the discernibility matrix after
  eliminating supersets and repeated cells in $M_{IS}$. This new discernibility matrix has the same reducts
  as the original one~\citep{Yao09}. An equivalent concept exists in the Testor Theory, called 
  \textit{Basic Matrix}. The basic matrix was proven to have the same TT as the original discernibility 
  matrix~\citep{Lazo01}.

\section{Related Work}\label{relatedWork}
  In this section, we will be first discussing heuristic algorithms for reduct computation. Some of these 
  algorithm are capable of finding several reducts and others are intended to obtain a single \textit{minimal} 
  reduct. Then, two kind of algorithm for computing the complete set of reducts will be exposed: those 
  from RST and those from TT. Researches in TT have been focused in algorithm for finding all typical testors.
  Finally, we will make a survey of parallel accelerations reported in literature.  
  
  Figure~\ref{fig_Tax} shows a taxonomy of the reported algorithms for reduct computation. This classification
  corresponds to the sequence that we will be following throughout our survey of the state of the art. 
  Table~\ref{tab_Alg} makes a correspondence between the codename used in figure~\ref{fig_Tax} and the 
  publication reporting the algorithm. Some parallel approaches are implementations of existing sequential 
  algorithm; for those cases, we use a color association in figure~\ref{fig_Tax}.

  \begin{figure}[htb] 
    \centering
    \includegraphics[width=\textwidth]{Reduct_Computation_Algorithms.png}
	\caption{Taxonomy of reduct computation algorithms.}
	\label{fig_Tax}
 \end{figure}
 
 \newcolumntype{L}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 \begin{table}[t]
	\caption{reduct computation algorithms.} \label{tab_Alg}
	\centering
 	\begin{tabular}{l||L{13cm}}
 		\multicolumn{1}{c||}{Codename} &  \multicolumn{1}{c}{Publication} \\
 		\hline \hline
		\textsc{quickreduct} 	&  Rough set-aided keyword reduction for text 
								   categorization~\citep{Chouchoulas01} \\
		YangLiHuang 				&  An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility
				 				   Matrix~\citep{Yang08}\\
		EBR						&  A rough set--aided system for sorting WWW bookmarks~\citep{Jensen01}\\
		RSFSACO					&  A rough set approach to feature selection based on ant colony 
								   optimization~\citep{Chen10}\\
		AntRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
		GenRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
		BjorvandKomorowski		&  Practical applications of genetic algorithms for efficient reduct 
								   computation~\citep{Bjorvand97}\\
		\hline		   
		BTHW						&  FPGA Based Architecture for Computing Testors~\citep{Rojas07}\\
		BruteForce				&  On the Design and Implementation of a High Performance Configurable
								   Architecture for Testor Identification~\citep{Cumplido06}\\
		TiwariKothariShah		&  FPGA Implementation of a Reduct Generation Algorithm based on Rough 
								   Set Theory~\citep{Tiwari13}\\
		TiwariKothari			&  Architecture and Implementation of Attribute Reduction Algorithm Using 
								   Binary Discernibility Matrix~\citep{Tiwari11}\\
		DM-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
		DT-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
		SRGonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\citep{Jiao10}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\citep{Jiao10}\\
		\hline
		TB \& BT					&  BT and TB algorithms for computing all irreducible testors~\citep{Ruiz85}\\
		CT--EXT					&  CT--EXT: an algorithm for computing typical testor set~\citep{Sanchez07}\\
		LEX						&  LEX: a new algorithm for the calculus of typical
								   testors~\citep{Santiesteban03}\\
		BR						&  BR: A new method for computing all typical testors~\citep{Lias09}\\
		YYC						&  YYC: A Fast Performance Incremental Algorithm for Finding Typical
								   Testors~\citep{Alba14}\\
		RSAR	--SAT				&  Finding rough and fuzzy--rough set reducts with SAT~\citep{Jensen14}\\
		Expansion Algorithm		&  Reduct generation in information systems~\citep{Starzyk99}\\
		GonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\	
 	\end{tabular}             
 \end{table}
 
\subsection{Heuristics Approaches}
  The algorithm presented in~\citep{Chouchoulas01} (\textsc{quickreduct}) starts with an empty set of 
  attributes and adds, one each iteration, the attribute having the highest significance. This greedy algorithm 
  uses the maximal dependency criterion for finding one optimal reduct.  
  A similar approach is the Johnson Reducer~\citep{Johnson74}, first introduced in RST in \citep{Ohrn00}.
  This simple greedy algorithm begins with an empty set of attributes evaluating each conditional attribute in the
  discernibility function according to a heuristic measure. In the simplest case, those attributes with highest 
  appearance frequency within the logical context termed clauses, are considered to be more relevant. Works 
  in~\citep{Nguyen97} and~\citep{Wang01} use alternative heuristic functions guiding the search down better paths. 
  Variations of this algorithm~\citep{Wang01} and~\citep{Yang08} use the discernibility matrix instead of the
  discernibility function.
  Algorithm presented in \citep{Zhong01} starts from the core (since it must be contained in every reduct) and
  follows a similar procedure adding selected attributes. This optimization may be impractical for large datasets
  \citep{Jensen14} since the core must be computed a priori.
  
  The work presented in~\citep{Jiao10} improves the efficiency of computing reducts by means of subdivision. The 
  original dataset is broken down into a master-table and several sub-tables that are simpler, more manageable 
  and more solvable. Results are then joined together in order to solve the original dataset. Different variants
  for decomposition of reduct computation problem are discussed and proposed in~\citep{Strakowski08}.
  
  Special attention deserve the approaches using genetic algorithms to discover locally shortest reducts. Although 
  these algorithms do not guarantee finding globally shortest reducts, many reducts may be found in a determined
  time. A good point in this approach is the use of the fitness function to guide the search down to a set of 
  reducts with the desired properties. The work reported in~\citep{Wroblewski95} encodes candidates as bit 
  strings with a positional representation of attributes presence in the candidate set. The fitness function
  depends on the number of attributes in the subset, penalizing strings with a large number of bits set. The 
  second optimization parameter is the number of objects classified by the given candidate. The reduct should 
  discern as many objects as possible.
  
  Other evolutionary approaches to reduct computation include Ant Colony Optimization~\citep{Jensen03}
  and~\citep{Chen10}; and Particle Swarm Optimization~\citep{Wang07}.
    
\subsection{Algorithms for Reducts and Testors Computation}
  One of the first algorithm designed to overcome the exponential complexity (regarding
  the number of features) of the problem of finding all the TT, was 
  proposed by \cite{Ruiz85}. This algorithm, called BT,
  codified a subset of features as a binary word with as many bits as features in the 
  dataset. A 0 represents the absence of the corresponding feature in the current
  subset while a 1 represents its inclusion. This way, candidates subsets are evaluated
  in the natural order of binary numbers. The pruning process in the
  search space is based on the minimal condition of TT and a convenient sorting
  of the basic matrix associated to the dataset. Finally, 
  testors found by BT algorithm must by compared with each other in order to remove
  any superset (not a TT by definition).
  In \citep{Shulcloper95b} a new algorithm (REC) is presented.
  The main drawback of REC was that it operated directly over the dataset (instead of the
  basic matrix), handling a huge amount of superfluous information. \cite{Ayaquica97}
  presented the algorithm CER directed to solve this problem by using a different traversing
  order. 
	
  Then, \cite{Santiesteban03} proposed a revolutionary algorithm
  called LEX. Main ideas behind LEX are a new traversing order of candidates (which resembles the
  lexicographical order in which string characters are compared) and the concept of gap. In LEX
  the typical condition is verified first and only for those potentially TT, the testor 
  condition is checked. This way, the out-coming testors from this algorithm are always typical.
  The concept of gap allows us; once obtained a TT (or a not testor) candidate, including 
  the last feature in the dataset, avoid the evaluation of any subset of this candidate.
	
  \cite{Sanchez07} proposed the CT\_EXT algorithm for computing all
  TT. Following a traversing order similar to that in LEX, this algorithm search for
  testors without verifying the typical condition. This way, a larger number of candidates are 
  evaluated, in comparison to LEX; but the cost of each evaluation is lower. Results from experiments
  show that CT\_EXT is faster than the previous existing algorithm for most datasets. Then, \cite{Lias09}
  presented the BR algorithm, a Recursive algorithm based on 
  Binary operations. BR is very similar to LEX in its bones but its recursive nature encloses a great
  gain. Given a candidate subset, the remaining features are tested a priori and those being rejected are
  excluded from subsequent evaluations. \cite{Sanchez10} presented a cumulative
  procedure for the CT\_EXT algorithm. This fast-CT\_EXT implementation reduces drastically the runtime
  for most datasets at no extra cost. In \citep{Lias13} the
  gap elimination and column reduction are added to BR. This fast-BR algorithm is, no doubt the one 
  evaluating the minimum number of candidates in the state of the art. The main drawback of fast-BR and 
  BR is, as in LEX, the high cost of evaluating the typical condition for every candidate. 
 
  Recently, a new internal typical testor--finding algorithm (YYC) was proposed by~\cite{Alba14}. Although 
  they claim that this algorithm verify less candidates than previous alternatives, two weak points should
  be addressed. First, BR is not included in comparisons; and second, the evaluation cost for a candidate
  in YYC is high compared to that of previous algorithms. YYC verifications involve calculations of the 
  Hamming weight.

  A method for the computation of all reducts in an Information System is proposed in \citep{Starzyk99,Starzyk00}.
  This is a divide and conquer approach. On each step, the absorption laws are applied over the incoming
  discernibility matrix to obtain a basic matrix. Then, the strong equivalent attributes are compressed
  (which is a local reduction of columns). The most discerning attributed is selected (in the same way as 
  Johnson's reducer does) and the problem y divided into two sub-problems: 
  \begin{itemize}
  \item This selected attribute belongs to the reducts and the recursive function is called with a new basic 
  matrix, having only those rows where the selected attribute does not appear.
  \item This selected attribute does not belong to the reducts and the recursive function is called with a new 
  discernibility matrix, removing the column corresponding to the selected attribute.
  \end{itemize}
  The base case is reached when each attribute in the incoming discernibility matrix appears in a single
  clause. Finally a set of super-reducts is obtained and supersets must be removed in order to obtain 
  the complete reducts set.
  Noticed that this algorithm is oriented to the binary discernibility function, and 
  terms such as discernibility and basic matrix, rows and columns are not used in the paper. The
  algorithm is presented in an iterative fashion and its recursive nature is not explicitly stated.
  
  \cite{WangP07} proposed a new algorithm for computing all the reducts (RGonCRS). Reported two years before
  \citep{Lias09}, this algorithm is incredibly similar to BR. Notice that this is a rough sets approach to the
  problem and the nomenclature is totally different to that of BR. Essentially, every proposition supporting the
  pruning process in \citep{WangP07} have an equivalent proposition in \citep{Lias09}. Main differences with BR
  are:
  \begin{itemize}
  \item Works directly over the dataset instead of the basic matrix.
  \item Starts searching the core and if exist looks for reducts as supersets of the core.
  \item A recursive implementation is proposed instead of the iterative solution of BR.
  \item Contributing attributes are sorted, as in the Johson reducer, during the algorithm execution.
  \item A second algorithm (SRGonCRS) is proposed for the dataset subdivision and incrementally 
  		finding reducts.
  \end{itemize}
  
  
  In~\citep{Lin04}, a heuristic is followed to find a short reduct. This first reduct is used to limit the search
  space to attributes combinations with lower cardinality. The main drawback of this algorithm is that the second
  step search for reducts by checking all possible $s$-subtables of the whole database. A $s$-subtable means a 
  subtable whose conditional attributes have size $s$. In other words, it is a decision table with conditional
  attribute of size $s$ plus the decision attributes of original table. This final process uses no pruning strategy 
  and explores the combinatorial possibilities, which is unfeasible in most cases.
  
  Although originally intended for computing a single minimal reduct, the algorithm proposed in~\citep{Jensen14}
  may be modified in order to obtain all reducts in an Information System. The method introduced in this work
  reduces the problem of finding a reduct from the discernibility function to the SAT problem~\citep{Davis62}. 
  The boolean function generated in this way is always satisfied since the complete set of attributes is a trivial
  solution.
  
\subsection{Parallel Accelerations}

  A parallel acceleration of the algorithm presented in~\citep{Yang08} for reduct generation from binary
  discernibility matrix was developed in~\citep{Tiwari11,Tiwari12}. This FPGA implementation computes a 
  single reduct. A real application of object identification system by an intelligent robot is presented.
  In~\citep{Tiwari13} a quick algorithm, similar to those presented in~\citep{Chouchoulas01}, is proposed
  and implemented in a hardware fashion. A recent work from these authors~\citep{Tiwari14}, shows a thorough
  survey of FPGA applications in rough sets reduct computation.

  From the Typical Testors theory, several attempts have been made to overcome the problem 
  complexity by means of FPGA implementations of algorithms. In a first work~\citep{Cumplido06}, an 
  FPGA-based brute force approach for computing testors was proposed. This first approach did 
  not take advantage of dataset characteristics to reduce the number of candidates to be tested; 
  thus all $2^n$ combinations of $n$ features have to be tested. Then, in \citep{Rojas07} a hardware 
  architecture of the BT algorithm for computing typical testors was implemented. 
  This algorithm uses a candidate pruning process for avoiding many unnecessary candidate evaluation, 
  reducing the number of verifications of the typical testor condition. These two previous works computed 
  a set of testors on the FPGA device whilst typical condition was evaluated afterwards by the 
  software component in the hosting PC. Thus, in~\citep{Rojas12} a hardware-software platform for 
  computing typical testors that implemented the BT algorithm, as in \citep{Rojas07}, was proposed; but it also 
  included a new module that eliminates most of the non typical testors before transferring them to 
  a host software application for final filtering. 
	
	%TODO estos trabajos de Wroblewski hay q revisarlos bien pq parece falso lo de los GA paralelos
  In~\citep{Wroblewski98}, a parallel variant of the algorithm proposed in~\citep{Wroblewski95} is presented.
  Developments in parallel implementations of genetic algorithms are exploited to provide a speedup for the 
  problem of finding reducts.
%  This line of thinking brought us an unexplored acceleration idea from~\citep{Jensen14}. We can combine 
%  available architectures for SAT solving on FPGA~\citep{Safar07,Kanazawa11} with the transformation
%  presented in~\citep{Jensen14} to obtain a new hardware platform for computing all reducts of an information 
%  system.
  
  In~\citep{Grzes13,Kopczynski14}, an FPGA application for a single reduct computation is presented. Although
  authors claim that a huge acceleration is achieved, some weak points have to be mentioned. Experiments presented 
  in~\citep{Kopczynski14} to validate their results are performed over a small dataset which in our experience 
  does not implies its applicability to larger cases where such acceleration is needed. On the other hand, 
  runtime estimations for FPGA component executions are made by means of a oscilloscope without taking into 
  account communication overhead, which cannot be neglected in the presented dataset.
  
\section{Research Proposal}\label{ResearchProposal} 

\subsection{Justification and Motivation}\label{Justification}{
  RST can be used to reduce the number of attributes in a dataset without relevant information lost. 
  Therefore, there has been much research in the area of finding reducts, particularly, shortest 
  reducts~\citep{Jensen14}. 
  
  Heuristic methods such as~\citep{Chouchoulas01,Jensen2004,Zhong01} are fast alternatives for finding 
  reducts but they do not guarantee minimal reductions. Stochastic approaches~\citep{Wroblewski95,Jensen03,
  Chen10,Wang07} still do not guarantee finding the smallest reducts, as we will see further. Techniques 
  for finding the complete reducts set~\citep{Ruiz85,Santiesteban03,Sanchez07,Lias09} can, of course, find 
  shrotest reducts but with a higher computational effort.
  
  The motivation of this work is the development of an algorithm for finding reducts with minimal length 
  in an information system. This is an NP-Hard problem which makes every attempt for reducing its execution
  time, a challenging task. Our proposal must be competitive with the state of the art algorithm in all 
  cases and will be faster in most datasets. The main arena for comparison will be a large set of synthetic,
  randomly generated datasets and benchmarking datasets from~\citep{Bache13}. We will be dealing only with
  classical reducts definition. The practical applications (or relevance) of globally shortest reducts in
  supervised classification is beyond our goals.
  
  The results of this research will impact the supervised classification methods specially in large datasets.
  Nowadays, data is automatically collected, thus generating huge databases in almost every field. The 
  current growth of the size of data and the number of existing databases, is the justification for our
  research on efficient algorithms for dimensionality reduction without semantics lost.}  
  
\subsection{The Research Questions}\label{ResearchQuestions} 
  Throughout our preliminary work, we noticed that there is no faster algorithm for finding reducts in 
  all datasets. Different algorithms use different strategies for traversing and pruning the complete search 
  space. These strategies are better suited for some datasets and are time consuming for others. This leads 
  us to our first scientific question:
  
\begin{quote}
  \emph{Is there a relationship between the external properties of the discernibility matrix and the runtime 
  		cost of traversing strategies for finding reducts of an information system?}
\end{quote}
  		
  In this context, external properties are those characteristics that we can extract from the discernibility
  matrix by traversing their cells just one time. Lets take for instance, the minimum and maximum number of
  attributes in a cell, the core or the mean number of attributes per cell. Internal properties require, on 
  the other hand, more complex operations. Internal properties are, for instance, the number of reducts, the
  cardinality of the shortest and largest reducts, etc.
  
  From this research question we formulate the following hypothesis:
  
\begin{quote}  
  \emph{There is a relationship between the external properties of the discernibility matrix and the runtime 
  		cost of traversing strategies for finding reducts of an information system}
\end{quote}
  		
  Several attempts for the decomposition of the original problem of the complete reducts set computation have 
  been made~\citep{Strakowski08,Jiao10,Kopczynski14}. The main disadvantage of the problem decomposition or
  parallelization is the strong dependency between the speed--up of the method used and the particularities of 
  the dataset~\citep{Strakowski08}. This leads us to our second scientific question:
  
\begin{quote}
  \emph{Is there a relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding reducts of an information system 
  		in a parallel environment?}
\end{quote}

  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{There is a relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding reducts of an information
  		system in a parallel environment}
\end{quote}
  
  The more sophisticated a scanning strategy is, the less number of candidates attributes sets are 
  evaluated. Unfortunately, a more sophisticated scanning strategy has usually a greater runtime cost.
  This trade--off between the number of evaluated candidates and the runtime cost of evaluation leads us 
  to our third scientific question:
  
\begin{quote}
  \emph{Can be obtained a runtime reduction in computing reducts of an information system by 
  		dynamically changing the traversing strategy?}
\end{quote}
  By dynamically changing we mean the change of the traversing strategy during the algorithm execution.
  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{There can be obtained a statistically significant runtime reduction in computing reducts of an 
  		information system by dynamically changing the traversing strategy}
\end{quote}

  Based on these three scientific questions we can formulate the main hypothesis for our research proposal:
  
\begin{quote}
  \emph{Using some external properties of the discernibility matrix, and dynamically changing the traversing 
  		strategy, we can design a new algorithm for computing reducts of an information system; which is
  		comparable to the state of the art alternatives in most datasets, and faster in some of them}
\end{quote}  

\subsection{The Research Goals}\label{Goals} 
  The main goal in our research is the \emph{development of  a new algorithm for computing reducts of an 
  information system; which is comparable to the state of the art alternatives 
  in most datasets, and faster in some of them}. This algorithm will use some external properties of the
  discernibility matrix to conveniently select the traversing strategy for the search space and a 
  decomposition method suitable for paralellization. We will be exploring two variants of this problem,
  the problem of computing all reducts and the problem of computing globally shortest reducts of an 
  information system. The problem of finding shortest reducts is also NP--hard~\citep{Lin04} but different 
  pruning rules may be used.
  
  Our specific goals are:
  \begin{enumerate}
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		traversing strategy for computing all reducts.
  		  	
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		decomposition method for computing all reducts.
  		
  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
  		for computing all reducts.
  
  \item Develop a new algorithm for computing all reducts of an information system, based 
  		on the three previously found relationships.
  		
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		traversing strategy for computing globally shortest reducts.
  		  	
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		decomposition method for computing globally shortest reducts.
  		
  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
  		for computing globally shortest reducts.
  
  \item Develop a new algorithm for computing shrotest reducts of an information system, based 
  		on the three previously found relationships.

  \end{enumerate}

\subsection{Methodology}\label{Methodology} 
\begin{enumerate}
	\item Selecting a priori the fastest traversing strategy for finding all reducts.
	\begin{itemize}
  		\item Generate a set of random datasets, systematically covering the space of possible combinations in 
  			  the values of the external properties of discernibility matrices.
  		\item Implement the main traversing strategies reported for the computation of all reducts.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing shrotest reducts using the classifier found in
  			  the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Selecting a priori the fastest decomposition method for finding all reducts.
	\begin{itemize}
  		\item Implement the main decomposition methods reported for the computation of all reducts
  			  in a parallel fashion.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest decomposition method as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  decomposition method for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing all reducts using the classifier found in the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Find a relationship between the traversed space and the expected cost of traversing strategies.
  	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of all reducts in a way
  			  that we can collect statistics for every execution stage.
  		\item Make a statistical description  of strategies runtime cost over synthetic and benchmarking datasets,
  			  using the traversed space as a factor.
  		\item Find a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Develop a new algorithm for computing all reducts of an information system.
  	\begin{itemize}
  		\item Design a new algorithm for computing all reducts using some external
  		      properties of the discernibility matrix.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	
  	\item Selecting a priori the fastest traversing strategy for finding globally shortest reducts.
	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of minimal length reducts.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing shrotest reducts using the classifier found in
  			  the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Selecting a priori the fastest decomposition method for finding globally shortest reducts.
	\begin{itemize}
  		\item Implement the main decomposition methods reported for the computation of minimal length reducts
  			  in a parallel fashion.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest decomposition method as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  decomposition method for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing shrotest reducts using the classifier 
  			  found in the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Find a relationship between the traversed space and the expected cost of traversing strategies.
  	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of globally shortest 
  			  reducts in a way that we can collect statistics for every execution stage.
  		\item Make a statistical description  of strategies runtime cost over synthetic and benchmarking datasets,
  			  using the traversed space as a factor.
  		\item Find a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Develop a new algorithm for computing shrotest reducts of an information system.
  	\begin{itemize}
  		\item Design a new algorithm for computing shrotest reducts using some external
  		      properties of the discernibility matrix.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
\end{enumerate}
	  	
%\section{Publication Plan}\label{PubPlan}
%  In this section we present our publication plan. Results from this research are to be submitted indistinctly 
%  to the following journals or congresses, according to the advisor’s recommendations.
%  
%  Main journals:
%  \begin{itemize}
% 	\item Information Sciences\footnote{http://www.journals.elsevier.com/information-sciences/}. IF: 3.893
% 	\item Expert Systems with 
% 		  Applications\footnote{http://www.journals.elsevier.com/expert-systems-with-applications/}. IF: 1.965
% 	\item IEEE Transactions on Knowledge and Data
% 		  Engineering\footnote{http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69}. IF: 1.815
% 	\item International Journal of Advanced Computer Science and
% 		  Applications\footnote{http://thesai.org/Publications/IJACSA}. IF: 1.32
%  \end{itemize}
% 
%  Main congresses:
%  \begin{itemize}
% 	\item Rough Sets and Current Trends in Computing
% 	\item Rough Sets and Intelligent Systems Paradigms
% 	\item CIARP: Iberoamerican Congress on Pattern Recognition
% 	\item Mexican Conference on Pattern Recognition
%  \end{itemize}
%  
%  In table~\ref{tab_PP} we present the estimated dates for our proposed publications. The first publication will
%  discuss the results of the first experiment. In our second publication we will make a review of the state of 
%  the art in rough sets reduct computation. Third and fourth publications will present the results from the last
%  three experiments as shown in table~\ref{tab_PP}.
%  
%     \begin{table}[htb]
%		\caption{Publication plan.} \label{tab_PP}
%		\centering
% 	\begin{tabular}{c||l|l}
% 		\# & \multicolumn{1}{c|}{Date} & \multicolumn{1}{c}{Results}\\
% 		\hline \hline
%		1 & July 2015 & First Experiment (\ref{exprimet1})\\
%		2 & December 2015 & State of the art survey \\
%		3 & July 2016 & Second Experiment (\ref{exprimet2})\\
%		4 & July 2017 & Third and fourth Experiments (\ref{exprimet3} \& \ref{exprimet4})\\
% 	\end{tabular}             
% 	\end{table}

%-------------------------------------------------------------------------------
% Bibliography
%-------------------------------------------------------------------------------
\newpage 
\bibliography{mybib}{}
\bibliographystyle{authordate1}
\end{document}
