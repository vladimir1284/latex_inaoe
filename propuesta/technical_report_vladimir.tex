%-------------------------------------------------------------------------
%\documentclass[11pt,authoryear]{elsarticle}
\documentclass[authoryear,11pt]{elsarticle}

\setlength{\parskip}{1em}			% espaciar parrafos
\usepackage{hyperref}				% enlaces en el pdf
\hypersetup{backref,colorlinks=true}	% colores en vez de cajas en los enlaces
\usepackage{times}              		% la letra
\usepackage{graphicx}           		% para manejar imagenes
\usepackage{subfigure}          		% para manejar subfiguras
\usepackage{tabularx}		   		% para ajustar el ancho de las columnas
\usepackage[margin=2.5cm]{geometry}	% Change margins
\usepackage[table]{xcolor}			% Colores en el cronograma
\usepackage{multirow}				% Cabecera del cronograma
\usepackage{watermark}				% Para la portada
\usepackage{datetime}				% Fecha de creado
\usepackage{pst-tree}				% Para la taxonomía
\usepackage{stackengine}				% Para listar los articulos en el nodo de la taxonomía
%\newdateformat{mydate}{\monthname[\THEMONTH] \THEYEAR} 
%-------------------------------------------------------------------------
% Configuring Taxonomy
%-------------------------------------------------------------------------
\setstackEOL{\\}
\def\psedge{\ncangles[angleA=-90,angleB=90]}
\psset{levelsep=20mm,treesep=1cm,nodesep=3pt, arrows=->}
\def\PSBL#1{\small\pspicture(7,0.8)\psTextFrame[shadow,
  fillstyle=solid,linecolor=blue,framearc=0.3](0,0)(7,0.8){%
    \shortstack{#1}}\endpspicture}
\def\PSBS#1{\pspicture(3,.7)\psTextFrame[shadow,
  fillstyle=solid,linecolor=blue,framearc=0.3](0,0)(3,0.8){%
    \shortstack{#1}}\endpspicture}

%-------------------------------------------------------------------------
% Referencias a una palabra
%-------------------------------------------------------------------------
\newcommand{\setword}[2]{%
  \phantomsection
  #1\def\@currentlabel{\unexpanded{#1}}\label{#2}%
}

\begin{document}
	\input{./frontPage.tex}
	
	\title{Development of fast algorithms for reduct computation}
	
	\author{Vlad\'imir Rodr\'iguez Diez}
	\author{Jos\'e Francisco Mart\'inez Trinidad}
	
	\address{Computer Science Department\\National Institute of
	Astrophysics, Optics and Electronics\\
	Luis Enrique Erro \# 1, Santa Mar\'{\i}a Tonantzintla, Puebla,
	72840, M\'{e}xico} 
%	\email{\{vladimir.rodriguez,fmartine\}@inaoep.mx}
	
	
%	\thispagestyle{empty}
	
	\begin{abstract}
		Information systems in Rough Set Theory (RST) are tables of objects described by some attributes. 
		This type of tables are widely used in different pattern recognition problems particularly in 
		supervised classification. RST reducts are minimal subsets of attributes preserving 
		the discernibility capacity of the whole set of attributes. Reducts computation has an exponential
		complexity regarding the number of attributes in the information system. In the literature several
		algorithms for reduct computation have been reported, but their high computational cost makes 
		infeasible their use in large problems. For this reason, in this research we will develop new fast
		algorithms in two directions, the computation of all the reducts and the computation of globally 
		shortest reducts. The proposed algorithms will be faster than state of the art algorithms, and hence 
		make the reduct computation viable for larger information systems than it is today. As part of this 
		PhD research proposal, we present some preliminary results, which show that it is possible developing
		faster algorithms for computing reducts.
	\end{abstract}
	
	\begin{keyword}
		Rough Sets\sep Dimensionality Reduction\sep Reduct Computation.
	\end{keyword}

	\maketitle

\pagebreak 
\tableofcontents
\pagebreak 

%-------------------------------------------------------------------------------
% your earth-shattering contribution

\section{Introduction}
  Rough set theory (RST), proposed by Z. Pawlak in 1982 \citep{Pawlak81,Pawlak81-2,Pawlak82,Pawlak91}, 
  is a relatively new mathematical theory 
  to deal with imperfect knowledge, in particular with vague concepts. Into RST, information systems
  are tables of objects described by a set of attributes (columns). 
  When data is collected or recorded, every single aspect (attribute) of the object under study is considered 
  to have a complete representation and to ensure that no potentially useful information is lost.
  As a result, information systems are usually characterized by a large number of attributes,
  degrading the performance of machine learning tools~\citep{Parthalain08}.
  One of the main concepts in RST is the notion of reduct, which is a minimal subset of attributes 
  preserving the required classification capabilities~\citep{Pawlak91}. A new information system using 
  only those features in a reduct, is a reduced representation of the original data that allows obtaining  
  the same classification quality than the original information system. 
  However, the main restriction in practical applications of RST is that computing all the reducts of an information 
  system has an exponential complexity~\citep{Skowron92}. Therefore an active research line is the development 
  of fast algorithms for reduct computation.
  
  Several attempts to speed up the reduct computation have been reported. Many of these algorithms are 
  based on some heuristics. The main drawback of this approach is that these algorithms do not necessarily 
  return the complete set of reducts in the information system, and may obtain super-reducts (non minimal subsets). 
  Another way to speed up reduct computation is parallelization~\citep{Strakowski08}. There are also 
  interesting alternatives such as the use of a parallel version of genetic algorithms \citep{Wroblewski98}
  and the transformation of the reduct computation problem to the well known SAT problem~\citep{Jensen14}.
  
  Recently the RST reducts have been related to the typical testors (TT) from the logical combinatorial approach to 
  pattern recognition~\citep{Lazo15}. Testor Theory was created by Yablonskii and Chegis in the middle of fifties 
  of the last century as a tool for analysis of problems connected with control and diagnosis of faults in
  circuits. 
  Testor Theory can be used for feature selection as shown in~\citep{Ruiz08} and~\citep{Martinez01}. Algorithms for
  typical testors computation like, for instance~\citep{Ruiz85},~\citep{Santiesteban03},~\citep{Sanchez07} 
  and~\citep{Lias09}, can be applied to reduct computation due to the similarity between these two concepts. 
  Fast implementations of these algorithms; based on cumulative binary operations~\citep{Sanchez10}, genetic 
  algorithms~\citep{Sanchez99} and hardware architectures~\citep{Rojas12}; have been developed to reduce the
  computation time. One strength of our research is that we will be testing, for the first time, these two 
  families of algorithms in the same arena.
     
  Throughout our research we will conduct a comparative study between the most relevant algorithms for reduct 
  computation. Although our main focus will be on algorithms for computing all the reducts and globally shortest
  reducts, experiences form heuristics approaches will be considered as well. We will be exploring the relationship 
  between algorithms' performance and the characteristics of the information system. Based on this relationship,
  we will propose fast algorithms for reduct computation. Finally the proposed algorithms 
  will be redesigned and implemented in a hardware fashion in order to improve, even more, their efficiency.  

\section{Basic Concepts}\label{basicConcepts}
  RST is based on the assumption that every object in the universe of discourse is described, through a 
  set of attributes, by some information associated to it. This information constitutes the basis for the
  classification of new unseen objects. RST motto is \textit{Let the data speak for themselves}.
%   Moreover, RST is a formal framework to deal with imprecise and incomplete data with no need of additional information.
  
  From the RST point of view, two objects are indistinguishable (indiscernible) if they have an equivalent 
  value for each attribute in their description. Indiscernibility relations arising this way constitute the
  mathematical foundations of RST. 
%  Any partition of a dataset in which every pair of indiscernible objects
%  belongs to the same subdivision is called a crisp (precise) set; otherwise, the set is rough (imprecise, 
%  vague). 
  Some basic concepts of RST are presented bellow. Although we will be following the explanation 
  in~\citep{Polkowski00}, some modifications in the notation are introduced to provide clarity in the rest 
  of the document.
  
\subsection{Information System}
  The basic representation of data in RST is an \emph{Information System} (IS). An IS is a table with rows
  representing objects while columns specify its attributes or features. Formally, an IS can be defined as 
  $IS=(U,A)$ where $U$ is a finite non-empty set of objects $U=\lbrace x_1,x_2,...,x_n\rbrace$ and $A$ is a 
  finite non-empty set
  of attributes (features, variables). Every attribute in $A$ is a map: $a: U \rightarrow V_a$. The set $V_a$ is
  called the \textit{value set} of $A$. Attributes in $A$ are further classified as condition attributes $C$ and 
  decision attributes $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. 
%  We can of course define two value sets $V_c$ and $V_d$, for condition and decision attributes respectively, 
%  such that $V_a=V_c \cup V_d$. 
  Table~\ref{tab_IS} shows a typical IS.
  
  
 \begin{table}[htb]
		\caption{An Information System.} \label{tab_IS}
		\centering
 	\begin{tabular}{c||c|c|c||c}
 			  & $c_1$ & $c_2$ &  $c_3$ & $d$ \\
 		\hline \hline
		$x_1$ &   1   &    3  &  0  &   0   \\
		$x_2$ &   1   &    0  &  0  &   0   \\
		$x_3$ &   3   &    1  &  1  &   1   \\
		$x_4$ &   3   &    3  &  2  &   1   \\
		$x_5$ &   4   &    2  &  3  &   1   \\
		$x_6$ &   4   &    3  &  1  &   0   \\
		$x_7$ &   4   &    2  &  5  &   1   \\
 	\end{tabular}             
 \end{table}
 
   
  \textit{Decision attributes} determine to which class the object belongs. In the IS of
  table~\ref{tab_IS}, $d$ is the decision attribute. 
  %For this example $V_d = \lbrace 0,1 \rbrace$; hence 
  This is a two-class system. \textit{Condition attributes} do not absolutely determine the class but help 
  to decide to which class the object belongs. In supervised classification, condition attributes are the 
  only information available for classifying new objects; while, decision attributes are only 
  available for objects in the training set. IS with distinguished decision and condition attributes are 
  called decision tables. In table~\ref{tab_IS}, $c_1$, $c_2$ and $c_3$ are condition attributes.
 
%\subsection{Indiscernibility Relations}
%  For an object $x \in U$, the information about $x$ with respect to a set $B \subseteq A$ may be defined as
%  \textit{the B-information set} 
%  
%  \begin{equation}
%  	Inf_B(x)=\lbrace (a,a(x)):a \in B \rbrace
%  \end{equation}  
%  
%  of $x$.
%  
%  \emph{Indiscernibility relation} of $B$ is defined as follows:
%  
%  \begin{equation}
%  	(x,y) \in IND_B \Longleftrightarrow Inf_B(x)=Inf_B(y)
%  \end{equation} 
%  
%  Equivalent classes $[x]_B$ of the relation $IND_B$ represent therefore elementary (atomic) portions
%  of knowledge represented by the subsystem $IS_B=(U,B)$.
%  
%  For a \textit{concept} (set of objects), $X \subseteq U$, we say that $X$ is \textit{B-exact} if and 
%  only if
%  	
%  \begin{equation}
%  	X=\cup_{i=1}^{k} [x_i]_B
%  \end{equation} 
%  
%  for some $x_1, x_2,...,x_k \in U$ i.e. where $X$ is the union of some \textit{B-indiscernibility} classes.
%  
%  For example, in table~\ref{tab_IS} the three possible subset of conditional attributes are 
%  $\lbrace c_1 \rbrace$, $\lbrace c_2 \rbrace$ and $\lbrace c_1, c_2 \rbrace$. The indiscernibility relation 
%  for these sets defines three partitions of the universe:
%  
%  $$\begin{array}{lcc}
%  IND_{\lbrace c_1 \rbrace} &=& \lbrace \lbrace x_1, x_2 \rbrace, 
%  								\lbrace x_3, x_4 \rbrace, 
%  								\lbrace x_5, x_6, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_2 \rbrace} &=& \lbrace \lbrace x_1, x_4, x_6 \rbrace, 
%  								\lbrace x_2 \rbrace, 
%  								\lbrace x_3 \rbrace,
%  								\lbrace x_5, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_1, c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
%  									\lbrace x_2 \rbrace, 
%  									\lbrace x_3 \rbrace,
%  									\lbrace x_4 \rbrace,
%  									\lbrace x_5, x_7 \rbrace,
%  									\lbrace x_6 \rbrace \rbrace 
%  \end{array}$$
%
%\subsection{Concept Approximations}
%  RST capability of handling non-exact (rough) concepts arises from the approximation of a rough concept 
%  by means of two crisp concepts: the \textit{lower} and the \textit{upper approximations of} $X$. 
%  Denoted by $\underline{B}X$ and $\overline{B}X$, respectively as follows:
%  
%  \begin{equation}
%  	\begin{array}{lcc}
%  	\underline{B}X &=& \lbrace x \in U : [x]_B \subseteq X \rbrace\\
%  	\overline{B}X  &=& \lbrace x:[x]_B \cap X \neq \emptyset \rbrace
%  	\end{array}
%  \end{equation}
%  
%  We would say that objects in $\underline{B}X$ can be certainly classified as elements of $X$ on the 
%  basis of knowledge in $IS_B$, while objects in $\overline{B}X$ can only be possibly classified as 
%  elements of $X$ on the basis of knowledge in $IS_B$. In other words, $\underline{B}X$ is 
%  composed by those objects in $X$ having no indiscernible objects outside $X$. $\overline{B}X$ on the 
%  other hand, is composed by all objects in $X$ plus all objects outside $X$ having an indiscernible 
%  object in $X$.
%  
%  The set 
%  
%  \begin{equation}
%  	BN_B(X)=\overline{B}X-\underline{B}X
%  \end{equation}
%  
%  is called the \textit{B-boundary region of X} and it contains the objects which neither are certainly 
%  members of $X$ nor they are certainly member of $U-X$. The presence of a non-empty boundary region
%  indicates that the concept in question is rough (\textit{B-rough}).
%  
%  From the previous example we can see that e.g. concept $X=\lbrace x_1,x_2,x_3 \rbrace$ is rough for the 
%  three attributes sets while e.g. the concept $Y=\lbrace x_1,x_2 \rbrace$ is both $\lbrace c_2 \rbrace$-
%  and $\lbrace c_1, c_2 \rbrace$-exact. Notice that
%  
%  $$\begin{array}{lcc}
%  \underline{c_1}X &=& \emptyset\\
%  \overline{c_1}X  &=& \lbrace x_1,x_2,x_3,x_4,x_6 \rbrace\\
%  \underline{c_2}X &=& \lbrace x_1,x_2 \rbrace
%  \end{array}$$
  
\subsection{Positive Region}\label{subsect_Pos}
  Decision attributes induce a partition of the universe $U$ into equivalence classes 
  (\textit{decision classes}). Since we will be trying to associate a decision class to an object, 
  based on the attributes belonging to $B \subseteq C$, we are interested in those 
  $B-classes$ (classes induced by $B$) which correspond to classes induced by $d$. 
  This idea leads to the notion of the  \textit{positive region of the decision}. The set $POS_B(d)$, 
  called the \textit{B-positive region of d}, is defined as the set of all objects in $U$ such 
  that all their indistinguishable objects (under the knowledge in $B$) belong to its same class induced 
  by $d$.
  
  Taking for example the IS in table~\ref{tab_IS}, we can see that
  
  $$\begin{array}{lcc}
  POS_{\lbrace c_1 \rbrace}(d)     &=& \lbrace x_1,x_2,x_3,x_4 \rbrace\\
  POS_{\lbrace c_2 \rbrace}(d)     &=& \lbrace x_2,x_3,x_5,x_7 \rbrace\\
  POS_{\lbrace c_1, c_2 \rbrace}(d)&=& U
  \end{array}$$
 
\subsection{Reducts and Core}
  Given an information system $IS=(U,A)$ with condition attributes set $C$ and decision attributes set
  $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. A subset $B \subseteq C$ is a \textit{reduct} 
  of $IS$ relative to $D$ if
  \begin{enumerate}
  	\item $POS_B(D)=POS_C(D)$. \label{cond_1}
  	\item $B$ is a minimal subset (with respect to inclusion) satisfying condition~\ref{cond_1}.
  \end{enumerate}
  
  The intersection of all the reducts in an IS is called the \textit{core}, the attributes that cannot be
  eliminated without reducing the classification accuracy.
  
\subsection{Discernibility Matrix and Discernibility Function}
  The discernibility knowledge of the information system is commonly stored in a symmetric $|U| \times |U|$
  matrix called the \textit{discernibility matrix}. Each element $m_{ij}$ in the discernibility matrix 
  $M_{IS}$ is defined as   
  \begin{equation}
  	m_{ij}=\left\lbrace\begin{array}{cl}
  			\lbrace c \in C: c(x_i) \neq c(x_j) \rbrace & \mathrm{for~~}D(x_i) \neq D(x_j)\\
  			\emptyset 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation}  
  Here, $c(x_i)$ stands for the value of the condition attribute $c$ in the object (row) $x_i$, and 
  $$D(x_i) \neq D(x_j) \Rightarrow \exists d \in D~ |~ d(x_i) \neq d(x_j)$$ 
  where $d(x_i)$ stands for the value  of the decision attribute $d$ in the object $x_i$.
  
  Table~\ref{tab_DM} shows the discernibility matrix for the IS in table~\ref{tab_IS} as a lower triangular 
  matrix ($\emptyset$'s are omitted for clarity).
  
   \begin{table}[htb]
		\caption{Discernibility Matrix Example.} \label{tab_DM}
		\centering
 	\begin{tabular}{c|ccccccc}
 		$x \in U$ & 1 & 2 &  3 & 4 & 5 &  6 & 7\\
 		\hline
		1 &&&&&&&\\
		2 &&&&&&&\\
		3 & $c_1,c_2,c_3$ & $c_1,c_2,c_3$ &&&&&\\
		4 & $c_1,c_3$ & $c_1,c_2,c_3$ &&&&&\\
		5 & $c_1,c_2,c_3$ & $c_1,c_2,c_3$ &&&&&\\
		6 &&& $c_1,c_2$ & $c_1,c_3$ & $c_2,c_3$ &&\\
		7 & $c_1,c_2,c_3$ & $c_1,c_2,c_3$ &&&& $c_2,c_3$ &\\
 	\end{tabular}             
 \end{table}
  
  Once the discernibility matrix $M_{IS}$ is found, we can define the \textit{discernibility function} $f_{IS}$.
  This is a Boolean function of $n$ Boolean variables $c_1^*, c_2^*,...,c_n^*$, representing the presence of
  the corresponding attribute (True) or its absence (False) in $M_{IS}$. Here, the disjunction ($\vee$) and 
  conjunction ($\wedge$) operators have their common meaning.
  %TODO definir la operacion sobre un conjunto
  \begin{equation}
  	f_{IS}(c_1^*, c_2^*,...,c_n^*)=\wedge \lbrace \vee c_{ij}^* : 1 \leq j \leq i \leq |U|, 
  									m_{ij} \neq \emptyset \rbrace
  \end{equation}

  where $c_{ij}^*=\lbrace c^* : c \in m_{ij} \rbrace$. Only the lower triangular matrix from $M_{IS}$ is
  taken into consideration since $M_{IS}$ is symmetric. An equivalence between the prime implicants of
  $f_{IS}$ and all the reducts of $IS$ has been found and reported in~\citep{Pawlak07}.
  
  The discernibility function for the discernibility matrix in table~\ref{tab_DM}, after simplifying by 
  deleting, is  
  $$f_{IS}(c_1^*,c_2^*,c_3^*)=(c_1^* \vee c_2^* \vee c_3^*) \wedge (c_1^* \vee c_2^*) 
   \wedge (c_1^* \vee c_3^*) \wedge (c_2^* \vee c_3^*) $$
  
  From this example we can easily see that the only reduct (also the core) for this IS is $c_1$.
  
%\subsection{Attributes Dependency and Significance}
%  One important aspect of data analysis is the study of dependencies between attributes describing the 
%  objects. Intuitively, a subset of attributes $D$ depends totally on a set of attributes $B$, denoted 
%  $B \Rightarrow D$ if all attribute values from $D$ are uniquely determine by values of attributes
%  in $B$. Formally in RST we say that for $B,D \subset A$, $D$ depends on $B$ in a degree 
%  $k(0 \leq k \leq 1)$, denoted $B \Rightarrow _{k}D$ if
%  
%  \begin{equation}
%  	k=\gamma _B (D)=\frac{|POS_B(D)|}{|U|}
%  \end{equation}
%    
%  This is called the \textit{Positive Dependency Degree}, and it is the ratio of the number of objects belonging
%  to the positive region to the number of all objects in universe $U$. If $k=1$, $D$ depends totally on $B$, if
%  $0 < k < 1$, $D$ depends partially (in a degree $k$) on $B$ and if $k=0$, $D$ does not depends on $B$.
%  
%  Taking for example the positive regions from subsection~\ref{subsect_Pos}, we can see that
%  
%  $$\begin{array}{lcccc}  
%  \gamma _{\lbrace c_1 \rbrace} (d)&=&\frac{|U|}{|U|}&=&1\\
%  \gamma _{\lbrace c_2 \rbrace}(d)&=& \frac{|\lbrace x_1,x_2,x_3,x_4\rbrace|}{|U|}&=&\frac{4}{7} \\
%  \gamma _{\lbrace c_1, c_2 \rbrace}(d)&=&\frac{|U|}{|U|}&=&1
%  \end{array}$$
%  
%  We can express the \textit{significance} of feature $c \in B$ upon $D$ as
%  
%  \begin{equation}
%  	SIG(c,B,D)=\gamma _B (D)-\gamma _{B-\lbrace c \rbrace} (D)
%  \end{equation}
%  
%  From  the previous example:
%  
%  $$\begin{array}{lcccccc}
%  SIG(c_1,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_2 \rbrace}(d)
%  										&=&1-\frac{4}{7}&=&\frac{3}{7}\\
%  SIG(c_2,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_1 \rbrace}(d)
%  										&=&1-1&=&0
%  \end{array}$$
%  
%  From this it follows that attribute $c_1$ is indispensable, but attribute $c_2$ can be dispense when 
%  considering the dependency between the decision attribute $d$ on condition attributes $c_1$ and $c_2$.
%  
%  Finally, we would like to say that an alternative definition of reduct from dependency exist. We say 
%  that a subset $B \in C$ is a reduct of $C$ if it is a minimal set (with respect to inclusion) satisfying 
%  the condition $\gamma _B(D)=\gamma _C(D)$.

\subsection{Binary Discernibility Matrix}
  The \textit{Binary Discernibility Matrix} is a binary table representing discernibility between pairs of 
  objects. This is another representation of the information in $M_{IS}$. In the binary discernibility
  matrix, columns are single condition attributes and rows are objects pairs belonging to different classes.
  The discernibility element $m(i, j, c)$ between two objects $x_i$ and $x_j$ by using a single condition 
  attribute $c \in C$ is given in a binary representation, such that:
  
  \begin{equation}
  	m(i, j, c)=\left\lbrace\begin{array}{cl}
  			1 & \mathrm{for~~}c(x_i) \neq c(x_j),D(x_i) \neq D(x_j)\\
  			0 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation} 
  
  Table~\ref{tab_BDM} shows the binary discernibility matrix for the information system of table~\ref{tab_IS}.  
  
  \begin{table}[htb]
		\caption{Binary Discernibility Matrix Example.} \label{tab_BDM}
		\centering
 	\begin{tabular}{cccc}
 		& $c_1$ & $c_2$ & $c_3$\\
 		\hline
		$x_1,x_3$ & 1 & 1 & 1 \\
		$x_1,x_4$ & 1 & 0 & 1 \\
		$x_1,x_5$ & 1 & 1 & 1 \\
		$x_1,x_7$ & 1 & 1 & 1 \\
		$x_2,x_3$ & 1 & 1 & 1 \\
		$x_2,x_4$ & 1 & 1 & 1 \\
		$x_2,x_5$ & 1 & 1 & 1 \\
		$x_2,x_7$ & 1 & 1 & 1 \\
		$x_3,x_6$ & 1 & 1 & 0 \\
		$x_4,x_6$ & 1 & 0 & 1 \\
		$x_5,x_6$ & 0 & 1 & 1 \\
		$x_6,x_7$ & 0 & 1 & 1 
 	\end{tabular}             
  \end{table}
  
\subsection{Simplified Discernibility Matrix}
  The \textit{Simplified Discernibility Matrix} is a reduced version of the discernibility matrix after
  eliminating supersets and repeated cells in $M_{IS}$. This new discernibility matrix has the same reducts
  as the original one~\citep{Yao09}. An equivalent concept exists in the Testor Theory, called 
  \textit{Basic Matrix}. The basic matrix was proven to have the same TT as the original information
  system~\citep{Lazo01}. Table~\ref{tab_SDM} shows the basic matrix for the
  discernibility matrix from table~\ref{tab_BDM}.
  
     \begin{table}[htb]
		\caption{Basic Matrix Example.} \label{tab_SDM}
		\centering
 	\begin{tabular}{ccc}
 		$c_1$ & $c_2$ & $c_3$\\
 		\hline
		1 & 0 & 1 \\
		1 & 1 & 0 \\
		0 & 1 & 1
 	\end{tabular}             
 \end{table}

\section{Related Work}\label{relatedWork}
  In this section, we will be first discussing heuristic algorithms for reduct computation. Some of these 
  algorithm are capable of finding several reducts and others are intended to obtain a single \textit{minimal} 
  reduct. Then, two kind of algorithms for computing the complete set of reducts will be exposed: those 
  from RST and those from TT. Researches in TT have been focused on algorithms for finding all the typical 
  testors. Finally, we will make a review of parallel accelerations reported in the literature.  
  
  In figure~\ref{fig_Tax1} we propose a taxonomy of the reported algorithms for computing a single reduct,
  and in figure~\ref{fig_Tax2}, a taxonomy of the reported algorithms for computing all the reducts. 
  This classification corresponds to the sequence that we will be following throughout our review of the 
  state of the art.
   
  \begin{figure}[htb] 
  \center
    \pstree[treesep=2cm]{\Tr{\PSBL{Algorithms for Computing a single Reduct}}}{
       \pstree[treesep=3cm]{\Tr{\PSBS{Psudo--minimal}}}{
       		\pstree[levelsep=7em,treesep=.5cm]{\Tr{\PSBS{Heuristic}}}{
       			\pstree[levelsep=3.5em]{\Tr{\PSBS{FPGA}}}{
       				\small \Shortstack[l]{
       					\citep{Tiwari11}\\
       					\citep{Tiwari13}
       				}
       			}
       			\pstree[levelsep=3.5em]{\Tr{\PSBS{Sequential}}}{
       				\small \Shortstack[c]{
       					\hyperref[quickreduct]{\textsc{quickreduct}}\\
       					%EBR\\
       					\citep{Yang08}
       				}
       			}
       			\pstree[levelsep=3em]{\Tr{\PSBS{Subdivision}}}{
       				\small \Shortstack[l]{
       					\hyperref[FSDCRS]{FSDC-RS}\\
       					\hyperref[FSDCRS]{FSDC-HS}	
       				}
       			}
       		}
       		\pstree[levelsep=6em]{\Tr{\PSBS{Evolutionary}}}{
       			\small \Shortstack[c]{
       				\citep{Wroblewski95}\\
       				~~\citep{Bjorvand97}\\
       				\hyperref[AntRSAR]{AntRSAR}\\
       				\hyperref[GenRSAR]{GenRSAR}\\
       				\hyperref[RSFSACO]{RSFSACO}
       			}
       		}
       		}
       \pstree[levelsep=3.5em]{\Tr{\PSBS{Minimal}}}{
       		\small \Shortstack[c]{
       			\citep{Lin04}\\
       			\hyperref[RSARSAT]{RSAR-SAT}		
       		}
       }
  }
  \caption{Taxonomy of algorithms for computing a single reduct.}
  \label{fig_Tax1}
  \end{figure}
  	
  \begin{figure}[htb] 
    	\center
    \pstree{\Tr{\PSBL{Algorithms for Computing all the Reducts}}}{%
    \pstree{\Tr{\PSBS{Rough Sets}}}{%
    		\pstree[levelsep=5em]{\Tr{\PSBS{Subdivision}}}{
    			\small \Shortstack[l]{
    				\hyperref[SRGonCRS]{SRGonCRS}\\
    				\hyperref[DT]{DT}\\
    				\hyperref[DT]{DISC FUNCTION}\\
    				\hyperref[DT]{CANDIDATE REDUCTS}	  
       		}
    		}
       	\pstree[levelsep=4.5em]{\Tr{\PSBS{Sequential}}}{
       		\small \Shortstack[l]{
       			\citep{Starzyk99}\\
       			\hyperref[RGonCRS]{RGonCRS}\\
       			\hyperref[RSARSAT]{RSAR-SAT}						  
       		}
       	}
  }
  	\pstree{\Tr{\PSBS{Testor Theory}}}{%
  		\pstree[levelsep=3.5em]{\Tr{\PSBS{FPGA}}}{
  			\small \Shortstack[c]{\citep{Cumplido06}\\
  					 			  BT~\citep{Rojas07}}
  		}
       	\pstree[levelsep=5.5em]{\Tr{\PSBS{Sequential}}}{
      			\small \Shortstack[l]{\hyperref[BT]{BT \& TB}\\
      								  \hyperref[LEX]{LEX} \\
      								  \hyperref[CTEXT]{CT\_EXT} \\
      								  \hyperref[BR]{BR} \\
      								  \hyperref[YYC]{YYC} }
      	}
  }}
  \caption{Taxonomy of algorithms for computing all the reducts.}
  \label{fig_Tax2}
  \end{figure}
 
% \newcolumntype{L}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% \begin{table}[t]
%	\caption{reduct computation algorithms.} \label{tab_Alg}
%	\centering
% 	\begin{tabular}{l||L{13cm}}
% 		\multicolumn{1}{c||}{Codename} &  \multicolumn{1}{c}{Publication} \\
% 		\hline \hline
%		\textsc{quickreduct} 	&  Rough set-aided keyword reduction for text 
%								   categorization~\citep{Chouchoulas01} \\
%		YangLiHuang 				&  An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility
%				 				   Matrix~\citep{Yang08}\\
%		EBR						&  A rough set--aided system for sorting WWW bookmarks~\citep{Jensen01}\\
%		RSFSACO					&  A rough set approach to feature selection based on ant colony 
%								   optimization~\citep{Chen10}\\
%		AntRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
%		GenRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
%		BjorvandKomorowski		&  Practical applications of genetic algorithms for efficient reduct 
%								   computation~\citep{Bjorvand97}\\
%		\hline		   
%		BTHW						&  FPGA Based Architecture for Computing Testors~\citep{Rojas07}\\
%		BruteForce				&  On the Design and Implementation of a High Performance Configurable
%								   Architecture for Testor Identification~\citep{Cumplido06}\\
%		TiwariKothariShah		&  FPGA Implementation of a Reduct Generation Algorithm based on Rough 
%								   Set Theory~\citep{Tiwari13}\\
%		TiwariKothari			&  Architecture and Implementation of Attribute Reduction Algorithm Using 
%								   Binary Discernibility Matrix~\citep{Tiwari11}\\
%		DM-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
%		DT-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
%		SRGonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\
%		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
%								   composition~\citep{Jiao10}\\
%		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
%								   composition~\citep{Jiao10}\\
%		\hline
%		TB \& BT					&  BT and TB algorithms for computing all irreducible testors~\citep{Ruiz85}\\
%		CT--EXT					&  CT--EXT: an algorithm for computing typical testor set~\citep{Sanchez07}\\
%		LEX						&  LEX: a new algorithm for the calculus of typical
%								   testors~\citep{Santiesteban03}\\
%		BR						&  BR: A new method for computing all typical testors~\citep{Lias09}\\
%		YYC						&  YYC: A Fast Performance Incremental Algorithm for Finding Typical
%								   Testors~\citep{Alba14}\\
%		RSAR	--SAT				&  Finding rough and fuzzy--rough set reducts with SAT~\citep{Jensen14}\\
%		Expansion Algorithm		&  Reduct generation in information systems~\citep{Starzyk99}\\
%		GonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\	
% 	\end{tabular}             
% \end{table}
 
\subsection{Algorithm Finding a Single Reduct}
%TODO cambiar lo de la significancia
  The algorithm presented in~\citep{Chouchoulas01} \setword{\textsc{quickreduct}}{quickreduct} starts with 
  an empty set of attributes and adds, one at a time, the attribute having the highest significance. 
  This greedy algorithm evaluates the significance of an attribute by the number of objects added to 
  the positive region after its inclusion.  
  A similar approach is the Johnson Reducer~\citep{Johnson74}, first introduced in RST in \citep{Ohrn00}.
  This simple greedy algorithm begins with an empty set of attributes evaluating each conditional attribute in the
  discernibility function according to a heuristic measure. In the simplest case, those attributes with highest 
  appearance frequency within the logical clauses of the discernibility function, are considered to be more
  relevant. Works in~\citep{Nguyen97} and~\citep{Wang01} use alternative heuristic functions for guiding the
  search. 
  Variations of this algorithm~\citep{Wang01} and~\citep{Yang08} use the discernibility matrix instead of the
  discernibility function.
  The algorithm presented in \citep{Zhong01} starts from the core (since it must be contained in every reduct) and
  follows a similar procedure adding selected attributes. This optimization may be impractical for large datasets
  \citep{Jensen14} since the core must be computed a priori.
  
  The method presented in~\citep{Jiao10} improves the efficiency of computing reducts by means of subdivision 
  of the dataset. The original dataset is broken down into a master-table and several sub-tables that are simpler,
  more manageable. Two algorithms are proposed (\setword{FSDC-RS}{FSDCRS} and FSDC-HS) using 
  different decomposition strategies. Results are then joined together in order to solve the original dataset. 
    
  Special attention deserves the approaches using genetic algorithms to discover locally shortest reducts. Although 
  these algorithms do not guarantee finding globally shortest reducts, many reducts may be found in a determined
  time. A good point in this approach is the use of the fitness function to guide the search down to a set of 
  reducts with the desired properties. The algorithm reported in~\citep{Wroblewski95} encodes candidates as bit 
  strings with a positional representation of attributes presence in the candidate set. The fitness function
  depends on the number of attributes in the subset, penalizing strings with a large number of bits set. The 
  second optimization parameter is the number of objects classified by the given candidate. The reduct should 
  discern as many objects as possible. \cite{Jensen03} also introduced a simple algorithm 
  (\setword{GenRSAR}{GenRSAR}), which uses a genetic search strategy in order to determine reducts.
  
  Other evolutionary approaches to reduct computation include Ant Colony Optimization~\citep{Jensen03} 
  (\setword{AntRSAR}{AntRSAR}) and~\citep{Chen10} (\setword{RSFSACO}{RSFSACO}); and Particle Swarm 
  Optimization~\citep{Wang07}.
    
  In~\citep{Lin04}, a heuristic is followed to find a short reduct. This first reduct is used to limit the search
  space, in order to only consider those attributes combinations with lower cardinality. 
  The main drawback of this algorithm is that the second step searches for reducts by checking all possible 
  $s$-subtables of the whole database. A $s$-subtable means a subtable whose conditional attributes set have 
  size $s$. In other words, it is a decision table with conditional attribute of size $s$ plus the decision
  attributes of original table. This final process uses no pruning strategy and explores the combinatorial
  possibilities of attribute combinations, which is unfeasible in most cases.
  
  Although originally intended for computing a single minimal reduct, the algorithm proposed in~\citep{Jensen14}
  (\setword{RSAR-SAT}{RSARSAT}) may be modified in order to obtain all the reducts in an Information System. 
  The method introduced in this work
  reduces the problem of finding a reduct from the discernibility function to the SAT problem~\citep{Davis62}. 
  The boolean function generated in this way is always satisfied since the complete set of attributes is a trivial
  solution.
  
    
\subsection{Algorithms for all Reducts and all Typical Testors Computation}
  One of the first algorithm designed to overcome the exponential complexity (regarding
  the number of features) of the problem of finding all the TT, was 
  proposed by \cite{Ruiz85}. This algorithm, called \setword{BT}{BT},
  codified a subset of features as a binary word with as many bits as features in the 
  dataset. A 0 represents the absence of the corresponding feature in the current
  subset while a 1 represents its inclusion. This way, candidates subsets are evaluated
  in the natural order induced by binary numbers. The pruning process in the
  search space is based on the minimal condition of TT and a convenient sorting
  of the basic matrix associated to the dataset. Finally, 
  testors found by BT algorithm must be filtered in order to remove any non-TT.
  In \citep{Shulcloper95b} a new algorithm (REC) is presented.
  The main drawback of REC is that it works directly over the dataset (instead of the
  basic matrix), handling a huge amount of superfluous information. \cite{Ayaquica97}
  presented the algorithm CER directed to solve this problem by using a different traversing
  order. 
	
  Then, \cite{Santiesteban03} proposed a new algorithm
  called \setword{LEX}{LEX}. Main ideas behind LEX are a new traversing order of candidates (which resembles the
  lexicographical order in which string characters are compared) and the concept of gap. In LEX
  the typical condition is verified first and only for those potentially TT, the testor 
  condition is checked. %This way, the out-coming testors from this algorithm are always typical.
  The concept of gap allows us, once obtained a TT (or a not testor) candidate including 
  the last feature in the dataset, to avoid the evaluation of any subset of this candidate.
	
  \cite{Sanchez07} proposed the \setword{CT\_EXT}{CTEXT} algorithm for computing all the
  TT. Following a traversing order similar to that in LEX, this algorithm searches for
  testors without verifying the typical condition. This way, a larger number of candidates are 
  evaluated, in comparison to LEX; but the cost of each evaluation is lower. Results from experiments
  show that CT\_EXT is faster than the previous existing algorithms for most datasets. Then, \cite{Lias09}
  presented the \setword{BR}{BR} algorithm, a Recursive algorithm based on 
  Binary operations. BR is very similar to LEX in its bones but its recursive nature encloses a great
  gain. Given a candidate subset, the remaining features are tested a priori and those being rejected are
  excluded from subsequent evaluations. \cite{Sanchez10} presented a cumulative
  procedure for the CT\_EXT algorithm. This fast-CT\_EXT implementation reduces drastically the runtime
  for most datasets at no extra cost. In \citep{Lias13} the
  gap elimination and column reduction are added to BR. This fast-BR algorithm is, no doubt the one 
  evaluating the minimum number of candidates in the state of the art. The main drawback of fast-BR and 
  BR is, as in LEX, the high cost of evaluating the typical condition for each candidate. 
 
  Recently, a new internal typical testor--finding algorithm (\setword{YYC}{YYC}) was proposed by~\cite{Alba14}. Although 
  they claim that this algorithm verify less candidates than previous alternatives, two weak points should
  be addressed. First, BR is not included in comparisons; and second, the evaluation cost for a candidate
  in YYC is high compared to that of previous algorithms. YYC verifications involve calculations of the 
  Hamming weight.

  A method for the computation of all the reducts in an Information System is proposed in
  \citep{Starzyk99,Starzyk00}.
  This is a divide and conquer approach. On each step, the absorption laws are applied over the incoming
  discernibility matrix to obtain a basic matrix. Then, the strong equivalent attributes are compressed
  (which is a local reduction of columns). The most discerning attribute is selected (in the same way as 
  Johnson's reducer does) and the problem is divided into two sub-problems: 
  \begin{itemize}
  \item Finding reducts containing the selected attribute. Thus a recursive function is called with a new basic 
  matrix, having only those rows where the selected attribute does not appear.
  \item Finding reducts that do not contain the selected attribute. Thus a recursive function is called with a new 
  discernibility matrix, removing the column corresponding to the selected attribute.
  \end{itemize}
  The base case in the recursion is reached when each attribute in the incoming discernibility matrix appears 
  in a single clause. Finally a set of super-reducts is obtained and supersets must be removed in order to obtain 
  the final reduct set.
  Notice that this algorithm is oriented to the binary discernibility function, then 
  terms such as discernibility and basic matrix, rows and columns are not used in the paper. The
  algorithm is presented in an iterative fashion and its recursive nature is not cleared expressed.
  
  \cite{WangP07} proposed a new algorithm for computing all the reducts \setword{RGonCRS}{RGonCRS}. 
  Even though this algorithm 
  was developed independently and reported two years before to the one reported in~\citep{Lias09}, it is very 
  similar to BR. Notice that this is a rough set approach to the problem and the nomenclature is totally 
  different to that of BR. Essentially, every proposition supporting the pruning process in \citep{WangP07} 
  have an equivalent proposition in \citep{Lias09}. The main differences of RGonCRS with BR are:
  \begin{itemize}
  \item It works directly over the dataset instead of the basic matrix.
  \item It starts searching the core and looks for reducts as supersets of the core.
  \item A recursive implementation, instead of the iterative solution used in BR, is proposed.
  \item During the algorithm execution, contributing attributes are sorted as in the Johson reducer.
  \item A second algorithm \setword{SRGonCRS}{SRGonCRS} is proposed for subdividing the dataset and the reducts are
  		incrementally found.
  \end{itemize}
  
  Different variants (\setword{DT}{DT}, DISC FUNCTION and CANDIDATE REDUCTS) for decomposition of a reduct 
  computation problem are discussed and proposed in~\citep{Strakowski08}.

\subsection{Parallel Accelerations}

  A parallel acceleration of the algorithm presented in~\citep{Yang08}, for reduct generation from a binary
  discernibility matrix, was developed in~\citep{Tiwari11,Tiwari12}. This FPGA implementation computes a 
  single reduct. A real application of an object identification system for an intelligent robot is presented.
  In~\citep{Tiwari13} a \emph{quick reduct} algorithm, similar to those presented in~\citep{Chouchoulas01}, 
  is proposed and implemented in a hardware fashion. A recent work from these authors~\citep{Tiwari14}, 
  shows a thorough survey of FPGA applications in rough set reduct computation.

  From the Typical Testors theory, several attempts have been made to overcome the problem 
  complexity by means of FPGA implementations of algorithms. In a first work~\citep{Cumplido06}, an 
  FPGA-based brute force approach for computing testors was proposed. This first approach did 
  not take advantage of dataset characteristics to reduce the number of candidates to be tested; 
  thus all $2^n$ combinations of $n$ features have to be tested. Then, in \citep{Rojas07} a hardware 
  architecture of the BT algorithm for computing typical testors was implemented. 
  This algorithm uses a candidate pruning process for avoiding many unnecessary candidate evaluation, 
  reducing the number of verifications of the typical testor condition. These two previous works computed 
  a set of testors on the FPGA device whilst the typical condition was evaluated afterwards by the 
  software component in the hosting PC. Thus, in~\citep{Rojas12} a hardware-software platform for 
  computing typical testors that implemented the BT algorithm, similar to \citep{Rojas07}, was proposed; 
  but it also included a new module that eliminates most of the non typical testors before transferring them to 
  a host software application for final filtering. 
	
	%TODO estos trabajos de Wroblewski hay q revisarlos bien pq parece falso lo de los GA paralelos
  In~\citep{Wroblewski98}, a parallel variant of the algorithm proposed in~\citep{Wroblewski95} is presented.
  Developments in parallel implementations of genetic algorithms are exploited to provide a speedup for the 
  problem of finding reducts.
  
  In~\citep{Grzes13,Kopczynski14}, an FPGA application for a single reduct computation is presented. Although
  authors claim that a huge acceleration is achieved, some weak points have to be mentioned. Experiments presented 
  in~\citep{Kopczynski14} to validate their results are performed over a small dataset which in our experience 
  does not implies its applicability to larger cases where such acceleration is needed. On the other hand, 
  runtime estimations for FPGA component executions are made by means of a oscilloscope without taking into 
  account communication overhead.
%TODO añadir una tabla comparativa de los algoritmos.
\clearpage
\section{Research Proposal}\label{ResearchProposal} 
 In this section we present the justification and motivation, the research questions, the objectives and the
 expected contributions of this research. We also include a detailed methodology and the schedule for reaching
 our objectives.

\subsection{Justification and Motivation}\label{Justification}
  RST can be used to reduce the number of attributes in a dataset without relevant information lost. 
  Therefore, there has been a lot of research on finding reducts, particularly, shortest 
  reducts~\citep{Jensen14}. %TODO exponer la importancia x muchos autores
  
  Heuristic methods such as~\citep{Chouchoulas01,Jensen04,Zhong01} are fast alternatives for finding 
  reducts but they do not guarantee a shortest reduct. Stochastic approaches~\citep{Wroblewski95,Jensen03,
  Chen10,Wang07} still do not guarantee finding a shortest reduct, as we have seen before. Techniques 
  for finding all the reducts~\citep{Starzyk99,WangP07} can, of course, find the
  shrotest reducts but with a high computational effort.
  
  The motivation of this research is the development of algorithms for computing all the reducts and 
  globally shortest reducts in information systems. These are problems with exponential complexity, which 
  make every attempt for reducing execution time, a challenging task.
  Our proposal must be competitive with the state of 
  the art algorithms in the general case and faster in some specific cases. The main arena for comparison 
  will be a large set of synthetic, randomly generated datasets and benchmarking datasets from~\citep{Bache13}. 
  Practical applications of reducts in supervised classification are beyond our goals.
  
  The results of this research will impact the feature selection methods specially in large datasets.
  Nowadays, data is automatically collected, thus generating huge databases in almost every field. The 
  current growth of the size of data and the number of existing databases, is the justification for our
  research on fast algorithms for dimensionality reduction without classification accuracy lost.  
  
\subsection{Research Questions}\label{ResearchQuestions} 
  Throughout our state of the art review, we noticed that there is not a fastest algorithm for finding reducts 
  on any dataset. Algorithms reported in the literature use different strategies for traversing and pruning 
  the whole search space. Consequently, some strategies are better suited for some datasets while they are time
  consuming for some others. This leads us to our first research question:
  
\begin{quote}
  \emph{Is there a relationship between some properties of the basic matrix and the runtime 
  		of traversing strategies for finding reducts in information systems?}
\end{quote}
  		
  We will be considering those properties that can be extracted from the basic matrix by traversing their 
  cells just one time. Lets take for instance, the minimum and maximum number of attributes in a cell, the 
  core or the mean number of attributes per cell. Other properties that require more complex operations to 
  be extracted such as the number of reducts, the cardinality of the shortest and largest reducts, etc; 
  will not be considered.
  
  From this research question we formulate the following hypothesis:
  
\begin{quote}  
  \emph{There is a relationship between the properties of the basic matrix and the runtime 
  		of traversing strategies for finding reducts in information systems}
\end{quote}
  		
%  Several attempts for the decomposition of the original problem of the complete reducts set computation have 
%  been made~\citep{Strakowski08,Jiao10,Kopczynski14}. The main disadvantage of the problem decomposition or
%  parallelization is the strong dependency between the speed--up of the method used and the particularities of 
%  the dataset~\citep{Strakowski08}. This leads us to our second scientific question:
%  
%\begin{quote}
%  \emph{Is there a relationship between the external properties of the discernibility matrix and the 
%  		speed--up of the decomposition method used for finding reducts of an information system 
%  		in a parallel environment?}
%\end{quote}
%
%  From this research question we formulate the following hypothesis:
%    
%\begin{quote}
%  \emph{There is a relationship between the external properties of the discernibility matrix and the 
%  		speed--up of the decomposition method used for finding reducts of an information
%  		system in a parallel environment}
%\end{quote}
  
  The more sophisticated a traversing strategy is, the less number of candidates attributes sets are 
  evaluated to verify whether they are reducts or not. Unfortunately, a more sophisticated traversing 
  strategy has usually a higher computational cost. This trade-off between the number of evaluated 
  candidates and their evaluation cost, leads us to our second research question:
  
\begin{quote}
  \emph{Can be reduced the runtime for computing reducts in information systems by 
  		dynamically changing the traversing strategy?}
\end{quote}
  By dynamically changing we mean the change of the traversing strategy during the reduct computation.
  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{The runtime for computing reducts in information systems can be reduced by 
  		dynamically changing the traversing strategy}
\end{quote}

  Based on these two scientific questions we can formulate the main hypothesis for our research:
  
\begin{quote}
  \emph{Using some properties of the basic matrix, and dynamically changing the traversing 
  		strategy, we can design new algorithms for computing reducts in information systems; which are
  		faster than the state of the art alternatives in a kind of datasets}
\end{quote}  

\subsection{Research Objectives}\label{Goals} 
  The main objective in our research is the \emph{development of new algorithms for computing reducts in
  information systems; which will be comparable to state of the art algorithms 
  in most datasets, and faster in some specific kinds of datasets}. 
  
  These algorithms will use some properties 
  of the basic matrix to conveniently select the traversing strategy for the search space. We will explore two
  variants of this problem, the problem of computing all the reducts and the problem of computing globally 
  shortest reducts. The problem of finding shortest reducts has also 
  exponential complexity~\citep{Lin04} but different pruning rules could be used.
  
  Our specific objectives are:
  \begin{enumerate}
  \item Finding a relationship between some properties of the basic matrix and the fastest 
  		traversing strategy for computing all the reducts.
  		  	
%  \item Find a relationship between some properties of the discernibility matrix and the fastest 
%  		decomposition method for computing all the reducts.
  		
%  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
%  		for computing all the reducts.
  
  \item Developing a new algorithm for computing all the reducts.
  		
  \item Finding a relationship between some properties of the basic matrix and the fastest 
  		traversing strategy for computing globally shortest reducts.
  		  	
%  \item Finding a relationship between some properties of the discernibility matrix and the fastest 
%  		decomposition method for computing globally shortest reducts.
  		
%  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
%  		for computing globally shortest reducts.
  
  \item Developing a new algorithm for computing globally shortest reducts.
%  
%  \item Redesign and implement in a hardware fashion the proposed algorithms in order to evaluated the
%  		runtime reduction achieved by these accelerations.  		
  \end{enumerate}

\subsection{Expected Contributions}\label{Contributions} 
  %Expected contributions from this research are:
  \begin{itemize}
%  \item A survey on algorithms for reducts computations. The novelty of this contribution relay in two
%  		aspects. First, algorithms from Rough Sets Theory and Testor Theory will be exposed under a 
%  		unified theoretical framework. Second, propositions supporting algorithms reported in the literature will
%  		be explained using the same theoretical basis. 
  \item A new algorithm for computing all the reducts, which will be comparable to 
  		state of the art algorithms in most datasets, and faster in some specific kinds of datasets.
  \item A new algorithm for computing shortest reducts, which will be comparable to 
  		state of the art algorithms in most datasets, and faster in some specific kind of datasets.
  \item A meta-characterization of algorithms' efficiency in relation to some properties of the
  		basic matrix associated to a dataset.
  \item Software and hardware implementations for computing both, all the reducts and shortest reducts.
  \end{itemize}


\subsection{Methodology}\label{Methodology} 
\begin{enumerate}
	\item Finding a relationship between some properties of the basic matrix and the fastest 
  		  traversing strategy for computing all the reducts.\label{task1_all}
	\begin{itemize}
  		\item Generating a set of random datasets, systematically covering the space of possible combinations
  			  of properties of basic matrices (factorial design). These properties are, for instance, the 
  			  density of ones in the basic matrix, the minimum and maximum number of ones in a row, the 
  			  standard deviation of the density of ones in rows and columns, etc.
  		\item Implementing the main traversing strategies reported in the literature for the computation of all
  			  reducts.
  		\item Generating an information system with the properties of each basic matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extracting a relevant subset of properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation. We will use
  			  Rough Set Theory for this purpose.
  		\item Proposing a new algorithm for computing all the reducts using the rules found in
  			  the previous step.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets~\citep{Bache13}.
  	\end{itemize}
  	\item Finding a relationship between the traversed space and the expected cost of traversing strategies.
  	\label{task2_all}
  	\begin{itemize}
  		\item Implementing the main traversing strategies reported in the literature for the computation of all
  			  reducts in such a way that we can collect statistics for every execution stage.
  		\item Making a statistical description  of strategies' runtime cost over synthetic and benchmarking
  			  datasets.
  		\item Finding a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Developing a new algorithm for computing all the reducts in information systems.
  	\begin{itemize}
  		\item Proposing a new algorithm for computing all the reducts based on the relations found in 
  			  steps~\ref{task1_all} and~\ref{task2_all}.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	
  	\item Finding a relationship between some properties of the basic matrix and the fastest 
  		  traversing strategy for computing globally shortest reducts.\label{task1_short}
	\begin{itemize}
  		\item Implementing the main traversing strategies reported in the literature for the computation of 
  			  minimal length reducts.
  		\item Generating an information system with the properties of each basic matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extracting the relevant subset of properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Proposing a new algorithm for computing shrotest reducts using the rules found in
  			  the previous step.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Finding a relationship between the traversed space and the expected cost of traversing strategies.
  	\label{task2_short}
  	\begin{itemize}
  		\item Implementing the main traversing strategies reported for the computation of globally shortest 
  			  reducts in such a way that we can collect statistics for every execution stage.
  		\item Making a statistical description  of strategies' runtime cost over synthetic and benchmarking.
  		\item Finding a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Developing a new algorithm for computing shortest reducts in information systems.
  	\begin{itemize}
  		\item Proposing a new algorithm for computing shortest reducts based on the relations found in 
  			  steps~\ref{task1_short} and~\ref{task2_short}.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
%  	\item Implement in a hardware fashion the proposed algorithms.
%  	\begin{itemize}
%  		\item Redesign and implement in a hardware fashion the proposed algorithm for computing
%  			  all the reducts in information systems.
%  		\item Evaluate the implemented algorithm over synthetic and benchmarking datasets.
%  		\item Redesign and implement in a hardware fashion the proposed algorithm for computing
%  			  shortest reducts in information systems.
%  		\item Evaluate the implemented algorithm over synthetic and benchmarking datasets.
%  	\end{itemize}
  	\item Finally, the proposed algorithms will be redesigned and implemented in a hardware fashion in order to 
  		  evaluate the speed up that can be obtained.
\end{enumerate}

\clearpage 
\subsection{Schedule}
  Table~\ref{tab_Schedule} shows the schedule of the main tasks that will be carried out throughout this research.
 \begin{table}[h!]
		\caption{Research schedule (quarterly\protect\footnotemark).} \label{tab_Schedule}
		\centering
 	\begin{tabular}{|p{8cm}|c|c|c|c|c|c|c|c|c|c|c|c|}
 		\hline
		\multicolumn{1}{|c|}{\multirow{3}{*}{Task}} & \multicolumn{12}{c|}{Quarters}\\
 		\cline{2-13}
		 & 2014 & \multicolumn{3}{c|}{2015} & \multicolumn{3}{c|}{2016} & \multicolumn{3}{c|}{2017}
		 & \multicolumn{2}{c|}{2018} \\
 		\cline{2-13}
		 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
		\hline
		Literature review &\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}\\
		\hline
		Writing the research proposal &\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Critical study of algorithms for computing all the reducts in information systems
		&\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Implementation of algorithms for computing all the reducts in information systems
		&&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Development of a new algorithm for computing all the reducts in information systems
		&&&&\cellcolor[gray]{0.9}&&&&&&&&\\
		\hline
		Critical study of algorithms for computing shortest reducts in information systems
		&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&&&\\
		\hline
		Implementation of algorithms for computing shortest reducts in information systems
		&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&&&\\
		\hline
		Development of a new algorithm for computing shortest reducts in information systems
		&&&&&&&\cellcolor[gray]{0.9}&&&&&\\
		\hline
		Critical study of hardware accelerations of algorithms for computing reducts in information systems
		&&&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Designing and implementing in hardware the proposed algorithms
		&&&&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Experimental set-up &\cellcolor{blue}&\cellcolor{blue}&&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&\\
		\hline
		Experiments run &&&\cellcolor{blue}&\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&
		\cellcolor[gray]{0.9}&&&\\
		\hline
		Writing papers &\cellcolor{blue}&&\cellcolor{blue}&&\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&&
		\cellcolor[gray]{0.9}&&&\\
		\hline
		Writing dissertation &&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Submit final draft of dissertation to supervisors &&&&&&&&&&\cellcolor[gray]{0.9}&&\\
		\hline
		Submit final version of dissertation to the PhD committee &&&&&&&&&&&\cellcolor[gray]{0.9}&\\
		\hline
		
 	\end{tabular}             
 \end{table}
 
 \footnotetext{Quarters are: [January-April], [May-August] and [September-December]. Schedule starts in 
 			   September 2014, according to the admission of the student in the PhD. program.}
	  	
%\section{Publication Plan}\label{PubPlan}
%  In this section we present our publication plan. Results from this research are to be submitted indistinctly 
%  to the following journals or congresses, according to the advisor’s recommendations.
%  
%  Main journals:
%  \begin{itemize}
% 	\item Information Sciences\footnote{http://www.journals.elsevier.com/information-sciences/}. IF: 3.893
% 	\item Expert Systems with 
% 		  Applications\footnote{http://www.journals.elsevier.com/expert-systems-with-applications/}. IF: 1.965
% 	\item IEEE Transactions on Knowledge and Data
% 		  Engineering\footnote{http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69}. IF: 1.815
% 	\item International Journal of Advanced Computer Science and
% 		  Applications\footnote{http://thesai.org/Publications/IJACSA}. IF: 1.32
%  \end{itemize}
% 
%  Main congresses:
%  \begin{itemize}
% 	\item Rough Sets and Current Trends in Computing
% 	\item Rough Sets and Intelligent Systems Paradigms
% 	\item CIARP: Iberoamerican Congress on Pattern Recognition
% 	\item Mexican Conference on Pattern Recognition
%  \end{itemize}
%  
%  In table~\ref{tab_PP} we present the estimated dates for our proposed publications. The first publication will
%  discuss the results of the first experiment. In our second publication we will make a review of the state of 
%  the art in rough sets reduct computation. Third and fourth publications will present the results from the last
%  three experiments as shown in table~\ref{tab_PP}.
%  
%     \begin{table}[htb]
%		\caption{Publication plan.} \label{tab_PP}
%		\centering
% 	\begin{tabular}{c||l|l}
% 		\# & \multicolumn{1}{c|}{Date} & \multicolumn{1}{c}{Results}\\
% 		\hline \hline
%		1 & July 2015 & First Experiment (\ref{exprimet1})\\
%		2 & December 2015 & State of the art survey \\
%		3 & July 2016 & Second Experiment (\ref{exprimet2})\\
%		4 & July 2017 & Third and fourth Experiments (\ref{exprimet3} \& \ref{exprimet4})\\
% 	\end{tabular}             
% 	\end{table}

%-------------------------------------------------------------------------------
% Bibliography
%-------------------------------------------------------------------------------
\newpage 
\bibliography{mybib}{}
\bibliographystyle{authordate1}
\end{document}
