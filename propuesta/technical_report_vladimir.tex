%-------------------------------------------------------------------------
%\documentclass[11pt,authoryear]{elsarticle}
\documentclass[authoryear,11pt]{elsarticle}


\usepackage{times}              		% la letra
\usepackage{graphicx}           		% para manejar imagenes
\usepackage{subfigure}          		% para manejar subfiguras
\usepackage{tabularx}		   		% para ajustar el ancho de las columnas
\usepackage[margin=2.5cm]{geometry}	% Change margins
%-------------------------------------------------------------------------
% Se definen los margenes para el documento
%\evensidemargin=0in
%\topmargin=0.5in \oddsidemargin=0in \evensidemargin=0in
%\textwidth=6.5in \textheight=8.5in
%-------------------------------------------------------------------------


\begin{document}
\begin{frontmatter}
	
	\title{Efficient algorithm for reduct computation}
	
	\author{Vlad\'imir Rodr\'iguez Diez}
	\author{Jos\'e Francisco Mart\'inez Trinidad}
	
	\address{Computer Science Department\\National Institute of
	Astrophysics, Optics and Electronics\\
	Luis Enrique Erro \# 1, Santa Mar\'{\i}a Tonantzintla, Puebla,
	72840, M\'{e}xico} 
	%\email{\{vladimir.rodriguez,fmartine\}@inaoep.mx}
	
	%\maketitle
	
	%\thispagestyle{empty}
	
	\begin{abstract}
	    Rough Set Theory reducts are minimal subsets of attributes preserving the semantics of an 
	    information system. The reduct computation problem has exponential complexity regarding the number of 
	    attributes in a dataset. Parallel acceleration of efficient algorithms have been made to
	    reduce the runtime for large datasets. The development of a new algorithm designed from scratch
	    for an efficient parallel implementation must improve the performance of existing alternatives. 
	    Throughout our research, most efficient reported algorithm for reduct computation will be 
	    implemented in a parallel acceleration fashion for benchmarking. Experiences and difficulties 
	    encountered in these experiments will constitute the basis for the development of our resulting algorithm. 
	    We expect this new approach would reduce the runtime, and hence make the reduct computation viable for 
	    larger datasets than it is today.
	\end{abstract}
	
	\begin{keyword}
		Rough Sets\sep Dimensionality Reduction\sep reduct computation\sep Parallel Acceleration.
	\end{keyword}
	
\end{frontmatter}

\pagebreak 
\tableofcontents
\pagebreak 

%-------------------------------------------------------------------------------
% your earth-shattering contribution

\section{Introduction}
  Rough set theory (RST), proposed by Z. Pawlak in 1982 \citep{Pawlak81,Pawlak81-2,Pawlak82,Pawlak91}, 
  is a relatively new mathematical theory 
  to deal with imperfect knowledge, in particular with vague concepts. Information systems in RST 
  are tables of objects described by some attributes (columns). 
  When data is collected or recorded, every single aspect of the object under study is considered 
  to have a complete representation and to ensure that no potentially useful information is lost.
  As a result, information systems are usually characterized by a large number of attributes,
  degrading the performance of machine learning tools \citep{Parthalain08}.
  One of the main concepts in RST is the notion of reduct, which is a minimal subset of attributes 
  preserving the required classification features \citep{Pawlak91}. A new information system using 
  only those features in a reduct, is a reduced representation of the original data with the same 
  classification quality. 
  However, the main restriction in practical use of RST is that computing all reducts has been proven 
  as an NP-hard problem~\citep{Skowron92}.
  It is therefore of high importance the development of efficient algorithms for reduct computation.
  
  Several attempts to speedup computing of reducts are reported. Many of the presented algorithms are 
  based on some heuristics. Main drawback of this approach is that these algorithm do not necessarily 
  return the complete set of attributes and may obtain super-reducts (non minimal subsets). Another 
  way to speedup computation of reducts is the parallelization \citep{Strakowski08}. There are also 
  interesting alternatives such as the use of a parallel version of genetic algorithms \citep{Wroblewski98}
  and the transformation of reduct computation to the well known problem SAT \citep{Jensen14}.
  
  Testor Theory (TT) was created by Yablonskii and Chegis in the middle of fifties 
  of the last century as a tool for analysis of problems connected with control and 
  diagnosis of faults in circuits. TT can be used for feature selection as shown in~\citep{Ruiz08}
  and \citep{Martinez01}. The relation between the rough set reducts and typical testors from the
  logical combinatorial approach to pattern recognition (TT) is exposed in \citep{Lazo15}. Algorithm for
  typical testors computation:~\citep{Ruiz85},~\citep{Santiesteban03},~\citep{Sanchez07} and~\citep{Lias09},
  may be applied to reduct computation because of the similarity between these two concepts. One strength 
  of our research is that we will be testing, for the first time, these two families of algorithm in the 
  same arena.
  
\subsection{Justification and Motivation}\label{Justification}{
  RST can be used to reduce the number of attributes in a dataset without relevant information lost. 
  Therefore, there has been much research in the area of finding reducts, particularly, reducts with 
  minimal cardinality~\citep{Jensen14}. 
  
  Heuristic methods such as~\citep{Chouchoulas01,Jensen2004,Zhong01} are fast alternatives for finding 
  reducts but they do not guarantee minimal reductions. Stochastic approaches~\citep{Wroblewski95,Jensen03,
  Chen10,Wang07} still do not guarantee finding the smallest reducts, as we will see further. Techniques 
  for finding the complete reducts set~\citep{Ruiz85,Santiesteban03,Sanchez07,Lias09} can, of course, find 
  minimal cardinality reducts but with a higher computational effort.
  
  The motivation of this work is the development of an algorithm for finding reducts with minimal length 
  in an information system. This is an NP-Hard problem which makes every attempt for reducing its execution
  time, a challenging task. Our proposal must be competitive with the state of the art algorithm in all 
  cases and will be faster in most datasets. The main arena for comparison will be a large set of synthetic,
  randomly generated datasets and benchmarking datasets from~\citep{Bache13}. We will be dealing only with
  classical reducts definition. The practical applications (or relevance) of shortest reducts in supervised
  classification is beyond our goals.
  
  The results of this research will impact the supervised classification methods specially in large datasets.
  Nowadays, data is automatically collected, thus generating huge databases in almost every field. The 
  current growth of the size of data and the number of existing databases, is the justification for our
  research on efficient algorithms for dimensionality reduction without semantics lost.}  
    
  Throughout our research we will conduct a comparative study between the most relevant algorithms for reduct 
  computation. Although main focus will be on algorithm obtaining minimal cardinality reducts, experiences 
  form heuristics approaches will be considered. The rest of this document is structured as follows. In 
  section~\ref{basicConcepts}, some basic concepts of rough set theory are introduced. A thorough revision of 
  the state of the art in classic reduct computation is presented in section \ref{relatedWork}. Our research
  questions are exposed in section~\ref{ResearchQuestions}; and our research goals in section~\ref{Goals}.
  In section~\ref{DOE} we present the design of experiments to be conducted throughout our research and;
  finally, the publication plan is exposed in section~\ref{PubPlan}.
  

\section{Basic Concepts}\label{basicConcepts}
  RST philosophy is based on the assumption that every object in the universe of discourse is described by 
  some information associated to it. This information constitutes the basis through which classification of 
  objects can be achieved. RST motto is "Let the data speak for themselves". Moreover, RST is a formal 
  framework to deal with imprecise and incomplete data with no need of additional information.
  
  From the RST point of view, two objects are indistinguishable (indiscernible) if they have an equivalent 
  value for each attribute in their description. Indiscernibility relations arising this way constitute the
  mathematical foundations of RST. Any partition of a dataset in which every pair of indiscernible objects
  belongs to the same subdivision is called a crisp (precise) set; otherwise, the set is rough (imprecise, 
  vague). Some basic concepts of RST are presented bellow. Although we will be following the explanation 
  in~\citep{Polkowski00}, some modifications are introduced to provide sufficient basis for the further 
  discussion.
  
\subsection{Information System}
  The basic representation of data in RST is an \emph{Information System} (IS). An IS is a table with rows
  representing objects while columns specify its attributes or features. Formally, an IS can be defined as 
  $IS=(U,A)$ where $U$ is a finite non-empty set of objects $U=\lbrace x_1,x_2,...,x_n\rbrace$ and $A$ is a 
  finite non-empty set
  of attributes (features, variables). Every attribute in $A$ is a map: $a: U \rightarrow V_a$. The set $V_a$ is
  called the \textit{value set} of $A$. Attributes in $A$ are further classified as condition attributes $C$ and 
  decision attributes $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. We can of course define two 
  value sets $V_c$ and $V_d$, for condition and decision attributes respectively, such that 
  $V_a=V_c \cup V_d$. Table~\ref{tab_IS} shows a typical IS.
  
  
 \begin{table}[htb]
		\caption{An Information System.} \label{tab_IS}
		\centering
 	\begin{tabular}{c||c|c||c}
 			  & $c_1$ & $c_2$ &  $d$ \\
 		\hline \hline
		$x_1$ &   1   &    3  &   0   \\
		$x_2$ &   1   &    0  &   0   \\
		$x_3$ &   3   &    1  &   1   \\
		$x_4$ &   3   &    1  &   1   \\
		$x_5$ &   4   &    2  &   1   \\
		$x_6$ &   1   &    2  &   0   \\
		$x_7$ &   4   &    2  &   1   \\
 	\end{tabular}             
 \end{table}
 
   
  \textit{Decision attributes} absolutely decide to which class the object belongs. In the IS of
  table~\ref{tab_IS}, $d$ is the decision attribute column. For this example $V_d = \lbrace 0,1 \rbrace$;
  hence this is a 2 classes system. \textit{Condition attributes} do not absolutely decide the class for 
  an object but help to decide. In supervised classification condition attributes are the only information
  available during classification of new objects while decision attributes are only present in the training set. 
  IS with distinguished decision and condition attributes are called decision tables. In table~\ref{tab_IS},
  $c_1$ and $c_2$ are condition attributes.
 
\subsection{Indiscernibility Relations}
  For an object $x \in U$, the information about $x$ with respect to a set $B \subseteq A$ may be defined as
  \textit{the B-information set} 
  
  \begin{equation}
  	Inf_B(x)=\lbrace (a,a(x)):a \in B \rbrace
  \end{equation}  
  
  of $x$.
  
  \emph{Indiscernibility relation} of $B$ is defined as follows:
  
  \begin{equation}
  	(x,y) \in IND_B \Longleftrightarrow Inf_B(x)=Inf_B(y)
  \end{equation} 
  
  Equivalent classes $[x]_B$ of the relation $IND_B$ represent therefore elementary (atomic) portions
  of knowledge represented by the subsystem $IS_B=(U,B)$.
  
  For a \textit{concept} (set of objects), $X \subseteq U$, we say that $X$ is \textit{B-exact} if and 
  only if
  	
  \begin{equation}
  	X=\cup_{i=1}^{k} [x_i]_B
  \end{equation} 
  
  for some $x_1, x_2,...,x_k \in U$ i.e. where $X$ is the union of some \textit{B-indiscernibility} classes.
  
  For example, in table~\ref{tab_IS} the three possible subset of conditional attributes are 
  $\lbrace c_1 \rbrace$, $\lbrace c_2 \rbrace$ and $\lbrace c_1, c_2 \rbrace$. The indiscernibility relation 
  for these sets defines three partitions of the universe:
  
  $$\begin{array}{lcc}
  IND_{\lbrace c_1 \rbrace} &=& \lbrace \lbrace x_1, x_2, x_6 \rbrace, 
  								\lbrace x_3, x_4 \rbrace, 
  								\lbrace x_5, x_7 \rbrace \rbrace \\
  IND_{\lbrace c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
  								\lbrace x_2 \rbrace, 
  								\lbrace x_3, x_4 \rbrace,
  								\lbrace x_5, x_6, x_7 \rbrace \rbrace \\
  IND_{\lbrace c_1, c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
  									\lbrace x_2 \rbrace, 
  									\lbrace x_3, x_4 \rbrace,
  									\lbrace x_5, x_7 \rbrace,
  									\lbrace x_6 \rbrace \rbrace 
  \end{array}$$

\subsection{Concept Approximations}
  RST capability of handling non-exact (rough) concepts arises from the approximation of a rough concept 
  by means of two crisp concepts: the \textit{lower} and the \textit{upper approximations of} $X$. 
  Denoted by $\underline{B}X$ and $\overline{B}X$, respectively as follows:
  
  \begin{equation}
  	\begin{array}{lcc}
  	\underline{B}X &=& \lbrace x \in U : [x]_B \subseteq X \rbrace\\
  	\overline{B}X  &=& \lbrace x:[x]_B \cap X \neq \emptyset \rbrace
  	\end{array}
  \end{equation}
  
  We would say that objects in $\underline{B}X$ can be certainly classified as elements of $X$ on the 
  basis of knowledge in $IS_B$, while objects in $\overline{B}X$ can only be possibly classified as 
  elements of $X$ on the basis of knowledge in $IS_B$. In other words, $\underline{B}X$ is 
  composed by those objects in $X$ having no indiscernible objects outside $X$. $\overline{B}X$ on the 
  other hand, is composed by all objects in $X$ plus all objects outside $X$ having an indiscernible 
  object in $X$.
  
  The set 
  
  \begin{equation}
  	BN_B(X)=\overline{B}X-\underline{B}X
  \end{equation}
  
  is called the \textit{B-boundary region of X} and it contains the objects which neither are certainly 
  members of $X$ nor they are certainly member of $U-X$. The presence of a non-empty boundary region
  indicates that the concept in question is rough (\textit{B-rough}).
  
  From the previous example we can see that e.g. concept $X=\lbrace x_1,x_2,x_3 \rbrace$ is rough for the 
  three attributes sets while e.g. the concept $Y=\lbrace x_1,x_2 \rbrace$ is both $\lbrace c_2 \rbrace$-
  and $\lbrace c_1, c_2 \rbrace$-exact. Notice that
  
  $$\begin{array}{lcc}
  \underline{c_1}X &=& \emptyset\\
  \overline{c_1}X  &=& \lbrace x_1,x_2,x_3,x_4,x_6 \rbrace\\
  \underline{c_2}X &=& \lbrace x_1,x_2 \rbrace
  \end{array}$$
  
\subsection{Positive Region}\label{subsect_Pos}
  The decision attribute $d$ induces a partition of the universe $U$ into equivalence classes 
  (\textit{decision classes}) of the relation $IND_d$. Each decision class $X_1,X_2,...,X_k$, where 
  $k=|V_d|$, may be approximated by its lower and upper approximations over a set $B \subseteq A$ of 
  attributes. Since we will be trying to associate a decision class to an object, based on the 
  knowledge in attributes belonging to $B$, we are interested in those $B-classes$ $[x]_B$ which 
  satisfy the condition $[x]_B \subseteq X_i$ for some $X_i$. This idea leads to the notion of the 
  \textit{positive region of the decision}.
  
  \begin{equation}
  	POS_B(d) = \lbrace x \in U: \exists i \in \lbrace 1,2,...,k \rbrace : [x]_B \subseteq X_i \rbrace
  \end{equation}
  
  The set $POS_B(d)$ is called the \textit{B-positive region of d}.
  
  Taking for example the IS in table~\ref{tab_IS}, we can see that
  
  $$\begin{array}{lcc}
  POS_{\lbrace c_1 \rbrace}(d)&=&U\\
  POS_{\lbrace c_2 \rbrace}(d)&=& \lbrace x_1,x_2,x_3,x_4 \rbrace\\
  POS_{\lbrace c_1, c_2 \rbrace}(d)&=&U
  \end{array}$$
 
\subsection{Reducts and Core}
  Given an information system $IS=(U,A)$ with condition attributes set $C$ and decision attributes set
  $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. A subset $B \subseteq C$ is a \textit{reduct} 
  of $IS$ relative to $D$ if
  \begin{enumerate}
  	\item $POS_B(D)=POS_C(D)$. \label{cond_1}
  	\item $B$ is a minimal subset (with respect to inclusion) satisfying condition~\ref{cond_1}.
  \end{enumerate}
  
  The intersection of all the reducts in an IS is called the \textit{core}, the elements of which cannot be
  eliminated without introducing more contradictions to the representation of the dataset.
  
\subsection{Discernibility Matrix and Discernibility Function}
  The discernibility knowledge of the information system is commonly recorded in a symmetric $|U| \times |U|$
  matrix called the \textit{discernibility matrix}, and each element $m_{ij}$ in the discernibility matrix 
  $M_{IS}$ is defined as 
  
  \begin{equation}
  	m_{ij}=\left\lbrace\begin{array}{cl}
  			\lbrace c \in C: c(x_i) \neq c(x_j) \rbrace & \mathrm{for~~}D(x_i) \neq D(x_j)\\
  			\emptyset 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation}
  
  Table~\ref{tab_DM} shows the discernibility matrix for the IS in table~\ref{tab_IS} as a lower triangular 
  matrix.
  
   \begin{table}[htb]
		\caption{Discernibility Matrix Example.} \label{tab_DM}
		\centering
 	\begin{tabular}{c|ccccccc}
 		$x \in U$ & 1 & 2 &  3 & 4 & 5 &  6 & 7\\
 		\hline
		1 &&&&&&&\\
		2 &&&&&&&\\
		3 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		4 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		5 & $c_1,c_2$ & $c_1,c_2$ &&&&&\\
		6 &&& $c_1,c_2$ & $c_1,c_2$ &&&\\
		7 & $c_1,c_2$ & $c_1,c_2$ &&&& $c_1$ &\\
 	\end{tabular}             
 \end{table}
  
  Once the discernibility matrix $M_{IS}$ is found, we can define the \textit{discernibility function} $f_{IS}$.
  This is a boolean function of $n$ boolean variables $c_1^*, c_2^*,...,c_n^*$, representing the presence of
  the corresponding attribute (True) or its absence (False) in $M_{IS}$.
  
  \begin{equation}
  	f_{IS}(c_1^*, c_2^*,...,c_n^*)=\wedge \lbrace \vee c_{ij}^* : 1 \leq j \leq i \leq |U|, 
  									c_{ij} \neq \emptyset \rbrace
  \end{equation}

  where $c_{ij}^*=\lbrace c^* : c \in c_{ij} \rbrace$. Only the lower triangular matrix from $M_{IS}$ is
  taken into consideration since $M_{IS}$ is symmetric. An equivalence between the prime implicants of
  $f_{IS}$ and all the reducts of $IS$ has been found~\citep{Pawlak07}.
  
  The discernibility function for the discernibility matrix in table~\ref{tab_DM} is, without repetitions  
  $f_{IS}(c_1^*,c_2^*)=(c_1^* \vee c_2^*) \wedge c_1^*$
  
  From this we can easily see that the only reduct (also the core) for this IS is $c_1$.
  
\subsection{Attributes Dependency and Significance}
  One important aspect of data analysis is the study of dependencies between attributes describing the 
  objects. Intuitively, a subset of attributes $D$ depends totally on a set of attributes $B$, denoted 
  $B \Rightarrow D$ if all attribute values from $D$ are uniquely determine by values of attributes
  in $B$. Formally in RST we say that for $B,D \subset A$, $D$ depends on $B$ in a degree 
  $k(0 \leq k \leq 1)$, denoted $B \Rightarrow _{k}D$ if
  
  \begin{equation}
  	k=\gamma _B (D)=\frac{|POS_B(D)|}{|U|}
  \end{equation}
    
  This is called the \textit{Positive Dependency Degree}, and it is the ratio of the number of objects belonging
  to the positive region to the number of all objects in universe $U$. If $k=1$, $D$ depends totally on $B$, if
  $0 < k < 1$, $D$ depends partially (in a degree $k$) on $B$ and if $k=0$, $D$ does not depends on $B$.
  
  Taking for example the positive regions from subsection~\ref{subsect_Pos}, we can see that
  
  $$\begin{array}{lcccc}  
  \gamma _{\lbrace c_1 \rbrace} (d)&=&\frac{|U|}{|U|}&=&1\\
  \gamma _{\lbrace c_2 \rbrace}(d)&=& \frac{|\lbrace x_1,x_2,x_3,x_4\rbrace|}{|U|}&=&\frac{4}{7} \\
  \gamma _{\lbrace c_1, c_2 \rbrace}(d)&=&\frac{|U|}{|U|}&=&1
  \end{array}$$
  
  We can express the \textit{significance} of feature $c \in B$ upon $D$ as
  
  \begin{equation}
  	SIG(c,B,D)=\gamma _B (D)-\gamma _{B-\lbrace c \rbrace} (D)
  \end{equation}
  
  From  the previous example:
  
  $$\begin{array}{lcccccc}
  SIG(c_1,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
  										-\gamma _{\lbrace c_2 \rbrace}(d)
  										&=&1-\frac{4}{7}&=&\frac{3}{7}\\
  SIG(c_2,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
  										-\gamma _{\lbrace c_1 \rbrace}(d)
  										&=&1-1&=&0
  \end{array}$$
  
  From this it follows that attribute $c_1$ is indispensable, but attribute $c_2$ can be dispense when 
  considering the dependency between the decision attribute $d$ on condition attributes $c_1$ and $c_2$.
  
  Finally, we would like to say that an alternative definition of reduct from dependency exist. We say 
  that a subset $B \in C$ is a reduct of $C$ if it is a minimal set (with respect to inclusion) satisfying 
  the condition $\gamma _B(D)=\gamma _C(D)$.

\section{Related Work}\label{relatedWork}
  In this section, we will be first discussing heuristic algorithms for reduct computation. Some of these 
  algorithm are capable of finding several reducts and others are intended to obtain a single \textit{minimal} 
  reduct. Then, two kind of algorithm for computing the complete set of reducts will be exposed: those 
  from RST and those from TT. Researches in TT have been focused in algorithm for finding all typical testors.
  Finally, we will make a survey of parallel accelerations reported in literature.  
  
  Figure~\ref{fig_Tax} shows a taxonomy of the reported algorithms for reduct computation. This classification
  corresponds to the sequence that we will be following throughout our survey of the state of the art. 
  Table~\ref{tab_Alg} makes a correspondence between the codename used in figure~\ref{fig_Tax} and the 
  publication reporting the algorithm. Some parallel approaches are implementations of existing sequential 
  algorithm; for those cases, we use a color association in figure~\ref{fig_Tax}.

  \begin{figure}[htb] 
    \centering
    \includegraphics[width=\textwidth]{Reduct_Computation_Algorithms.png}
	\caption{Taxonomy of reduct computation algorithms.}
	\label{fig_Tax}
 \end{figure}
 
 \newcolumntype{L}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
 \begin{table}[t]
	\caption{reduct computation algorithms.} \label{tab_Alg}
	\centering
 	\begin{tabular}{l||L{13cm}}
 		\multicolumn{1}{c||}{Codename} &  \multicolumn{1}{c}{Publication} \\
 		\hline \hline
		\textsc{quickreduct} 	&  Rough set-aided keyword reduction for text 
								   categorization~\citep{Chouchoulas01} \\
		YangLiHuang 				&  An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility
				 				   Matrix~\citep{Yang08}\\
		EBR						&  A rough set--aided system for sorting WWW bookmarks~\citep{Jensen01}\\
		RSFSACO					&  A rough set approach to feature selection based on ant colony 
								   optimization~\citep{Chen10}\\
		AntRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
		GenRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
		BjorvandKomorowski		&  Practical applications of genetic algorithms for efficient reduct 
								   computation~\citep{Bjorvand97}\\
		\hline		   
		BTHW						&  FPGA Based Architecture for Computing Testors~\citep{Rojas07}\\
		BruteForce				&  On the Design and Implementation of a High Performance Configurable
								   Architecture for Testor Identification~\citep{Cumplido06}\\
		TiwariKothariShah		&  FPGA Implementation of a Reduct Generation Algorithm based on Rough 
								   Set Theory~\citep{Tiwari13}\\
		TiwariKothari			&  Architecture and Implementation of Attribute Reduction Algorithm Using 
								   Binary Discernibility Matrix~\citep{Tiwari11}\\
		DM-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
		DT-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
		SRGonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\citep{Jiao10}\\
		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
								   composition~\citep{Jiao10}\\
		\hline
		TB \& BT					&  BT and TB algorithms for computing all irreducible testors~\citep{Ruiz85}\\
		CT--EXT					&  CT--EXT: an algorithm for computing typical testor set~\citep{Sanchez07}\\
		LEX						&  LEX: a new algorithm for the calculus of typical
								   testors~\citep{Santiesteban03}\\
		BR						&  BR: A new method for computing all typical testors~\citep{Lias09}\\
		YYC						&  YYC: A Fast Performance Incremental Algorithm for Finding Typical
								   Testors~\citep{Alba14}\\
		RSAR	--SAT				&  Finding rough and fuzzy--rough set reducts with SAT~\citep{Jensen14}\\
		Expansion Algorithm		&  Reduct generation in information systems~\citep{Starzyk99}\\
		GonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\	
 	\end{tabular}             
 \end{table}
 
\subsection{Heuristics Approaches}
  The algorithm presented in~\citep{Chouchoulas01} (\textsc{quickreduct}) starts with an empty set of 
  attributes and adds, one each iteration, the attribute having the highest significance. This greedy algorithm 
  uses the maximal dependency criterion for finding one optimal reduct.  
  A similar approach is the Johnson Reducer~\citep{Johnson74}, first introduced in RST in \citep{Ohrn00}.
  This simple greedy algorithm begins with an empty set of attributes evaluating each conditional attribute in the
  discernibility function according to a heuristic measure. In the simplest case, those attributes with highest 
  appearance frequency within the logical context termed clauses, are considered to be more relevant. Works 
  in~\citep{Nguyen97} and~\citep{Wang01} use alternative heuristic functions guiding the search down better paths. 
  Variations of this algorithm~\citep{Wang01} and~\citep{Yang08} use the discernibility matrix instead of the
  discernibility function.
  Algorithm presented in \citep{Zhong01} starts from the core (since it must be contained in every reduct) and
  follows a similar procedure adding selected attributes. This optimization may be impractical for large datasets
  \citep{Jensen14} since the core must be computed a priori.
  
  The work presented in~\citep{Jiao10} improves the efficiency of computing reducts by means of subdivision. The 
  original dataset is broken down into a master-table and several sub-tables that are simpler, more manageable 
  and more solvable. Results are then joined together in order to solve the original dataset. Different variants
  for decomposition of reduct computation problem are discussed and proposed in~\citep{Strakowski08}.
  
  Special attention deserve the approaches using genetic algorithms to discover sub-optimal reducts. Although 
  these algorithms does not guarantee finding optimal reducts, many reducts may be found in a determined time.
  A good point in this approach is the use of the fitness function to guide the search down to a set of 
  reducts with the desired properties. The work reported in~\citep{Wroblewski95} encodes candidates as bit 
  strings with a positional representation of attributes presence in the candidate set. The fitness function
  depends on the number of attributes in the subset, penalizing strings with a large number of bits set. The 
  second optimization parameter is the number of classifiable objects by the given candidate. The reduct should 
  discern between as many objects as possible.
  
  Other evolutionary approaches to reduct computation include Ant Colony Optimization~\citep{Jensen03}
  and~\citep{Chen10}; and Particle Swarm Optimization~\citep{Wang07}.
    
\subsection{Algorithms for Reducts and Testors Computation}
  A method for the generation of all reducts in an Information System is proposed in \citep{Starzyk99,Starzyk00}.
  Although its computational cost is high, this method provides all the reducts by means of manipulations of 
  the clauses in the discernibility function. In addition to standard simplifications laws, the concept of 
  strong compressibility is introduced and applied along with an expansion algorithm.
  
  Although originally intended for computing a single minimal reduct, the algorithm proposed in~\citep{Jensen14} may be
  modified in order to obtain all reducts in an Information System. The method introduced in this work reduces
  the problem of finding a reduct from the discernibility function to the SAT problem~\citep{Davis62}. The boolean
  function generated this way is always satisfy since the complete set of attributes is a trivial solution.
  In~\citep{Lin04}, a heuristic is followed to find a short reduct. This first reduct is used to limit the search
  space to attributes combinations with lower cardinality.
  
  A special mention deserves the work presented in~\citep{WangP07}. This approach avoids the computation of the
  discernibility matrix, which is a expensive computational task, and works directly over the information system. 

  
  One of the first algorithm designed to overcome the exponential complexity (regarding
  the number of features) of the problem of finding all the TT, was 
  proposed by Ruiz-Shulcloper et al.~(\cite{Ruiz85}). This algorithm, called BT,
  codified a subset of features as a binary word with as many bits as features in the 
  dataset. A 0 represents the absence of the corresponding feature in the current
  subset while a 1 represents its inclusion. This way, candidates subsets are evaluated
  in the natural order of binary numbers. The pruning process in the
  search space is based on the minimal condition of TT and a convenient sorting
  of the basic matrix (see definition~\ref{def:BM}) associated to the dataset. Finally, 
  testors found by BT algorithm must by compared with each other in order to remove
  any superset (not a TT by definition).
  In (Ruiz-Shulcloper et al.~\cite{Shulcloper95b}) a new algorithm (REC) is presented.
  The main drawback of REC was that it operated directly over the dataset (instead of the
  basic matrix), handling a huge amount of superfluous information. Ayaquica~(\cite{Ayaquica97})
  presented the algorithm CER directed to solve this problem by using a different traversing
  order. 
	
  Then, Santiesteban and Pons-Porrata~(\cite{Santiesteban03}) proposed a revolutionary algorithm
  called LEX. Main ideas behind LEX are a new traversing order of candidates (which resembles the
  lexicographical order in which string characters are compared) and the concept of gap. In LEX
  the typical condition is verified first and only for those potentially TT, the testor 
  condition is checked. This way, the out-coming testors from this algorithm are always typical.
  The concept of gap allows us; once obtained a TT (or a not testor) candidate, including 
  the last feature in the dataset, avoid the evaluation of any subset of this candidate.
	
  Sanchez-D\'iaz and Lazo-Cort\'es~(\cite{Sanchez07}) proposed the CT\_EXT algorithm for computing all
  TT. Following a traversing order similar to that in LEX, this algorithm search for
  testors without verifying the typical condition. This way, a larger number of candidates are 
  evaluated, in comparison to LEX; but the cost of each evaluation is lower. Results from experiments
  show that CT\_EXT is faster than the previous existing algorithm for most datasets. Then, Lias-Rodr\'iguez
  and Pons-Porrata~(\cite{Lias09}) presented the BR algorithm, a Recursive algorithm based on 
  Binary operations. BR is very similar to LEX in its bones but its recursive nature encloses a great
  gain. Given a candidate subset, the remaining features are tested a priori and those being rejected are
  excluded from subsequent evaluations. Sanchez-D\'iaz et al.~(\cite{Sanchez10}) presented a cumulative
  procedure for the CT\_EXT algorithm. This fast-CT\_EXT implementation reduces drastically the runtime
  for most datasets at no extra cost. In (Lias-Rodr\'iguez and Sanchez-D\'iaz~\cite{Lias13}) the
  gap elimination and column reduction are added to BR. This fast-BR algorithm is, no doubt the one 
  evaluating the minimum number of candidates in the state of the art. The main drawback of fast-BR and 
  BR is, as in LEX, the high cost of evaluating the typical condition for every candidate. 
%  The first work developed in the reduction of the search space for typical testors~\cite{Ruiz85}, follows 
%  the natural order of attributes codified as a binary number. Unnecessary candidates (attributes subset)
%  evaluation are avoided taking into account properties of typical testors and results from previous 
%  evaluations. Subsequent algorithm~\cite{Santiesteban03},~\cite{Sanchez07} and~\cite{Lias09} propose
%  more sophisticated evaluation orders. Recently, a new internal typical testor--finding algorithm was
%  proposed~\cite{Alba14}. In our experience, non of this algorithm is the fastest for all dataset.
  
\subsection{Parallel Accelerations}

  A parallel acceleration of the algorithm presented in~\citep{Yang08} for reduct generation from binary
  discernibility matrix was developed in~\citep{Tiwari11,Tiwari12}. This FPGA implementation computes a 
  single reduct. A real application of object identification system by an intelligent robot is presented.
  In~\citep{Tiwari13} a quick algorithm, similar to those presented in~\citep{Chouchoulas01}, is proposed
  and implemented in a hardware fashion. A recent work from this authors~\citep{Tiwari14}, shows a thorough
  survey of FPGA applications in rough sets reduct computation.

  From the Typical Testors theory, several attempts have been made to overcome the problem 
  complexity by means of FPGA implementations of algorithms. In a first work~\citep{Cumplido06}, an 
  FPGA-based brute force approach for computing testors was proposed. This first approach did 
  not take advantage of dataset characteristics to reduce the number of candidates to be tested; 
  thus all $2^n$ combinations of $n$ features have to be tested. Then, in \citep{Rojas07} a hardware 
  architecture of the BT algorithm for computing typical testors was implemented. 
  This algorithm uses a candidate pruning process for avoiding many unnecessary candidate evaluation, 
  reducing the number of verifications of the typical testor condition. These two previous works computed 
  a set of testors on the FPGA device whilst typical condition was evaluated afterwards by the 
  software component in the hosting PC. Thus, in~\citep{Rojas12} a hardware-software platform for 
  computing typical testors that implemented the BT algorithm, as in \citep{Rojas07}, was proposed; but it also 
  included a new module that eliminates most of the non typical testors before transferring them to 
  a host software application for final filtering. 
	
	%TODO estos trabajos de Wroblewski hay q revisarlos bien pq parece falso lo de los GA paralelos
  In~\citep{Wroblewski98}, a parallel variant of the algorithm proposed in~\citep{Wroblewski95} is presented.
  Developments in genetic algorithms are exploited to provide a speedup for the problem of finding reducts.
  This line of thinking brought us an unexplored acceleration idea from~\citep{Jensen14}. We can combine 
  available architectures for SAT solving on FPGA~\citep{Safar07,Kanazawa11} with the transformation
  presented in~\citep{Jensen14} to obtain a new hardware platform for computing all reducts of an information 
  system.
  
  In~\citep{Grzes13,Kopczynski14}, an FPGA application for a single reduct computation is presented. Although
  authors claim that a huge acceleration is achieved, some drawbacks have to be mentioned. Experiments presented 
  in~\citep{Kopczynski14} to validate their results is performed over a small dataset which in our experience 
  does not implies its applicability to larger cases where such acceleration are needed. On the other hand, 
  runtime estimations for FPGA component executions are made by means of a oscilloscope without taking into 
  account communication overhead, which cannot be neglected in the presented dataset.
\section{Research Proposal}\label{ResearchProposal} 
\subsection{The Research Questions}\label{ResearchQuestions} 
  Throughout our preliminary work, we noticed that there is no faster algorithm for finding reducts in 
  all datasets. Different algorithms use different strategies for traversing and pruning the complete search 
  space. These strategies are better suited for some datasets and are time consuming for others. This leads 
  us to our first scientific question:
  
\begin{quote}
  \emph{Is there a relationship between the external properties of the discernibility matrix and the runtime 
  		cost of traversing strategies for finding reducts of an information system?}
\end{quote}
  		
  In this context, external properties are those characteristics that we can extract from the discernibility
  matrix by traversing their cells just one time. Lets take for instance, the minimum and maximum number of
  attributes in a cell, the core or the mean number of attributes per cell. Internal properties require, on 
  the other hand, more complex operations. Internal properties are, for instance, the number of reducts, the
  cardinality of the shortest and largest reducts, etc.
  
  From this research question we formulate the following hypothesis:
  
\begin{quote}  
  \emph{There is a relationship between the external properties of the discernibility matrix and the runtime 
  		cost of traversing strategies for finding reducts of an information system}
\end{quote}
  		
  Several attempts for the decomposition of the original problem of the complete reducts set computation have 
  been made~\citep{Strakowski08,Jiao10,Kopczynski14}. The main disadvantage of the problem decomposition or
  parallelization is the strong dependency between the speed--up of the method used and the particularities of 
  the dataset~\citep{Strakowski08}. This leads us to our second scientific question:
  
\begin{quote}
  \emph{Is there a relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding reducts of an information system 
  		in a parallel environment?}
\end{quote}

  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{There is a relationship between the external properties of the discernibility matrix and the 
  		speed--up of the decomposition method used for finding reducts of an information
  		system in a parallel environment}
\end{quote}
  
  The more sophisticated a scanning strategy is, the less number of candidates attributes sets are 
  evaluated. Unfortunately, a more sophisticated scanning strategy has usually a greater runtime cost.
  This trade--off between the number of evaluated candidates and the runtime cost of evaluation leads us 
  to our third scientific question:
  
\begin{quote}
  \emph{Can be obtained a runtime reduction in computing reducts of an information system by 
  		dynamically changing the traversing strategy?}
\end{quote}
  By dynamically changing we mean the change of the traversing strategy during the algorithm execution.
  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{There can be obtained a statistically significant runtime reduction in computing reducts of an 
  		information system by dynamically changing the traversing strategy}
\end{quote}

  Based on these three scientific questions we can formulate the main hypothesis for our research proposal:
  
\begin{quote}
  \emph{Using some external properties of the discernibility matrix, and dynamically changing the traversing 
  		strategy, we can design a new algorithm for computing reducts of an information system; which is
  		comparable to the state of the art alternatives in most datasets, and faster in some of them}
\end{quote}  

\subsection{The Research Goals}\label{Goals} 
  The main goal in our research is the \emph{development of  a new algorithm for computing reducts of an 
  information system; which is comparable to the state of the art alternatives 
  in most datasets, and faster in some of them}. This algorithm will use some external properties of the
  discernibility matrix to conveniently select the traversing strategy for the search space and a 
  decomposition method suitable for paralellization. We will be exploring two variants of this problem,
  the problem of computing all reducts and the problem of computing shortest reducts of an information system.
  The problem of finding shortest reducts is also NP--hard~\citep{Lin04} but different pruning rules may
  be used.
  
  Our specific goals are:
  \begin{enumerate}
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		traversing strategy for computing all reducts.
  		  	
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		decomposition method for computing all reducts.
  		
  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
  		for computing all reducts.
  
  \item Develop a new algorithm for computing all reducts of an information system, based 
  		on the three previously found relationships.
  		
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		traversing strategy for computing shortest reducts.
  		  	
  \item Find a relationship between some external properties of the discernibility matrix and the fastest 
  		decomposition method for computing shortest reducts.
  		
  \item Find a relationship between the traversed space and the expected cost of each traversing strategy
  		for computing shortest reducts.
  
  \item Develop a new algorithm for computing minimal cardinality reducts of an information system, based 
  		on the three previously found relationships.

  \end{enumerate}

\subsection{Methodology}\label{Methodology} 
\begin{enumerate}
	\item Selecting a priori the fastest traversing strategy for finding all reducts.
	\begin{itemize}
  		\item Generate a set of random datasets, systematically covering the space of possible combinations in 
  			  the values of the external properties of discernibility matrices.
  		\item Implement the main traversing strategies reported for the computation of all reducts.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing minimal cardinality reducts using the classifier found in
  			  the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Selecting a priori the fastest decomposition method for finding all reducts.
	\begin{itemize}
  		\item Implement the main decomposition methods reported for the computation of all reducts
  			  in a parallel fashion.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest decomposition method as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  decomposition method for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing all reducts using the classifier found in the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Find a relationship between the traversed space and the expected cost of traversing strategies.
  	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of all reducts in a way
  			  that we can collect statistics for every execution stage.
  		\item Make a statistical description  of strategies runtime cost over synthetic and benchmarking datasets,
  			  using the traversed space as a factor.
  		\item Find a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Develop a new algorithm for computing all reducts of an information system.
  	\begin{itemize}
  		\item Design a new algorithm for computing all reducts using some external
  		      properties of the discernibility matrix.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	
  	\item Selecting a priori the fastest traversing strategy for finding shortest reducts.
	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of minimal length reducts.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing minimal cardinality reducts using the classifier found in
  			  the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Selecting a priori the fastest decomposition method for finding shortest reducts.
	\begin{itemize}
  		\item Implement the main decomposition methods reported for the computation of minimal length reducts
  			  in a parallel fashion.
  		\item Generate an information system with the external properties of each discernibility matrix, using 
  			  the fastest decomposition method as decision attribute.
  		\item Extract the relevant subset of external properties for determining a priori the appropriate 
  			  decomposition method for a given dataset; and the rules governing this relation.
  		\item Design a new algorithm for computing minimal cardinality reducts using the classifier 
  			  found in the previous task.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Find a relationship between the traversed space and the expected cost of traversing strategies.
  	\begin{itemize}
  		\item Implement the main traversing strategies reported for the computation of shortest reducts in a way
  			  that we can collect statistics for every execution stage.
  		\item Make a statistical description  of strategies runtime cost over synthetic and benchmarking datasets,
  			  using the traversed space as a factor.
  		\item Find a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Develop a new algorithm for computing minimal cardinality reducts of an information system.
  	\begin{itemize}
  		\item Design a new algorithm for computing minimal cardinality reducts using some external
  		      properties of the discernibility matrix.
  		\item Evaluate the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
\end{enumerate}
	  	
%\section{Publication Plan}\label{PubPlan}
%  In this section we present our publication plan. Results from this research are to be submitted indistinctly 
%  to the following journals or congresses, according to the advisor’s recommendations.
%  
%  Main journals:
%  \begin{itemize}
% 	\item Information Sciences\footnote{http://www.journals.elsevier.com/information-sciences/}. IF: 3.893
% 	\item Expert Systems with 
% 		  Applications\footnote{http://www.journals.elsevier.com/expert-systems-with-applications/}. IF: 1.965
% 	\item IEEE Transactions on Knowledge and Data
% 		  Engineering\footnote{http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=69}. IF: 1.815
% 	\item International Journal of Advanced Computer Science and
% 		  Applications\footnote{http://thesai.org/Publications/IJACSA}. IF: 1.32
%  \end{itemize}
% 
%  Main congresses:
%  \begin{itemize}
% 	\item Rough Sets and Current Trends in Computing
% 	\item Rough Sets and Intelligent Systems Paradigms
% 	\item CIARP: Iberoamerican Congress on Pattern Recognition
% 	\item Mexican Conference on Pattern Recognition
%  \end{itemize}
%  
%  In table~\ref{tab_PP} we present the estimated dates for our proposed publications. The first publication will
%  discuss the results of the first experiment. In our second publication we will make a review of the state of 
%  the art in rough sets reduct computation. Third and fourth publications will present the results from the last
%  three experiments as shown in table~\ref{tab_PP}.
%  
%     \begin{table}[htb]
%		\caption{Publication plan.} \label{tab_PP}
%		\centering
% 	\begin{tabular}{c||l|l}
% 		\# & \multicolumn{1}{c|}{Date} & \multicolumn{1}{c}{Results}\\
% 		\hline \hline
%		1 & July 2015 & First Experiment (\ref{exprimet1})\\
%		2 & December 2015 & State of the art survey \\
%		3 & July 2016 & Second Experiment (\ref{exprimet2})\\
%		4 & July 2017 & Third and fourth Experiments (\ref{exprimet3} \& \ref{exprimet4})\\
% 	\end{tabular}             
% 	\end{table}

%-------------------------------------------------------------------------------
% Bibliography
%-------------------------------------------------------------------------------
\newpage 
\bibliography{mybib}{}
\bibliographystyle{authordate1}
\end{document}
