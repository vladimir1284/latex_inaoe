%-------------------------------------------------------------------------
%\documentclass[11pt,authoryear]{elsarticle}
\documentclass[authoryear,11pt]{elsarticle}


\setlength{\parskip}{1em}			% espaciar parrafos
\newtheorem{proposition}{Proposition}

\usepackage{hyperref}				% enlaces en el pdf
\hypersetup{backref,colorlinks=true}	% colores en vez de cajas en los enlaces
\usepackage{times}              		% la letra
\usepackage{graphicx}           		% para manejar imagenes
\usepackage{subfigure}          		% para manejar subfiguras
\usepackage{tabularx}		   		% para ajustar el ancho de las columnas
\usepackage[margin=2.5cm]{geometry}	% Change margins
\usepackage[table]{xcolor}			% Colores en el cronograma
\usepackage{multirow}				% Cabecera del cronograma
\usepackage{watermark}				% Para la portada
\usepackage{datetime}				% Fecha de creado
\usepackage{pst-tree}				% Para la taxonomía
\usepackage{stackengine}				% Para listar los articulos en el nodo de la taxonomía
\usepackage{algorithm}				% Para el seudocódigo
\usepackage{setspace}				% Para el seudocódigo
\usepackage{amsmath}					% Para el seudocódigo
\usepackage[noend]{algpseudocode}	% Para el seudocódigo


%-------------------------------------------------------------------------
% Configuring Taxonomy
%-------------------------------------------------------------------------
\setstackEOL{\\}
\def\psedge{\ncangles[angleA=-90,angleB=90]}
\psset{levelsep=20mm,treesep=1cm,nodesep=3pt, arrows=->}
\def\PSBL#1{\small\pspicture(7,0.8)\psTextFrame[shadow,
  fillstyle=solid,linecolor=blue,framearc=0.3](0,0)(7,0.8){%
    \shortstack{#1}}\endpspicture}
\def\PSBS#1{\pspicture(3,.7)\psTextFrame[shadow,
  fillstyle=solid,linecolor=blue,framearc=0.3](0,0)(3,0.8){%
    \shortstack{#1}}\endpspicture}

%-------------------------------------------------------------------------
% Referencias a una palabra
%-------------------------------------------------------------------------
\newcommand{\setword}[2]{%
  \phantomsection
  #1\def\@currentlabel{\unexpanded{#1}}\label{#2}%
}


\begin{document}
	\input{./frontPage.tex}
	
	\title{Development of fast algorithms for reduct computation}
	
	\author{Vlad\'imir Rodr\'iguez Diez}
	\author{Jos\'e Francisco Mart\'inez Trinidad}
	
	\address{Computer Science Department\\National Institute of
	Astrophysics, Optics and Electronics\\
	Luis Enrique Erro \# 1, Santa Mar\'{\i}a Tonantzintla, Puebla,
	72840, M\'{e}xico} 
%	\email{\{vladimir.rodriguez,fmartine\}@inaoep.mx}
	
	
%	\thispagestyle{empty}
	
	\begin{abstract}
		Information systems in Rough Set Theory (RST) are tables of objects described by some attributes. 
		This type of tables are widely used in different pattern recognition problems particularly in 
		supervised classification. RST reducts are minimal subsets of attributes preserving 
		the discernibility capacity of the whole set of attributes. Reducts computation has an exponential
		complexity regarding the number of attributes in the information system. In the literature several
		algorithms for reduct computation have been reported, but their high computational cost makes 
		infeasible their use in large problems. For this reason, in this research we will develop new fast
		algorithms in two directions, the computation of all reducts and the computation of globally 
		shortest reducts. The proposed algorithms will be faster than state of the art algorithms, and hence 
		make the reduct computation viable for larger information systems than it is today. As part of this 
		PhD research proposal, we present some preliminary results, which show that it is possible developing
		faster algorithms for computing reducts.
	\end{abstract}
	
	\begin{keyword}
		Rough Sets\sep Dimensionality Reduction\sep Reduct Computation.
	\end{keyword}

	\maketitle

\pagebreak 
\tableofcontents
\pagebreak 

%-------------------------------------------------------------------------------
% your earth-shattering contribution

\section{Introduction}
  Rough set theory (RST), proposed by Z. Pawlak in 1982 \citep{Pawlak81,Pawlak81-2,Pawlak82,Pawlak91}, 
  is a relatively new mathematical theory 
  to deal with imperfect knowledge, in particular with vague concepts. Into RST, information systems
  are tables of objects described by a set of attributes (columns). 
  When data is collected or recorded, every single aspect (attribute) of the object under study is considered 
  to have a complete representation and to ensure that no potentially useful information is lost.
  As a result, information systems are usually characterized by a large number of attributes,
  degrading the performance of machine learning tools~\citep{Parthalain08}.
  One of the main concepts in RST is the notion of reduct, which is a minimal subset of attributes 
  preserving the required classification capabilities~\citep{Pawlak91}. A new information system using 
  only those features in a reduct, is a reduced representation of the original data that allows obtaining  
  the same classification quality than the original information system. 
  However, the main restriction in practical applications of RST is that computing all reducts of an information 
  system has an exponential complexity~\citep{Skowron92}. Therefore an active research line is the development 
  of fast algorithms for reduct computation.
  
  Several attempts to speed up the reduct computation have been reported. Many of these algorithms are 
  based on some heuristics. The main drawback of this approach is that these algorithms do not necessarily 
  return the complete set of reducts in the information system, and may obtain super-reducts (non minimal subsets). 
  Another way to speed up reduct computation is parallelization~\citep{Strakowski08}. There are also 
  interesting alternatives such as the use of a parallel version of genetic algorithms \citep{Wroblewski98}
  and the transformation of the reduct computation problem to the well known SAT problem~\citep{Jensen14}.
  
  Recently the RST reducts have been related to the typical testors (TT) from the logical combinatorial approach to 
  pattern recognition~\citep{Lazo15}. Testor Theory was created by Yablonskii and Chegis in the middle of fifties 
  of the last century as a tool for analysis of problems connected with control and diagnosis of faults in
  circuits. 
  Testor Theory can be used for feature selection as shown in~\citep{Ruiz08} and~\citep{Martinez01}. Algorithms for
  typical testors computation like~\citep{Ruiz85},~\citep{Santiesteban03},~\citep{Sanchez07} 
  and~\citep{Lias09}, can be applied to reduct computation due to the similarity between these two concepts. 
  Fast implementations of these algorithms; based on cumulative binary operations~\citep{Sanchez10}, genetic 
  algorithms~\citep{Sanchez99} and hardware architectures~\citep{Rojas12}; have been developed to reduce the
  computation time. One strength of our research is that we will be testing, for the first time, these two 
  families of algorithms in the same arena.
     
  Throughout our research we will conduct a comparative study between the most relevant algorithms for reduct 
  computation. Although our main focus will be on algorithms for computing all reducts and globally shortest
  reducts, experiences from heuristics approaches will be considered as well. We will be exploring the relationship 
  between algorithms' performance and the characteristics of the information system. Based on this relationship,
  we will propose fast algorithms for reduct computation. Finally the proposed algorithms 
  will be redesigned and implemented in a hardware fashion in order to improve, even more, their efficiency.  

\section{Basic Concepts}\label{basicConcepts}
  RST is based on the assumption that every object in the universe of discourse is described, through a 
  set of attributes, by some information associated to it. This information constitutes the basis for the
  classification of unseen objects. RST motto is \textit{Let the data speak for themselves} \citep{Tiwari14}.
  
  From the RST point of view, two objects are indistinguishable (indiscernible) if they have an equivalent 
  value for each attribute in their description. Indiscernibility relations arising this way constitute the
  mathematical foundations of RST. 
  Some basic concepts of RST are presented bellow. Although we will be following the explanation 
  in~\citep{Polkowski00}, some modifications in the notation are introduced to provide clarity in the rest 
  of the document.
  
\subsection{Information System}
  The basic representation of data in RST is an \emph{Information System} (IS). An IS is a table with rows
  representing objects while columns specify attributes or features. Formally, an IS is defined as a pair
  $IS=(U,A)$ where $U$ is a finite non-empty set of objects $U=\lbrace x_1,x_2,...,x_n\rbrace$, and $A$ is a 
  finite non-empty set
  of attributes (features, variables). Every attribute in $A$ is a map: $a: U \rightarrow V_a$. The set $V_a$ is
  called the \textit{value set} of $A$. Attributes in $A$ are further divided into condition attributes $C$ and 
  decision attributes $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. 
  Table~\ref{tab_IS} shows an example IS.
  
  
 \begin{table}[htb]
		\caption{An Information System.} \label{tab_IS}
		\centering
 	\begin{tabular}{c||c|c|c||c}
 			  & $c_1$ & $c_2$ &  $c_3$ & $d$ \\
 		\hline \hline
		$x_1$ &   1   &    3  &  0  &   0   \\
		$x_2$ &   1   &    0  &  0  &   0   \\
		$x_3$ &   3   &    1  &  1  &   1   \\
		$x_4$ &   3   &    3  &  2  &   1   \\
		$x_5$ &   4   &    2  &  3  &   1   \\
		$x_6$ &   4   &    3  &  1  &   0   \\
		$x_7$ &   4   &    2  &  5  &   1   \\
 	\end{tabular}             
 \end{table}
 
   
  \textit{Decision attributes} determine to which class the object belongs. In the IS of
  table~\ref{tab_IS}, $d$ is the decision attribute. 
  %For this example $V_d = \lbrace 0,1 \rbrace$; hence 
  This is a two-class system. \textit{Condition attributes} do not absolutely determine the class but help 
  to decide to which class an object belongs to. In supervised classification, condition attributes are the 
  only information available for classifying new objects; while, decision attributes are only 
  available for objects in the training set. An IS with distinguished decision and condition attributes is 
  called a decision table. In table~\ref{tab_IS}, $c_1$, $c_2$ and $c_3$ are condition attributes.
 
%\subsection{Indiscernibility Relations}
%  For an object $x \in U$, the information about $x$ with respect to a set $B \subseteq A$ may be defined as
%  \textit{the B-information set} 
%  
%  \begin{equation}
%  	Inf_B(x)=\lbrace (a,a(x)):a \in B \rbrace
%  \end{equation}  
%  
%  of $x$.
%  
%  \emph{Indiscernibility relation} of $B$ is defined as follows:
%  
%  \begin{equation}
%  	(x,y) \in IND_B \Longleftrightarrow Inf_B(x)=Inf_B(y)
%  \end{equation} 
%  
%  Equivalent classes $[x]_B$ of the relation $IND_B$ represent therefore elementary (atomic) portions
%  of knowledge represented by the subsystem $IS_B=(U,B)$.
%  
%  For a \textit{concept} (set of objects), $X \subseteq U$, we say that $X$ is \textit{B-exact} if and 
%  only if
%  	
%  \begin{equation}
%  	X=\cup_{i=1}^{k} [x_i]_B
%  \end{equation} 
%  
%  for some $x_1, x_2,...,x_k \in U$ i.e. where $X$ is the union of some \textit{B-indiscernibility} classes.
%  
%  For example, in table~\ref{tab_IS} the three possible subset of conditional attributes are 
%  $\lbrace c_1 \rbrace$, $\lbrace c_2 \rbrace$ and $\lbrace c_1, c_2 \rbrace$. The indiscernibility relation 
%  for these sets defines three partitions of the universe:
%  
%  $$\begin{array}{lcc}
%  IND_{\lbrace c_1 \rbrace} &=& \lbrace \lbrace x_1, x_2 \rbrace, 
%  								\lbrace x_3, x_4 \rbrace, 
%  								\lbrace x_5, x_6, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_2 \rbrace} &=& \lbrace \lbrace x_1, x_4, x_6 \rbrace, 
%  								\lbrace x_2 \rbrace, 
%  								\lbrace x_3 \rbrace,
%  								\lbrace x_5, x_7 \rbrace \rbrace \\
%  IND_{\lbrace c_1, c_2 \rbrace} &=& \lbrace \lbrace x_1 \rbrace, 
%  									\lbrace x_2 \rbrace, 
%  									\lbrace x_3 \rbrace,
%  									\lbrace x_4 \rbrace,
%  									\lbrace x_5, x_7 \rbrace,
%  									\lbrace x_6 \rbrace \rbrace 
%  \end{array}$$
%
%\subsection{Concept Approximations}
%  RST capability of handling non-exact (rough) concepts arises from the approximation of a rough concept 
%  by means of two crisp concepts: the \textit{lower} and the \textit{upper approximations of} $X$. 
%  Denoted by $\underline{B}X$ and $\overline{B}X$, respectively as follows:
%  
%  \begin{equation}
%  	\begin{array}{lcc}
%  	\underline{B}X &=& \lbrace x \in U : [x]_B \subseteq X \rbrace\\
%  	\overline{B}X  &=& \lbrace x:[x]_B \cap X \neq \emptyset \rbrace
%  	\end{array}
%  \end{equation}
%  
%  We would say that objects in $\underline{B}X$ can be certainly classified as elements of $X$ on the 
%  basis of knowledge in $IS_B$, while objects in $\overline{B}X$ can only be possibly classified as 
%  elements of $X$ on the basis of knowledge in $IS_B$. In other words, $\underline{B}X$ is 
%  composed by those objects in $X$ having no indiscernible objects outside $X$. $\overline{B}X$ on the 
%  other hand, is composed by all objects in $X$ plus all objects outside $X$ having an indiscernible 
%  object in $X$.
%  
%  The set 
%  
%  \begin{equation}
%  	BN_B(X)=\overline{B}X-\underline{B}X
%  \end{equation}
%  
%  is called the \textit{B-boundary region of X} and it contains the objects which neither are certainly 
%  members of $X$ nor they are certainly member of $U-X$. The presence of a non-empty boundary region
%  indicates that the concept in question is rough (\textit{B-rough}).
%  
%  From the previous example we can see that e.g. concept $X=\lbrace x_1,x_2,x_3 \rbrace$ is rough for the 
%  three attributes sets while e.g. the concept $Y=\lbrace x_1,x_2 \rbrace$ is both $\lbrace c_2 \rbrace$-
%  and $\lbrace c_1, c_2 \rbrace$-exact. Notice that
%  
%  $$\begin{array}{lcc}
%  \underline{c_1}X &=& \emptyset\\
%  \overline{c_1}X  &=& \lbrace x_1,x_2,x_3,x_4,x_6 \rbrace\\
%  \underline{c_2}X &=& \lbrace x_1,x_2 \rbrace
%  \end{array}$$
  
\subsection{Positive Region}\label{subsect_Pos}
  Decision attributes induce a partition of the universe $U$ into equivalence classes 
  (\textit{decision classes}). Since we will be trying to associate a decision class to an object, 
  based on the attributes belonging to $B \subseteq C$, we are interested in those 
  $B-classes$ (classes induced by $B$) which correspond to classes induced by $d$. 
  This idea leads to the notion of the  \textit{positive region of the decision}. The set $POS_B(d)$, 
  called the \textit{B-positive region of d}, is defined as the set of all objects in $U$ such 
  that all their indistinguishable objects (under the knowledge in $B$) belong to its same class induced 
  by $d$.
  
  Taking for example the IS in table~\ref{tab_IS}, we can see that
  
  $$\begin{array}{lcc}
  POS_{\lbrace c_1 \rbrace}(d)     &=& \lbrace x_1,x_2,x_3,x_4 \rbrace\\
  POS_{\lbrace c_2 \rbrace}(d)     &=& \lbrace x_2,x_3,x_5,x_7 \rbrace\\
  POS_{\lbrace c_1, c_2 \rbrace}(d)&=& U
  \end{array}$$
 
\subsection{Reducts and Core}\label{def_reduct}
  Given an information system $IS=(U,A)$ with condition attribute set $C$ and decision attribute set
  $D$ such that $A=C \cup D$ and $C \cap D =\emptyset$. A subset $B \subseteq C$ is a \textit{reduct} 
  of $IS$ relative to $D$ if
  \begin{enumerate}
  	\item $POS_B(D)=POS_C(D)$. \label{cond_1}
  	\item $B$ is a minimal subset (with respect to inclusion) satisfying condition~\ref{cond_1}.\label{cond_2}
  \end{enumerate}
  
  The intersection of all reducts of an IS is called the \textit{core}, the attributes that cannot be
  eliminated without reducing the classification accuracy.
  
\subsection{Discernibility Matrix and Discernibility Function}
  The discernibility knowledge of the information system is commonly stored in a symmetric $|U| \times |U|$
  matrix called the \textit{discernibility matrix}. Each element $m_{ij}$ in the discernibility matrix 
  $M_{IS}$ is defined as   
  \begin{equation}
  	m_{ij}=\left\lbrace\begin{array}{cl}
  			\lbrace c \in C: c(x_i) \neq c(x_j) \rbrace & \mathrm{for~~}D(x_i) \neq D(x_j)\\
  			\emptyset 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation}  
  Here, $c(x_i)$ stands for the value of the condition attribute $c$ in the object (row) $x_i$, and 
  $$D(x_i) \neq D(x_j) \Rightarrow \exists d \in D~ |~ d(x_i) \neq d(x_j)$$ 
  where $d(x_i)$ stands for the value  of the decision attribute $d$ in the object $x_i$.
  
  Table~\ref{tab_DM} shows the discernibility matrix for the IS in table~\ref{tab_IS} as a lower triangular 
  matrix ($\emptyset$'s are omitted for clarity).
  
   \begin{table}[htb]
		\caption{Discernibility Matrix Example.} \label{tab_DM}
		\centering
 	\begin{tabular}{c|ccccccc}
 		$x \in U$ & 1 & 2 &  3 & 4 & 5 &  6 & 7\\
 		\hline
		1 &&&&&&&\\
		2 &&&&&&&\\
		3 & $\lbrace c_1,c_2,c_3\rbrace$ & $\lbrace c_1,c_2,c_3\rbrace$ &&&&&\\
		4 & $\lbrace c_1,c_3\rbrace$ & $\lbrace c_1,c_2,c_3\rbrace$ &&&&&\\
		5 & $\lbrace c_1,c_2,c_3\rbrace$ & $\lbrace c_1,c_2,c_3\rbrace$ &&&&&\\
		6 &&& $\lbrace c_1,c_2\rbrace$ & $\lbrace c_1,c_3\rbrace$ & $\lbrace c_2,c_3\rbrace$ &&\\
		7 & $\lbrace c_1,c_2,c_3\rbrace$ & $\lbrace c_1,c_2,c_3\rbrace$ &&&& $\lbrace c_2,c_3\rbrace$ &\\
 	\end{tabular}             
 \end{table}
  
  Once the discernibility matrix $M_{IS}$ is found, we can define the \textit{discernibility function} $f_{IS}$.
  This is a Boolean function of $n$ Boolean variables $c_1^*, c_2^*,...,c_n^*$, representing the presence of
  the corresponding attribute (True) or its absence (False) in $M_{IS}$. Here, the disjunction ($\vee$) and 
  conjunction ($\wedge$) operators have their common meaning. Their evaluation over a set of Boolean variables
  $X=\lbrace x_1^*, x_2^*, ..., x_n^* \rbrace$ should be denoted as 
  $\wedge X= x_1^* \wedge x_2^* \wedge ... \wedge x_n^* $.

  \begin{equation}
  	f_{IS}(c_1^*, c_2^*,...,c_n^*)=\wedge \lbrace \vee c_{ij}^* : 1 \leq j \leq i \leq |U|, 
  									m_{ij} \neq \emptyset \rbrace
  \end{equation}

  where $c_{ij}^*=\lbrace c^* : c \in m_{ij} \rbrace$. Only the lower triangular matrix from $M_{IS}$ is
  taken into consideration since $M_{IS}$ is symmetric. An equivalence between the prime implicants of
  $f_{IS}$ and all reducts of $IS$ has been found and reported in~\citep{Pawlak07}.
  
  The discernibility function for the discernibility matrix in table~\ref{tab_DM}, after simplifying by 
  deleting repeated clauses, is  
  $$f_{IS}(c_1^*,c_2^*,c_3^*)=(c_1^* \vee c_2^* \vee c_3^*) \wedge (c_1^* \vee c_2^*) 
   \wedge (c_1^* \vee c_3^*) \wedge (c_2^* \vee c_3^*) $$
  
  From this example we can easily see that the only reduct (also the core) for this IS is $c_1$.
  
%\subsection{Attributes Dependency and Significance}
%  One important aspect of data analysis is the study of dependencies between attributes describing the 
%  objects. Intuitively, a subset of attributes $D$ depends totally on a set of attributes $B$, denoted 
%  $B \Rightarrow D$ if all attribute values from $D$ are uniquely determine by values of attributes
%  in $B$. Formally in RST we say that for $B,D \subset A$, $D$ depends on $B$ in a degree 
%  $k(0 \leq k \leq 1)$, denoted $B \Rightarrow _{k}D$ if
%  
%  \begin{equation}
%  	k=\gamma _B (D)=\frac{|POS_B(D)|}{|U|}
%  \end{equation}
%    
%  This is called the \textit{Positive Dependency Degree}, and it is the ratio of the number of objects belonging
%  to the positive region to the number of all objects in universe $U$. If $k=1$, $D$ depends totally on $B$, if
%  $0 < k < 1$, $D$ depends partially (in a degree $k$) on $B$ and if $k=0$, $D$ does not depends on $B$.
%  
%  Taking for example the positive regions from subsection~\ref{subsect_Pos}, we can see that
%  
%  $$\begin{array}{lcccc}  
%  \gamma _{\lbrace c_1 \rbrace} (d)&=&\frac{|U|}{|U|}&=&1\\
%  \gamma _{\lbrace c_2 \rbrace}(d)&=& \frac{|\lbrace x_1,x_2,x_3,x_4\rbrace|}{|U|}&=&\frac{4}{7} \\
%  \gamma _{\lbrace c_1, c_2 \rbrace}(d)&=&\frac{|U|}{|U|}&=&1
%  \end{array}$$
%  
%  We can express the \textit{significance} of feature $c \in B$ upon $D$ as
%  
%  \begin{equation}
%  	SIG(c,B,D)=\gamma _B (D)-\gamma _{B-\lbrace c \rbrace} (D)
%  \end{equation}
%  
%  From  the previous example:
%  
%  $$\begin{array}{lcccccc}
%  SIG(c_1,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_2 \rbrace}(d)
%  										&=&1-\frac{4}{7}&=&\frac{3}{7}\\
%  SIG(c_2,\lbrace c_1, c_2 \rbrace,d)&=&\gamma _{\lbrace c_1, c_2 \rbrace}(d)
%  										-\gamma _{\lbrace c_1 \rbrace}(d)
%  										&=&1-1&=&0
%  \end{array}$$
%  
%  From this it follows that attribute $c_1$ is indispensable, but attribute $c_2$ can be dispense when 
%  considering the dependency between the decision attribute $d$ on condition attributes $c_1$ and $c_2$.
%  
%  Finally, we would like to say that an alternative definition of reduct from dependency exist. We say 
%  that a subset $B \in C$ is a reduct of $C$ if it is a minimal set (with respect to inclusion) satisfying 
%  the condition $\gamma _B(D)=\gamma _C(D)$.

\subsection{Binary Discernibility Matrix}
  The \textit{Binary Discernibility Matrix} is a binary table representing the discernibility sets between pairs 
  of objects. This is another representation of the information in $M_{IS}$. In the binary discernibility
  matrix, columns are single condition attributes and rows are objects pairs belonging to different classes.
  The discernibility element $m(i, j, c)$ between two objects $x_i$ and $x_j$ by using a single condition 
  attribute $c \in C$ is given in a binary representation, such that:
  
  \begin{equation}
  	m(i, j, c)=\left\lbrace\begin{array}{cl}
  			1 & \mathrm{for~~}c(x_i) \neq c(x_j),D(x_i) \neq D(x_j)\\
  			0 								   & \mathrm{otherwise} 
  	\end{array}\right.
  \end{equation} 
  
  Table~\ref{tab_BDM} shows the binary discernibility matrix for the information system of Table~\ref{tab_IS}.  
  
  \begin{table}[htb]
		\caption{Binary Discernibility Matrix Example.} \label{tab_BDM}
		\centering
 	\begin{tabular}{cccc}
 		& $c_1$ & $c_2$ & $c_3$\\
 		\hline
		$x_1,x_3$ & 1 & 1 & 1 \\
		$x_1,x_4$ & 1 & 0 & 1 \\
		$x_1,x_5$ & 1 & 1 & 1 \\
		$x_1,x_7$ & 1 & 1 & 1 \\
		$x_2,x_3$ & 1 & 1 & 1 \\
		$x_2,x_4$ & 1 & 1 & 1 \\
		$x_2,x_5$ & 1 & 1 & 1 \\
		$x_2,x_7$ & 1 & 1 & 1 \\
		$x_3,x_6$ & 1 & 1 & 0 \\
		$x_4,x_6$ & 1 & 0 & 1 \\
		$x_5,x_6$ & 0 & 1 & 1 \\
		$x_6,x_7$ & 0 & 1 & 1 
 	\end{tabular}             
  \end{table}
  
\subsection{Simplified Discernibility Matrix}
  The \textit{Simplified Discernibility Matrix} is a reduced version of the discernibility matrix after
  eliminating supersets and repeated cells in $M_{IS}$. This new discernibility matrix has the same reducts
  as the original one~\citep{Yao09}. An equivalent concept exists in Testor Theory, called 
  \textit{Basic Matrix}. The basic matrix was proven to have the same TT as the original information
  system~\citep{Lazo01}. Table~\ref{tab_SDM} shows the basic matrix for the
  discernibility matrix from Table~\ref{tab_BDM}.
  
     \begin{table}[htb]
		\caption{Basic Matrix Example.} \label{tab_SDM}
		\centering
 	\begin{tabular}{ccc}
 		$c_1$ & $c_2$ & $c_3$\\
 		\hline
		1 & 0 & 1 \\
		1 & 1 & 0 \\
		0 & 1 & 1
 	\end{tabular}             
 \end{table}

\section{Related Work}\label{relatedWork}
  In this section, we will be first discussing heuristic algorithms for reduct computation. Some of these 
  algorithms are capable of finding several reducts and others are intended to obtain a single \textit{shortest} 
  reduct. Then, two kind of algorithms for computing reducts will be exposed: those 
  from RST and those from TT. Finally, we will make a review of parallel accelerations reported in the literature.  
  
  In Figure~\ref{fig_Tax1}, we propose a taxonomy of the reported algorithms for computing a single reduct
  and, in Figure~\ref{fig_Tax2}, a taxonomy of the reported algorithms for computing all reducts. 
  This classification corresponds to the sequence that we will be following throughout our review of the 
  state of the art.
   
  \begin{figure}[htb] 
  \center
    \pstree[treesep=2cm]{\Tr{\PSBL{Algorithms for Computing a single Reduct}}}{
       \pstree[treesep=3cm]{\Tr{\PSBS{Short}}}{
       		\pstree[levelsep=7.5em,treesep=.7cm]{\Tr{\PSBS{Heuristic}}}{
       			\pstree[levelsep=3em]{\Tr{\PSBS{FPGA}}}{
       				\scriptsize \Shortstack[l]{
       					\citep{Tiwari11}\\
       					\citep{Tiwari13}
       				}
       			}
       			\pstree[levelsep=3em]{\Tr{\PSBS{Sequential}}}{
       				\scriptsize \Shortstack[c]{
       					\citep{Chouchoulas01}\\
       					%\hyperref[quickreduct]{\textsc{quickreduct}}\\
       					%EBR\\
       					\citep{Yang08}
       				}
       			}
       			\pstree[levelsep=3em]{\Tr{\PSBS{Subdivision}}}{
       				\scriptsize \Shortstack[r]{
       					\hyperref[FSDCRS]{FSDC-RS}~\citep{Jiao10}\\
       					\hyperref[FSDCRS]{FSDC-HS}~\citep{Jiao10}	
       				}
       			}
       		}
       		\pstree[levelsep=5.7em]{\Tr{\PSBS{Evolutionary}}}{
       			\scriptsize \Shortstack[l]{
       				\citep{Wroblewski95}\\
       				\citep{Bjorvand97}\\
       				\hyperref[AntRSAR]{AntRSAR}~\citep{Jensen03}\\
       				\hyperref[GenRSAR]{GenRSAR}~\citep{Jensen03}\\
       				\hyperref[RSFSACO]{RSFSACO}~\citep{Chen10}
       			}
       		}
       		}
       \pstree[levelsep=3em]{\Tr{\PSBS{Shortest}}}{
       		\scriptsize \Shortstack[l]{
       			\citep{Lin04}\\
       			\hyperref[RSARSAT]{RSAR-SAT}~\citep{Jensen14}	
       		}
       }
  }
  \caption{Taxonomy of algorithms for computing a single reduct.}
  \label{fig_Tax1}
  \end{figure}
  	
  \begin{figure}[htb] 
    	\center
    \pstree{\Tr{\PSBL{Algorithms for Computing all Reducts}}}{%
    \pstree{\Tr{\PSBS{Rough Sets}}}{%
    		\pstree[levelsep=3em]{\Tr{\PSBS{Subdivision}}}{
    			\scriptsize \Shortstack[l]{
    				\hyperref[SRGonCRS]{SRGonCRS}~\citep{WangP07}\\
    				\citep{Strakowski08}
%    				\hyperref[DT]{DT}\\
%    				\hyperref[DT]{DISC FUNCTION}\\
%    				\hyperref[DT]{CANDIDATE REDUCTS}	  
       		}
    		}
       	\pstree[levelsep=4em]{\Tr{\PSBS{Sequential}}}{
       		\scriptsize \Shortstack[l]{
       			\citep{Starzyk99}\\
       			\hyperref[RGonCRS]{RGonCRS}~\citep{WangP07}\\
       			\hyperref[RSARSAT]{RSAR-SAT}~\citep{Jensen14}					  
       		}
       	}
  }
  	\pstree{\Tr{\PSBS{Testor Theory}}}{%
  		\pstree[levelsep=3em]{\Tr{\PSBS{FPGA}}}{
  			\scriptsize \Shortstack[l]{\citep{Cumplido06}\\
  					 			  BT~\citep{Rojas07}}
  		}
       	\pstree[levelsep=5.5em]{\Tr{\PSBS{Sequential}}}{
      			\scriptsize \Shortstack[r]{\hyperref[BT]{BT \& TB}~\citep{Ruiz85}\\
      								  \hyperref[LEX]{LEX}~\citep{Santiesteban03} \\
      								  \hyperref[CTEXT]{CT\_EXT}~\citep{Sanchez07}\\
      								  \hyperref[BR]{BR}~\citep{Lias09}\\
      								  \hyperref[YYC]{YYC}~\citep{Alba14}}
      	}
  }}
  \caption{Taxonomy of algorithms for computing all reducts.}
  \label{fig_Tax2}
  \end{figure}
 
% \newcolumntype{L}[1]{>{\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
% \begin{table}[t]
%	\caption{reduct computation algorithms.} \label{tab_Alg}
%	\centering
% 	\begin{tabular}{l||L{13cm}}
% 		\multicolumn{1}{c||}{Codename} &  \multicolumn{1}{c}{Publication} \\
% 		\hline \hline
%		\textsc{quickreduct} 	&  Rough set-aided keyword reduction for text 
%								   categorization~\citep{Chouchoulas01} \\
%		YangLiHuang 				&  An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility
%				 				   Matrix~\citep{Yang08}\\
%		EBR						&  A rough set--aided system for sorting WWW bookmarks~\citep{Jensen01}\\
%		RSFSACO					&  A rough set approach to feature selection based on ant colony 
%								   optimization~\citep{Chen10}\\
%		AntRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
%		GenRSAR					&  Finding rough set reducts with ant colony optimization~\citep{Jensen03}\\
%		BjorvandKomorowski		&  Practical applications of genetic algorithms for efficient reduct 
%								   computation~\citep{Bjorvand97}\\
%		\hline		   
%		BTHW						&  FPGA Based Architecture for Computing Testors~\citep{Rojas07}\\
%		BruteForce				&  On the Design and Implementation of a High Performance Configurable
%								   Architecture for Testor Identification~\citep{Cumplido06}\\
%		TiwariKothariShah		&  FPGA Implementation of a Reduct Generation Algorithm based on Rough 
%								   Set Theory~\citep{Tiwari13}\\
%		TiwariKothari			&  Architecture and Implementation of Attribute Reduction Algorithm Using 
%								   Binary Discernibility Matrix~\citep{Tiwari11}\\
%		DM-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
%		DT-Reduct				&  FPGA in Rough--Granular Computing: Reduct Generation~\citep{Kopczynski14}\\
%		SRGonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\
%		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
%								   composition~\citep{Jiao10}\\
%		FSDC-RS					&  Two novel feature selection methods based on decomposition and 
%								   composition~\citep{Jiao10}\\
%		\hline
%		TB \& BT					&  BT and TB algorithms for computing all irreducible testors~\citep{Ruiz85}\\
%		CT--EXT					&  CT--EXT: an algorithm for computing typical testor set~\citep{Sanchez07}\\
%		LEX						&  LEX: a new algorithm for the calculus of typical
%								   testors~\citep{Santiesteban03}\\
%		BR						&  BR: A new method for computing all typical testors~\citep{Lias09}\\
%		YYC						&  YYC: A Fast Performance Incremental Algorithm for Finding Typical
%								   Testors~\citep{Alba14}\\
%		RSAR	--SAT				&  Finding rough and fuzzy--rough set reducts with SAT~\citep{Jensen14}\\
%		Expansion Algorithm		&  Reduct generation in information systems~\citep{Starzyk99}\\
%		GonCRS					&  Highly Scalable Rough Set Reducts Generation~\citep{WangP07}\\	
% 	\end{tabular}             
% \end{table}
 
\subsection{Algorithm Finding a Single Reduct}
  The algorithm presented in~\citep{Chouchoulas01} \setword{\textsc{quickreduct}}{quickreduct} starts with 
  an empty set of attributes and adds, one at a time, the attribute having the highest significance. 
  This greedy algorithm evaluates the significance of an attribute by the number of objects added to 
  the positive region after its inclusion.  
  A similar approach is the Johnson Reducer~\citep{Johnson74}, first introduced in RST in \citep{Ohrn00}.
  This simple greedy algorithm begins with an empty set of attributes evaluating each conditional attribute in the
  discernibility function according to a heuristic measure. In the simplest case, those attributes with highest 
  appearance frequency within the logical clauses of the discernibility function, are considered to be more
  relevant. Works in~\citep{Nguyen97} and~\citep{Wang01} use alternative heuristic functions for guiding the
  search. 
  Variations of this algorithm~\citep{Wang01} and~\citep{Yang08} use the discernibility matrix instead of the
  discernibility function.
  The algorithm presented in \citep{Zhong01} starts from the core (since it must be contained in every reduct) and
  follows a similar procedure adding selected attributes. This optimization may be impractical for large datasets
  \citep{Jensen14} since the core must be computed a priori.
  
  The method presented in~\citep{Jiao10} improves the efficiency of computing reducts by means of subdivision 
  of the dataset. The original dataset is broken down into a master-table and several sub-tables both, simpler
  and more manageable. Two algorithms are proposed (\setword{FSDC-RS}{FSDCRS} and FSDC-HS) using 
  different decomposition strategies. Results are then joined together in order to solve the original dataset. 
    
  Special attention deserves the approaches using genetic algorithms to discover locally shortest reducts. Although 
  these algorithms do not guarantee finding globally shortest reducts, many reducts may be found in a determined
  time. A good point in this approach is the use of the fitness function to guide the search down to a set of 
  reducts with the desired properties. The algorithm reported in~\citep{Wroblewski95} encodes candidates as bit 
  strings with a positional representation of attributes in the candidate set. The fitness function
  depends on the number of attributes in the subset, penalizing strings with a large number of bits set. The 
  second optimization parameter is the number of objects classified by the given candidate. The reduct should 
  discern as many objects as possible. \cite{Jensen03} also introduced a simple algorithm 
  (\setword{GenRSAR}{GenRSAR}), which uses a genetic search strategy in order to determine reducts.
  
  Other evolutionary approaches to reduct computation include Ant Colony Optimization~\citep{Jensen03} 
  (\setword{AntRSAR}{AntRSAR}) and~\citep{Chen10} (\setword{RSFSACO}{RSFSACO}); and Particle Swarm 
  Optimization~\citep{Wang07}.
    
  In~\citep{Lin04}, a heuristic is followed to find a short reduct. This first reduct is used to limit the search
  space, in order to only consider those attributes combinations with lower cardinality. 
  The main drawback of this algorithm is that the second step searches for reducts by checking all possible 
  $s$-subtables of the whole database. A $s$-subtable means a subtable whose conditional attribute set have 
  size $s$. In other words, it is a decision table with conditional attribute subset of size $s$ plus the decision
  attributes of original table. This final process uses no pruning strategy and explores the combinatorial
  possibilities of attribute combinations, which is unfeasible in most cases.
  
  Although originally intended for computing a single minimal reduct, the algorithm proposed in~\citep{Jensen14}
  (\setword{RSAR-SAT}{RSARSAT}) may be modified in order to obtain all reducts in an Information System. 
  The method introduced in this work
  reduces the problem of finding a reduct from the discernibility function to the SAT problem~\citep{Davis62}. 
  The boolean function generated in this way is always satisfied since the complete set of attributes is a trivial
  solution.
  
    
\subsection{Algorithms for all Reducts and all Typical Testors Computation}
  One of the first algorithm designed to overcome the exponential complexity (regarding
  the number of features) of the problem of finding all TT, was 
  proposed by \cite{Ruiz85}. This algorithm, called \setword{BT}{BT},
  codifies a subset of features as a binary word with as many bits as features in the 
  dataset. A 0 represents the absence of the corresponding feature in the current
  subset while a 1 represents its inclusion. This way, candidates subsets are evaluated
  in the natural order induced by binary numbers. The pruning process in the
  search space is based on the minimal condition of TT and a convenient sorting
  of the basic matrix associated to the dataset. Finally, 
  testors found by BT algorithm must be filtered in order to remove any non-TT.
  In \citep{Shulcloper95b} a new algorithm (REC) is presented.
  The main drawback of REC is that it works directly over the dataset (instead of the
  basic matrix), handling a huge amount of superfluous information. \cite{Ayaquica97}
  presented the algorithm CER directed to solve this problem by using a different traversing
  order. 
	
  Then, \cite{Santiesteban03} proposed a new algorithm
  called \setword{LEX}{LEX}. Main ideas behind LEX are a new traversing order of candidates (which resembles the
  lexicographical order in which string characters are compared) and the concept of gap. In LEX
  the typical condition is verified first and only for those potentially TT, the testor 
  condition is checked. %This way, the out-coming testors from this algorithm are always typical.
  The concept of gap allows us, once obtained a TT (or a not testor) candidate including 
  the last feature in the dataset, to avoid the evaluation of any subset of this candidate.
	
  \cite{Sanchez07} proposed the \setword{CT\_EXT}{CTEXT} algorithm for computing all
  TT. Following a traversing order similar to that in LEX, this algorithm searches for
  testors without verifying the typical condition. This way, a larger number of candidates are 
  evaluated, in comparison to LEX; but the cost of each evaluation is lower. Results from experiments
  show that CT\_EXT is faster than the previous existing algorithms for most datasets. Then, \cite{Lias09}
  presented the \setword{BR}{BR} algorithm, a Recursive algorithm based on 
  Binary operations. BR is very similar to LEX in its bones but its recursive nature encloses a great
  gain. Given a candidate subset, the remaining features are tested a priori and those being rejected are
  excluded from subsequent evaluations. \cite{Sanchez10} presented a cumulative
  procedure for the CT\_EXT algorithm. This fast-CT\_EXT implementation reduces drastically the runtime
  for most datasets at no extra cost. In \citep{Lias13} the
  gap elimination and column reduction are added to BR. This fast-BR algorithm is, no doubt the one 
  evaluating the minimum number of candidates in the state of the art. The main drawback of fast-BR and 
  BR is, as in LEX, the high cost of evaluating the typical condition for each candidate. 
 
  Recently, a new internal typical testor--finding algorithm (\setword{YYC}{YYC}) was proposed by~\cite{Alba14}. Although 
  they claim that this algorithm verify less candidates than previous alternatives, two weak points should
  be addressed. First, BR is not included in comparisons; and second, the evaluation cost for a candidate
  in YYC is high compared to that of previous algorithms. YYC verifications involve calculations of the 
  Hamming weight.

  A method for the computation of all reducts in an Information System is proposed in
  \citep{Starzyk99,Starzyk00}.
  This is a divide and conquer approach. On each step, the absorption laws are applied over the incoming
  discernibility matrix to obtain a basic matrix. Then, the strong equivalent attributes are compressed
  (which is a local reduction of columns). The most discerning attribute is selected (in the same way as 
  Johnson's reducer does) and the problem is divided into two sub-problems: 
  \begin{itemize}
  \item Finding reducts containing the selected attribute. Thus a recursive function is called with a new basic 
  matrix, having only those rows where the selected attribute does not appear.
  \item Finding reducts that do not contain the selected attribute. Thus a recursive function is called with a new 
  discernibility matrix, removing the column corresponding to the selected attribute.
  \end{itemize}
  The base case in the recursion is reached when each attribute in the incoming discernibility matrix appears 
  in a single clause. Finally a set of super-reducts is obtained and supersets must be removed in order to obtain 
  the final reduct set.
  Notice that this algorithm is oriented to the binary discernibility function, then 
  terms such as discernibility and basic matrix, rows and columns are not used in the paper. The
  algorithm is presented in an iterative fashion and its recursive nature is not clearly expressed.
  
  \cite{WangP07} proposed a new algorithm for computing all reducts \setword{RGonCRS}{RGonCRS}. 
  Even though this algorithm 
  was developed independently and reported two years before to the one reported in~\citep{Lias09}, it is very 
  similar to BR. Notice that this is a rough set approach to the problem and the nomenclature is totally 
  different to that of BR. Essentially, every proposition supporting the pruning process in \citep{WangP07} 
  have an equivalent proposition in \citep{Lias09}. The main differences of RGonCRS with BR are:
  \begin{itemize}
  \item It works directly over the dataset instead of the basic matrix.
  \item It starts searching the core and looks for reducts as supersets of the core.
  \item A recursive implementation, instead of the iterative solution used in BR, is proposed.
  \item During the algorithm execution, contributing attributes are sorted as in the Johson reducer.
  \item A second algorithm \setword{SRGonCRS}{SRGonCRS} is proposed for subdividing the dataset and the reducts are
  		incrementally found.
  \end{itemize}
  
  Different variants (\setword{DT}{DT}, DISC FUNCTION and CANDIDATE REDUCTS) for decomposition of a reduct 
  computation problem are discussed and proposed in~\citep{Strakowski08}.

\subsection{Parallel Accelerations}

  A parallel acceleration of the algorithm presented in~\citep{Yang08}, for reduct generation from a binary
  discernibility matrix, was developed in~\citep{Tiwari11,Tiwari12}. This FPGA implementation computes a 
  single reduct. A real application for object identification into an intelligent robot is presented.
  In~\citep{Tiwari13} a \emph{quick reduct} algorithm, similar to that one presented in~\citep{Chouchoulas01}, 
  is proposed and implemented in a hardware fashion. A recent work from these authors~\citep{Tiwari14}, 
  shows a thorough survey of FPGA applications in rough set reduct computation.

  From the Testor Theory, several attempts to overcome the problem's complexity, by means of FPGA
  implementations of algorithms, have been done. In a first work~\citep{Cumplido06}, an 
  FPGA-based brute force approach for computing testors was proposed. This first approach did 
  not take advantage of dataset characteristics to reduce the number of candidates to be tested; 
  thus all $2^n$ combinations of $n$ features have to be tested. Then, in \citep{Rojas07} a hardware 
  architecture of the BT algorithm for computing typical testors was implemented. 
  This algorithm uses a candidate pruning process for avoiding many unnecessary candidate evaluations, 
  reducing the number of verifications of the typical testor condition. These two previous works computed 
  a set of testors on the FPGA device whilst the typical condition was evaluated afterwards by the 
  software component in the hosting PC. Thus, in~\citep{Rojas12} a hardware-software platform for 
  computing typical testors that implements the BT algorithm, similar to \citep{Rojas07}, was proposed; 
  but it also included a new module that eliminates most of the non typical testors before transferring them to 
  a host software application for final filtering. 
	
	%TODO estos trabajos de Wroblewski hay q revisarlos bien pq parece falso lo de los GA paralelos
  In~\citep{Wroblewski98}, a parallel variant of the algorithm proposed in~\citep{Wroblewski95} is presented.
  Developments in parallel implementations of genetic algorithms are exploited to provide a speedup for the 
  problem of finding reducts.
  
  In~\citep{Grzes13,Kopczynski14}, an FPGA application for a single reduct computation is presented. Although
  authors claim that a huge acceleration is achieved, some weak points have to be mentioned. Experiments presented 
  in~\citep{Kopczynski14} to validate their results are performed over a small dataset which in our experience 
  does not implies its applicability to larger cases where such acceleration is needed. On the other hand, 
  runtime estimations for FPGA component executions are made by means of a oscilloscope without taking into 
  account communication overhead.
  
  %TODO Recapitulación 
  
\section{Research Proposal}\label{ResearchProposal} 
 In this section we present the justification and motivation, the research questions, the objectives and the
 expected contributions of this PhD. research proposal. We also include a detailed methodology and the schedule 
 for reaching our objectives.

\subsection{Justification and Motivation}\label{Justification}
  RST can be used to reduce the number of attributes in a dataset without relevant information loss. 
  Therefore, there has been a lot of research on finding reducts, particularly, shortest 
  reducts~\citep{Jensen14}. \cite{Zheng14} highlighted the relevance of feature selection through rough
  set reducts and illustrated the current research activity on this topic. Recently, \cite{Jiang15} said that 
  attribute reduction is one of the most important tasks in rough sets and, as a consequence,  many strategies 
  for finding reducts have been investigated. In general, we find consensus in the literature about both,
  the relevance and the actuality of research on rough set reducts.
  
  Heuristic methods such as~\citep{Chouchoulas01,Jensen04,Zhong01} are fast alternatives for finding 
  reducts but they do not guarantee to find a shortest reduct. Stochastic approaches such as~\citep{Wroblewski95,
  Jensen03,Chen10,Wang07} still do not guarantee finding a shortest reduct, as we have seen in 
  Section~\ref{relatedWork}. Techniques for finding all reducts~\citep{Starzyk99,WangP07} can, of course, find 
  the shortest reducts but with a high computational effort.
  
  The motivation of this research is the development of algorithms for computing all reducts and 
  globally shortest reducts in information systems. Both problems have an exponential complexity, which 
  make every attempt for reducing execution time a challenging task.
  Our proposals for computing all reducts and globally shortest reducts must be competitive with the state of 
  the art algorithms in the general case and faster in some determined cases. The main arena for comparison 
  will be a large set of synthetic, randomly generated datasets and benchmarking datasets from~\citep{Bache13}. 
  Practical applications of reducts in supervised classification or feature selection, are beyond our goals.
  
  The products of this research will impact feature selection methods specially in large datasets.
  Nowadays, data is automatically collected, thus the generation of huge databases appears in almost every 
  field. The current growth of the size of data, and the number of existing databases, is another justification 
  for our research on fast algorithms for dimensionality reduction without losing discriminative power.  
  
\subsection{Research Questions}\label{ResearchQuestions} 
  Throughout our state of the art review, we noticed that there is not a fastest algorithm for finding reducts 
  on any dataset. Algorithms reported in the literature use different strategies for traversing and pruning 
  the whole search space. Consequently, some strategies are better suited for some datasets while they are time
  consuming for some others. This leads us to our first research question:
  
\begin{quote}
  \emph{Is there a relationship between some properties of the basic matrix and the runtime 
  		of traversing strategies for finding reducts in information systems?}
\end{quote}
  		
  We will be considering those properties that can be extracted from the basic matrix by traversing their 
  cells just one time. Lets take for instance, the minimum and maximum number of attributes in a cell, the 
  core or the mean number of attributes per cell. Other properties that require more complex operations to 
  be extracted such as the number of reducts, the cardinality of the shortest and largest reducts, etc; 
  will not be considered.
  
  From this research question we formulate the following hypothesis:
  
\begin{quote}  
  \emph{There is a relationship between the properties of the basic matrix and the runtime 
  		of traversing strategies for finding reducts in information systems}
\end{quote}
  		
%  Several attempts for the decomposition of the original problem of the complete reducts set computation have 
%  been made~\citep{Strakowski08,Jiao10,Kopczynski14}. The main disadvantage of the problem decomposition or
%  parallelization is the strong dependency between the speed--up of the method used and the particularities of 
%  the dataset~\citep{Strakowski08}. This leads us to our second scientific question:
%  
%\begin{quote}
%  \emph{Is there a relationship between the external properties of the discernibility matrix and the 
%  		speed--up of the decomposition method used for finding reducts of an information system 
%  		in a parallel environment?}
%\end{quote}
%
%  From this research question we formulate the following hypothesis:
%    
%\begin{quote}
%  \emph{There is a relationship between the external properties of the discernibility matrix and the 
%  		speed--up of the decomposition method used for finding reducts of an information
%  		system in a parallel environment}
%\end{quote}
  
  The more sophisticated a traversing strategy is, the less number of candidates attributes sets are 
  evaluated to verify whether they are reducts or not. Unfortunately, a more sophisticated traversing 
  strategy has usually a higher computational cost. This trade-off between the number of evaluated 
  candidates and their evaluation cost, leads us to our second research question:
  
\begin{quote}
  \emph{Can be reduced the runtime for computing reducts in information systems by 
  		dynamically changing the traversing strategy?}
\end{quote}
  By dynamically changing we mean the change of the traversing strategy during the reduct computation.
  From this research question we formulate the following hypothesis:
    
\begin{quote}
  \emph{The runtime for computing reducts in information systems can be reduced by 
  		dynamically changing the traversing strategy}
\end{quote}

  Based on these two scientific questions we can formulate the main hypothesis for our research:
  
\begin{quote}
  \emph{Using some properties of the basic matrix, and dynamically changing the traversing 
  		strategy, we can design new algorithms for computing reducts in information systems; which are
  		faster than the state of the art alternatives in a kind of datasets}
\end{quote}  

\subsection{Research Objectives}\label{Goals} 
  The main objective in our research is the \emph{development of new algorithms for computing reducts in
  information systems; which will be comparable to state of the art algorithms 
  in most datasets, and faster in some specific kinds of datasets}. 
  
  These algorithms will use some properties 
  of the basic matrix to conveniently select the traversing strategy for the search space. We will explore two
  variants of this problem, the problem of computing all reducts and the problem of computing globally 
  shortest reducts. The problem of finding shortest reducts has also 
  exponential complexity~\citep{Lin04} but different pruning rules could be used.
  
  Our specific objectives are:
  \begin{enumerate}
  \item Finding a relationship between some properties of the basic matrix and the fastest 
  		traversing strategy for computing all reducts.
  
  \item Developing a new algorithm for computing all reducts.
  		
  \item Finding a relationship between some properties of the basic matrix and the fastest 
  		traversing strategy for computing globally shortest reducts.
  
  \item Developing a new algorithm for computing globally shortest reducts.

  \end{enumerate}

\subsection{Expected Contributions}\label{Contributions} 
  \begin{itemize}
  \item A new algorithm for computing all reducts, which will be comparable to 
  		state of the art algorithms in most datasets, and faster in some specific kinds of datasets.
  \item A new algorithm for computing shortest reducts, which will be comparable to 
  		state of the art algorithms in most datasets, and faster in some specific kind of datasets.
  \item A meta-characterization of algorithms' efficiency in relation to some properties of the
  		basic matrix associated to a dataset.
  \item Software and hardware implementations for computing both, all reducts and shortest reducts.
  \end{itemize}


\subsection{Methodology}\label{Methodology} 
\begin{enumerate}
	\item Finding a relationship between some properties of the basic matrix and the fastest 
  		  traversing strategy for computing all reducts.\label{task1_all}
	\begin{itemize}
  		\item Generating a set of random datasets, systematically covering the space of possible combinations
  			  of properties of basic matrices (factorial design). These properties are, for instance, the 
  			  density of ones in the basic matrix, the minimum and maximum number of ones in a row, the 
  			  standard deviation of the density of ones in rows and columns, etc.
  		\item Implementing the main traversing strategies reported in the literature for the computation of all
  			  reducts.
  		\item Generating an information system with the properties of each basic matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extracting a relevant subset of properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation. We will use
  			  Rough Set Theory for this purpose.
  		\item Proposing a new algorithm for computing all reducts using the rules found in
  			  the previous step.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets~\citep{Bache13}.
  	\end{itemize}
  	\item Finding a relationship between the traversed space and the expected cost of traversing strategies.
  	\label{task2_all}
  	\begin{itemize}
  		\item Implementing the main traversing strategies reported in the literature for the computation of all
  			  reducts in such a way that we can collect statistics for every execution stage.
  		\item Making a statistical description  of strategies' runtime cost over synthetic and benchmarking
  			  datasets.
  		\item Finding a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Developing a new algorithm for computing all reducts in information systems.
  	\begin{itemize}
  		\item Proposing a new algorithm for computing all reducts based on the relations found in 
  			  steps~\ref{task1_all} and~\ref{task2_all}.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	
  	\item Finding a relationship between some properties of the basic matrix and the fastest 
  		  traversing strategy for computing globally shortest reducts.\label{task1_short}
	\begin{itemize}
  		\item Implementing the main traversing strategies reported in the literature for the computation of 
  			  minimal length reducts.
  		\item Generating an information system with the properties of each basic matrix, using 
  			  the fastest strategy as decision attribute.
  		\item Extracting the relevant subset of properties for determining a priori the appropriate 
  			  traversing strategy for a given dataset; and the rules governing this relation.
  		\item Proposing a new algorithm for computing shrotest reducts using the rules found in
  			  the previous step.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Finding a relationship between the traversed space and the expected cost of traversing strategies.
  	\label{task2_short}
  	\begin{itemize}
  		\item Implementing the main traversing strategies reported for the computation of globally shortest 
  			  reducts in such a way that we can collect statistics for every execution stage.
  		\item Making a statistical description  of strategies' runtime cost over synthetic and benchmarking.
  		\item Finding a correlation between the traversed space and the expected cost of traversing strategies.
  	\end{itemize}
  	\item Developing a new algorithm for computing shortest reducts in information systems.
  	\begin{itemize}
  		\item Proposing a new algorithm for computing shortest reducts based on the relations found in 
  			  steps~\ref{task1_short} and~\ref{task2_short}.
  		\item Evaluating the proposed algorithm over synthetic and benchmarking datasets.
  	\end{itemize}
  	\item Finally, the proposed algorithms will be redesigned and implemented in a hardware fashion in order to 
  		  evaluate the speed up that can be obtained.
\end{enumerate}

\clearpage 
\subsection{Schedule}
  Table~\ref{tab_Schedule} shows the schedule of the main tasks that will be carried out throughout this research.
 \begin{table}[h!]
		\caption{Research schedule (quarterly\protect\footnotemark).} \label{tab_Schedule}
		\centering
 	\begin{tabular}{|p{8cm}|c|c|c|c|c|c|c|c|c|c|c|c|}
 		\hline
		\multicolumn{1}{|c|}{\multirow{3}{*}{Task}} & \multicolumn{12}{c|}{Quarters}\\
 		\cline{2-13}
		 & 2014 & \multicolumn{3}{c|}{2015} & \multicolumn{3}{c|}{2016} & \multicolumn{3}{c|}{2017}
		 & \multicolumn{2}{c|}{2018} \\
 		\cline{2-13}
		 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
		\hline
		Literature review &\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}\\
		\hline
		Writing the research proposal &\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Critical study of algorithms for computing all reducts in information systems
		&\cellcolor{blue}&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Implementation of algorithms for computing all reducts in information systems
		&&\cellcolor{blue}&\cellcolor{blue}&&&&&&&&&\\
		\hline
		Development of a new algorithm for computing all reducts in information systems
		&&&&\cellcolor[gray]{0.9}&&&&&&&&\\
		\hline
		Critical study of algorithms for computing shortest reducts in information systems
		&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&&&\\
		\hline
		Implementation of algorithms for computing shortest reducts in information systems
		&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&&&\\
		\hline
		Development of a new algorithm for computing shortest reducts in information systems
		&&&&&&&\cellcolor[gray]{0.9}&&&&&\\
		\hline
		Critical study of hardware accelerations of algorithms for computing reducts in information systems
		&&&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Designing and implementing in hardware the proposed algorithms
		&&&&&&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Experimental set-up &\cellcolor{blue}&\cellcolor{blue}&&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&&\\
		\hline
		Experiments run &&&\cellcolor{blue}&\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&
		\cellcolor[gray]{0.9}&&&\\
		\hline
		Writing papers &\cellcolor{blue}&&\cellcolor{blue}&&\cellcolor[gray]{0.9}&&\cellcolor[gray]{0.9}&&
		\cellcolor[gray]{0.9}&&&\\
		\hline
		Writing dissertation &&&&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&
		\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&\cellcolor[gray]{0.9}&&&\\
		\hline
		Submit final draft of dissertation to supervisors &&&&&&&&&&\cellcolor[gray]{0.9}&&\\
		\hline
		Submit final version of dissertation to the PhD committee &&&&&&&&&&&\cellcolor[gray]{0.9}&\\
		\hline
		
 	\end{tabular}             
 \end{table}
 
 \footnotetext{Quarters are: [January-April], [May-August] and [September-December]. Schedule starts in 
 			   September 2014, according to the admission of the student in the PhD. program.}
	  	

\section{Preliminary Results}
	In this section, we expose the advances and results obtained in this PhD research until present.
	Our first studies were directed to hardware implementations of algorithm for computing
	typical testors. Thus, in the subsection~\ref{OnlyTT} we present a new architecture 
	for computing typical testors, which is based on the platform proposed by \cite{Rojas12}. 
	This result has been reported in~\citep{Rodriguez14}. Then, in subsection~\ref{CTH} we
	describe a new platform for computing typical testors, based on the CT\_EXT algorithm.
	This platform was presented to be published in the review \emph{Expert Systems with 
	Applications\footnote{http://www.journals.elsevier.com/expert-systems-with-applications/}}, and
	it is currently under the second revision.
	 
	
\subsection{A Hardware Architecture for Filtering Typical Testors}\label{OnlyTT}
	\cite{Rojas12} presented a hardware-software platform for computing typical testors, based on the BT algorithm, 
	that included a new module for eliminating most of the non typical testors before transferring them to a host 
	software application for final filtering. Main disadvantages of this approach are the huge amount of data that 
	must be transferred to the PC and the extra cost of the final filtering stage in the software component.  
	For this reason, we developed a hardware module for eliminating all non typical testors 
	on the FPGA device, reducing the amount of data that must be transferred to the PC. This architecture is 
	applicable to any algorithm for computing typical testors implemented in FPGA, since it operates in the 
	candidate evaluation process.
	
	In the hardware platform, a feature subset is handled as an $n$-tuple, using a positional 
	representation for all the $n$ attributes of a basic matrix ($BM$). Given a subset $T$, its $n$-tuple 
	representation has a 1 in the corresponding position $j$ for each $c_j \in T$ and 0 otherwise.
	The information of $BM$ is hold in the $BM$ hardware module. This module handles the process of 
	deciding whether an $n$-tuple is a testor of $BM$, by comparing the candidate against each one of the 
	$BM$'s rows.

	In our proposed architecture, an \textit{N~to~N~Decoder} is introduced into each 
	row of the basic matrix. This new component receives as input 
	the result of the AND operation between the current candidate and the corresponding $BM$ row.
	The output from the \textit{N~to~N~Decoder} repeats the input when there is only one bit set
	to 1, and returns the null $n$-tuple $(0,...,0)$ otherwise. For those rows with only one bit having a 
	1 after ANDed with the candidate, the attribute in the position of that bit is indispensable if the 
	candidate is a testor.
	
	Two operations are added to the $BM$ module in order to verify the condition~\ref{cond_2} of the definition
	of reduct in Subsection~\ref{def_reduct}\footnote{Notice that we are, indeed, finding typical testors here. We
	used the definition of reduct from section~\ref{def_reduct} based on the close relation between these two
	concepts~\citep{Lazo15}}.
	First, a bitwise OR operation is performed among the output of the \textit{N~to~N~Decoder} of every row. 
	The result of this operation has a 1 in the positions corresponding to each indispensable attribute in the
	current 	candidate. This value is then compared to the current candidate, and 	given that this comparison 
	holds equality and the current candidate is a testor; we say that the current candidate is a typical testor.
	
	Lets us take for example the basic matrix shown in Table~\ref{tab_SDM}. We are going to illustrate the 
	operation of the proposed architecture using two testors for this basic matrix. First we will evaluate 
	the candidate $\{c_1,c_2\}$ which is a typical testor. Secondly, the candidate $\{c_1,c_2,c_3\}$ will 
	be evaluated. This last attribute set is a superset of $\{c_1,c_2\}$ and thus, it is not a typical testor.
	
	\newcolumntype{Y}{>{\centering\arraybackslash}X}% This do the magic of equal spaced columns
	\begin{table}[htb]
    \begin{minipage}{.5\linewidth}
		%\renewcommand{\arraystretch}{1.3}
		\caption{An example of typical testor}
		\label{tabTypical}
		\centering
		\begin{tabularx}{.7\textwidth}{YYY|YYY}
		 	\hline                       
	  		\multicolumn{3}{c|}{Cand. $\{c_1, c_2\}$} & 
	  		\multicolumn{3}{c}{Decoder output} \\
	  		\hline
			$c_1$ & $c_2$ & $c_3$ &
	  		$c_1$ & $c_2$ & $c_3$ \\
	  		\hline
	  		1 & 0 & 0 & 1 & 0 & 0\\
	  		1 & 1 & 0 & 0 & 0 & 0\\
	  		0 & 1 & 0 & 0 & 1 & 0\\
	  		\hline  
	  		\multicolumn{3}{c|}{Candidate $=$} & 1 & 1 & 0\\
	  		\hline  
		\end{tabularx}
 	\end{minipage}
    \begin{minipage}{.5\linewidth}
		%\renewcommand{\arraystretch}{1.3}
		\caption{An example of a non typical testor}
		\label{tabNonTypical}
		\centering
		\begin{tabularx}{.7\textwidth}{YYY|YYY}
		 	\hline                       
	  		\multicolumn{3}{c|}{Cand. $\{c_1, c_2, c_3\}$} & 
	  		\multicolumn{3}{c}{Decoder output} \\
	  		\hline
			$c_1$ & $c_2$ & $c_3$ &
	  		$c_1$ & $c_2$ & $c_3$ \\
	  		\hline
	  		1 & 0 & 1 & 0 & 0 & 0\\
	  		1 & 1 & 0 & 0 & 0 & 0\\
	  		0 & 1 & 1 & 0 & 0 & 0\\
	  		\hline  
	  		\multicolumn{3}{c|}{Candidate $\neq$} & 0 & 0 & 0\\
	  		\hline  
		\end{tabularx}
 	\end{minipage}
	\end{table}
	
	Left rows of Table~\ref{tabTypical} and Table~\ref{tabNonTypical} show the result of the AND operation 
	between each row of $BM$ and the corresponding candidate. Rows in the right side show the decoder output 
	taking as input its corresponding left row. In the last row, the result of an OR operation over all 
	above $n$-tuples is shown. According to our previous explanation, the candidate $\{c_1,c_2\}$ is an 
	typical testor given that the result of the OR operation is equal to the candidate itself; while the
	candidate $\{c_1,c_2,c_3\}$ is not.
	
\subsubsection{Evaluation and Discussion}
	This proposed architecture sends only typical testors from the FPGA device to the 
	host PC. This modification eliminates the final filtering stage in the software component of the 
	hardware-software platform~\citep{Rojas12}. The main draw back of our proposed modification is its 
	dependence with basic matrix dimensions which could lead to a larger hardware realization when the
	number of rows is much bigger than the number of columns in the basic matrix. 
	
	The final filtering stage in the software component needed in the original platform~\cite{Rojas12} checks 
	every pair of testors received from the FPGA. Testors which are superset of any other testor are 
	eliminated as they do not satisfy the condition~\ref{cond_2} stated in Subsection~\ref{def_reduct}. We 
	can establish the lower boundary of the computational complexity for this process as $N(N-1)/2$, 
	where $N$ is the number of typical testors in the basic matrix. 
	
	Table~\ref{tabTimes} shows the runtime, including the testors computation and the final filtering stage, 
	for some basic matrices obtained from real data. For this purpose eight standard datasets from the UCI
	Repository of Machine Learning \citep{Bache13} were used.
	Columns in Table~\ref{tabTimes} show the dataset name, the number of candidates tested by the BT algorithm,
	the number of typical testors found, the runtime for the main algorithm execution in the 
	FPGA device and the final filtering stage in the host PC. For these runtime calculations, 
	a frequency of 3.6GHz was used for software execution and 50MHz for the FPGA architecture.
	
	\begin{table}[htb]
	%\renewcommand{\arraystretch}{1.3}
	\caption{Algorithm execution and typical testors filtering stage runtimes for benchmarking datasets}
	\label{tabTimes}
	\centering
	\begin{tabular}{lccccc}
	 	\hline                       
	  	Dataset & Tested Candidates & Typical Testors & FPGA runtime ($ \mu $s) & PC runtime ($ \mu $s)\\
	  	\hline
	  	liverdisorder& 16   & 9    & 0.32  & 0.04 \\
	  	zoo          & 20   & 7    & 0.40  & 0.02 \\
	  	krvskp       & 36   & 4    & 0.72  & 0.01 \\	  	
	  	shuttle      & 38   & 19   & 0.76  & 0.17 \\
	  	tic-tac-toe  & 44   & 9    & 0.88  & 0.04 \\
	  	australian   & 330  & 44   & 6.60  & 0.95 \\
	  	lymphography & 802  & 75   & 16.04 & 2.77 \\
	  	german       & 16921& 846  & 338.42& 357.44 \\
	 	\hline 
	\end{tabular}
	\end{table}
	
	For most of datasets shown in Table~\ref{tabTimes} the processing time taken for FPGA and PC executions are 
	of the same order of magnitude. For those basic matrices with a large number of typical testors, the final 
	filtering stage could be  even more expensive than the main testors computation, as is the case for the dataset 
	labelled \textit{german}. The total runtime for the previous architecture is the sum of the FPGA and the 
	PC runtime. Our proposed platform will require only the FPGA runtime. Although finding all typical testors 
	for these datasets does not constitute a complex computational problem, they serve to show our point.

\subsection{A Hardware Architecture based the CT\_EXT Algorithm}\label{CTH}
	Here, we present an efficient hardware software platform for computing typical
	testors based on the CT-EXT algorithm proposed in~\citep{Sanchez07}. The main contribution of this
	work is the design and implementation of a hardware architecture that traverses the search space in a 
	different order than that presented in \citep{Rojas07, Rojas12,Rodriguez14}. This strategy evaluates less
	candidate subsets than previous architectures, which results in shorter runtime. In comparison to the 
	software versions of CT-EXT~\citep{Sanchez07, Sanchez10}, our proposal evaluates a candidate every clock 
	cycle, which leads to a faster execution. The runtime gain of our new hardware software platform is
	demonstrated throughout	experiments over synthetic datasets.
	
\subsubsection{CT-EXT algorithm}
	CT-EXT is one of the fastest algorithms for computing all typical testors reported in the 
	state of the art~\citep{Sanchez07,Sanchez10,Piza14}. In order to describe this algorithm we 
	introduce some definitions and notations.	
	
	Let $T$ be a subset of attributes, $T$ is a testor of the basic matrix $BM$ if the attributes in $T$ do 
	not form a zero row 	in $BM$. It means that every row in $BM$ has at least a 1 in those columns corresponding 
	to attributes belonging to $T$. We say that a testor $T$ is a typical testor if all the proper subsets of 
	$T$ are not	testors.
	
	We can interpret a typical testor as a subset of attributes being jointly sufficient and individually
	necessary to differentiate every pair of objects belonging to different classes.
	
	During the search, CT-EXT follows the idea that an attribute contributes to a subset $T$ (candidate to 
	be a typical testor) if after adding this attribute to $T,$ the attributes in $T$ form less zero rows 
	in $BM$ than the amount of zero rows before adding the attribute. This idea is used for pruning the search
	space.
	
	The following proposition, introduced and proved in~\citep{Sanchez07}, constitutes the basis for the CT-EXT
	algorithm.
	
	\begin{proposition}\label{prop1} 
	Given $T \subseteq R$ and $c_j \in R$ such that $c_j \notin T$. If $c_j$ 
	does not contribute to $T$, then $T\cup\{c_j\}$ cannot be a subset of any typical testor.
	\end{proposition}
	
	Algorithm~\ref{ctext_algo} shows the pseudocode of CT-EXT, a detailed explanation of
	this algorithm can be seen in~\citep{Sanchez07}. The function SortBM$(BM)$ sorts the basic matrix as follows.
	Randomly select one of the rows of $BM$ with the fewest number of 1's. The selected row goes first and
	all columns in which it has a 1 are moved to the left.
	
	\begin{algorithm}
	\begin{spacing}{1.0}
	\begin{small}
	\caption{CT-EXT algorithm}\label{ctext_algo}
	\begin{algorithmic}[1]
	%\Procedure{CT-EXT}{}
	\State \textbf{Input: } $BM$ - basic matrix with $m$ rows and $n$ columns.
	\State \textbf{Output: } $TT$ - set of typical testors.
	\State $TT \gets \{\}$
	\State $j \gets 0$ \Comment{first attribute from $BM$ to be analyzed}
	\State $BM \gets$ SortBM$(BM)$
	\While{$BM[0,j] \neq 0$}\label{row1condition}
	\State $T \gets \{c_j\}$ \Comment{current attribute subset}
	\State $testor, typical, zero\_rows \gets$ Evaluate$(BM,T)$
	\If{$testor=TRUE$}
	\If{$typical=TRUE$} \Comment{$T$ is a typical testor}
	\State $TT \gets TT \cup T$
	\EndIf
	\Else
	\State $i \gets j+1$
	\While{$i < n$}
	\State $T \gets T \cup \{c_i\}$
	\State $zero\_rows\_last \gets zero\_rows$
	\State $testor, typical, zero\_rows \gets$ Evaluate$(BM,T)$
	\If{$zero\_rows = zero\_rows\_last$}
	\State $T \gets T \setminus \{c_i\}$ \Comment{attribute $X_i$ does not contribute}
	\Else
	\If{$testor=TRUE$}
	\If{$typical=TRUE$}
	\State $TT \gets TT \cup T$
	\EndIf
	\State $T \gets T \setminus \{c_i\}$
	\State $zero\_rows \gets zero\_rows\_last$
	\EndIf
	\EndIf
	\If{$i = n-1$}
	\State $k \gets$ LastOne($T$)
	\If{$k = i$}
	\State $T \gets T \setminus \{c_k\}$
	\State $k \gets$ LastOne($T$)
	\EndIf
	\If{$k \neq j$}
	\State $T \gets T \setminus \{c_k\}$
	\State $testor, typical, zero\_rows \gets$ Evaluate$(BM,T)$
	\State $i \gets k+1$
	\Else
	\State $i \gets i+1$
	\EndIf
	\Else
	\State $i \gets i+1$
	\EndIf
	\EndWhile
	\EndIf
	\State $j \gets j+1$
	\EndWhile
	%\EndProcedure
	\end{algorithmic}
	\end{small}
	\end{spacing}
	\end{algorithm}
	
	The function Evaluate$(BM,T)$ returns three values: $testor$, $typical$ and $zero\_rows$. $testor$ is 
	TRUE if the set $T$ is a testor of $BM$ and FALSE otherwise. $typical$ is TRUE if the set $T$ is a
	typical testor and FALSE otherwise. $zero\_rows$ is the amount of zero rows of $T$. The function
	LastOne($T$) returns the position of the rightmost element in the set $T$.
	
	The proposed hardware platform replaces candidates generator module in the design of \citep{Rodriguez14}
	by a brand new module. Minor changes are introduced in the rest of the platform in order to integrate
	this new module. Essentially, the general scheme from previous architectures is followed.
	
\subsubsection{Evaluation and Discussion}
	In order to show the performance of the proposed platform, it was compared against a software 
	implementation of the CT-EXT algorithm~\citep{Sanchez07} and the BT hardware platform previously 
	reported in~\citep{Rodriguez14}; which is the most recent hardware implementation for computing typical
	testors reported in the literature.
	
	Either CT-EXT or BT hardware implementations are capable of evaluating a candidate per clock 
	cycle. If both architectures are running at the same frequency, as it will be the case in our experiments, 
	there are two reasons for differences in running time. The first one is the time taken for reorganization 
	of basic matrix, which is a more complex process in BT, altough it can be neglected as shown in
	\citep{Rojas12}. The second and the most relevant, is the amount of candidates to be evaluated. 
	
	Regarding to the software implementation, the CT-EXT hardware platform has two disadvantages. First, 
	VHDL code is generated for each $BM$ data and a process of synthesis must be executed previously to 
	executing the algorithm; while this is unnecessary in the software version of CT-EXT. Secondly, the 
	software will be running in a PC at a frequency of 3.10GHz while FPGA architecture will run at 50MHz. 
	
	These disadvantages make the hardware approach useful (faster) under two conditions. First, the number of 
	candidates to be evaluated is big enough to overcome the synthesis overhead. Second, the dimensions of the 
	$BM$ are big enough to provide a considerable speed up of the candidate evaluation process. Although 
	the hardware architecture could be designed for a fixed maximum matrix size and receive the $BM$ through the 
	USB port, by doing this, the size of the problem which can be solved would be significantly reduced. The 
	synthesis process comprehend an optimization of the design, taking advantage of the $BM$ data distribution for 
	the reduction of the generated hardware configuration. The number of operations for the evaluation of a single 
	candidate, in the software approach, is proportional to the number of rows and it is directly related to the 
	number of columns in the $BM$. Using this approach it is possible to achieve a significant reduction in the
	processing time, even if operating at a much lower clock frequency, by evaluating a candidate on each clock
	cycle.
	
	With these points in mind and in order to show the usability of the proposed platform, 
	three kinds of basic matrices were randomly generated. Each type containing different percentage of 1's: 
	\begin{enumerate}
		\item Very-low density matrices: approximately 8\%.
		\item Low density matrices: approximately 33\%.
		\item Medium density matrices: approximately 45\%.
	\end{enumerate}
	
	Higher density matrices were discarded because they do not constitute a computationally expensive problem, 
	as stated by \cite{Rojas12}. Here after, we will be referring to these three sets of matrices by its 
	approximate density of~1's.
	
	\begin{figure}[htb]
	\centering
	\begin{minipage}{.5\textwidth}
	  \centering
	   \includegraphics[width=6cm , height=6cm]{low_density.eps}
	  \caption{Total runtime for density 8\%.}
	  \label{fig:result1}
	\end{minipage}%
	\begin{minipage}{.5\textwidth}
	  \centering
	   \includegraphics[width=6cm , height=6cm]{med_density.eps}
	  \caption{Total runtime for density 33\%.}
	  \label{fig:result2}
	\end{minipage}
	\end{figure}
	
	\begin{figure}[htb]
	    \begin{center}
	       \includegraphics[height=6cm]{med48_density.eps}
	    \end{center}
	\caption{Total runtime for density 45\%.}
	\label{fig:result3}
	\end{figure}	
	
	For our experiments, 30 basic matrices of different sizes were randomly generated. 
	A random number generator was used to generate rows, which are
	filtered for the minimum and maximum number of 1's allowed. In this way the desired density was
	controlled. If accepted, the row is verified as basic against the saved rows. Basic rows 
	are saved until the desired number of rows is reached. 
	
	For the hardware platforms, we measure the runtimes including the time for the following stages: $BM$ 
	input parsing and VHDL code generation, synthesis process, and typical testor computation (with the
	hardware component running at 50MHz). The number of rows for each type of matrices is conditioned by the
	dimensions of the biggest matrix that may be synthesized at the desired running frequency.
	All experiments are performed using an Intel(R) Core(TM) i5-2400 CPU @ 3.10GHz for software executions and 
	an Atlys board, powered by a Spartan-6 LX45 FPGA device, for the hardware components.
	Figures\,\ref{fig:result1}, \,\ref{fig:result2}, and \,\ref{fig:result3} show graphics of the runtime 
	(in hours) for the three types of basic matrices. 
	
	The proposed CT-EXT hardware platform (CTH) results were taken as reference for axis limits in 
	Figures\,\ref{fig:result1}, \,\ref{fig:result2}, and \,\ref{fig:result3}.
	Slowest executions of the CT-EXT software implementation (CTS) are not shown in order to keep clarity in 
	the figures. 
	The hardware platform for BT (BTH) was not able to met the constrain of 50MHz clock frequency for some 
	matrices and running it at a lower frequency produces longer execution time; which fall out of the limits 
	of the figures.
	
	Experiment results show that the proposed platform beats the software implementation of
	the CT-EXT algorithm, with ratios of around \textbf{one order} of magnitude. However, for large 
	enough datasets this improvement could be significantly higher, as can be inferred from 
	Figure\,\ref{fig:result3}.
%-------------------------------------------------------------------------------
% Bibliography
%-------------------------------------------------------------------------------
\newpage 
\bibliography{mybib}{}
\bibliographystyle{authordate1}
\end{document}
