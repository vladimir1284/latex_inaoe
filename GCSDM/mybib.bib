@article{Aguila84,
author = {\'{A}guila, L. and Ruiz-Shulcloper, Jos\'{e}},
journal = {Revista Ciencias Matem\'{a}ticas},
mendeley-groups = {Reducts},
number = {3},
title = {{Algoritmo CC para la elaboraci\'{o}n de la informaci\'{o}n k- valente en problemas de Reconocimiento de Patrones}},
volume = {5},
year = {1984}
}

@incollection{Alba14,
author = {Alba-Cabrera, Eduardo and Ibarra-Fiallo, Julio and Godoy-Calderon, Salvador and Cervantes-Alonso, Fernando},
booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
file = {:home/bill/Documents/Mendeley Desktop/2014/Alba-Cabrera et al. - 2014 - YYC A Fast Performance Incremental Algorithm for Finding Typical Testors.pdf:pdf},
mendeley-groups = {Reducts},
pages = {416--423},
publisher = {Springer},
title = {{YYC: A Fast Performance Incremental Algorithm for Finding Typical Testors}},
year = {2014}
}
@article{Ayaquica97,
author = {Ayaquica, I O},
journal = {Memorias del II Taller Iberoamericano de Reconocimiento de Patrones},
mendeley-groups = {Reducts},
pages = {141--148},
title = {{Un nuevo algoritmo de escala exterior para el c\'{a}lculo de testores t\'{\i}picos}},
year = {1997}
}

@article{Bazan2001,
author = {Bazan, Jg and Szczuka, Marcin},
file = {:home/bill/Documents/Mendeley Desktop/2001/Bazan, Szczuka - 2001 - RSES and RSESlib-a collection of tools for rough set computations.pdf:pdf},
journal = {Rough Sets and Current Trends in Computing},
mendeley-groups = {Reducts},
pages = {106--113},
title = {{RSES and RSESlib-a collection of tools for rough set computations}},
url = {http://link.springer.com/chapter/10.1007/3-540-45554-X\_12},
year = {2001}
}
@misc{Bache13,
author = {Bache, Kevin and Lichman, Moshe},
booktitle = {URL http://archive. ics. uci. edu/ml},
mendeley-groups = {Reducts},
title = {{UCI machine learning repository}},
volume = {901},
year = {2013}
}
@article{Bjorvand97,
author = {Bjorvand, Anders Torvill and Komorowski, Jan},
file = {:home/vladimir/Documents/Mendeley Desktop/Vhwv, Dojrulwkpv, Uhgxfwlrq/Unknown/Vhwv, Dojrulwkpv, Uhgxfwlrq - Unknown - H Zloo Frqfhqwudwh Rq Rqo Rqh Uhsuhvhqwdwlrq Dqg H Shulphqw Ixuwkhu Zlwk Wkh Ilwqhvv Ixqfwl.pdf:pdf},
journal = {Wissenschaft \& Technik Verlag},
mendeley-groups = {Reducts},
pages = {601--606},
title = {{Practical applications of genetic algorithms for efficient reduct computation}},
volume = {4},
year = {1997}
}
@article{Bravo83,
author = {Bravo, A.},
journal = {Revista Ciencias Matematicas},
mendeley-groups = {Reducts},
number = {2},
pages = {123--144},
title = {{Algorithm CT for Calculating the Typical Testors of k-valued Matrix}},
volume = {4},
year = {1983}
}
@article{Chebrolu2015,
author = {Chebrolu, Srilatha and Sanjeevi, Sriram G},
file = {:home/vladimir/Documents/Mendeley Desktop/Chebrolu, Sanjeevi/Unknown/Chebrolu, Sanjeevi - 2015 - Attribute Reduction on Continuous Data in Rough Set Theory using Ant Colony Optimization Metaheuristic.pdf:pdf},
isbn = {9781450333610},
keywords = {ant colony optimization,attribute reduction,boolean reasoning,discernibility,discretization,in-,rough set theory},
mendeley-groups = {Reducts},
pages = {17--24},
title = {{Attribute Reduction on Continuous Data in Rough Set Theory using Ant Colony Optimization Metaheuristic}},
year = {2015}
}
@article{Cheguis55,
author = {Cheguis, I. A. and Yablonskii, S. V.},
journal = {Uspieji Matematicheskij Nauk},
mendeley-groups = {Reducts},
number = {66},
pages = {182--184},
title = {{About Testors for Electrical Outlines}},
volume = {4},
year = {1955}
}
@article{Chen10,
abstract = {Rough set theory is one of the effective methods to feature selection, which can preserve the meaning of the features. The essence of rough set approach to feature selection is to find a subset of the original features. Since finding a minimal subset of the features is a NP-hard problem, it is necessary to investigate effective and efficient heuristic algorithms. Ant colony optimization (ACO) has been successfully applied to many difficult combinatorial problems like quadratic assignment, traveling salesman, scheduling, etc. It is particularly attractive for feature selection since there is no heuristic information that can guide search to the optimal minimal subset every time. However, ants can discover the best feature combinations as they traverse the graph. In this paper, we propose a new rough set approach to feature selection based on ACO, which adopts mutual information based feature significance as heuristic information. A novel feature selection algorithm is also given. Jensen and Shen proposed a ACO-based feature selection approach which starts from a random feature. Our approach starts from the feature core, which changes the complete graph to a smaller one. To verify the efficiency of our algorithm, experiments are carried out on some standard UCI datasets. The results demonstrate that our algorithm can provide efficient solution to find a minimal subset of the features. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Chen, Yumin and Miao, Duoqian and Wang, Ruizhi},
doi = {10.1016/j.patrec.2009.10.013},
file = {:home/vladimir/Documents/Mendeley Desktop/Chen, Miao, Wang/Pattern Recognition Letters/Chen, Miao, Wang - 2010 - A rough set approach to feature selection based on ant colony optimization.pdf:pdf},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Ant colony optimization,Data mining,Feature selection,Mutual information,Rough sets},
mendeley-groups = {Reducts},
number = {3},
pages = {226--233},
publisher = {Elsevier B.V.},
title = {{A rough set approach to feature selection based on ant colony optimization}},
url = {http://dx.doi.org/10.1016/j.patrec.2009.10.013},
volume = {31},
year = {2010}
}
@article{Chen2012,
author = {Chen, Degang and Zhao, Suyun and Zhang, Lei and Yang, Yongping and Zhang, Xiao},
doi = {10.1109/TKDE.2011.89},
file = {:home/vladimir/Documents/Mendeley Desktop/Chen et al/IEEE Transactions on Knowledge and Data Engineering/Chen et al. - 2012 - Sample pair selection for attribute reduction with rough set.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Rough set,attribute reduction,sample pair core,sample pair selection},
mendeley-groups = {Reducts},
number = {11},
pages = {2080--2093},
title = {{Sample pair selection for attribute reduction with rough set}},
volume = {24},
year = {2012}
}
@article{chen2015,
  title={The relationship between attribute reducts in rough sets and minimal vertex covers of graphs},
  author={Chen, Jinkun and Lin, Yaojin and Lin, Guoping and Li, Jinjin and Ma, Zhouming},
  journal={Information Sciences},
  volume={325},
  pages={87--97},
  year={2015},
  publisher={Elsevier}
}
@article{Chen15,
author = {Chen, Yumin and Zhu, Qingxin and Xu, Huarong},
doi = {10.1016/j.knosys.2015.02.002},
file = {:home/vladimir/Documents/Mendeley Desktop/Chen, Zhu, Xu/Knowledge-Based Systems/Chen, Zhu, Xu - 2015 - Knowledge-Based Systems Finding rough set reducts with fish swarm algorithm.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
mendeley-groups = {Reducts},
pages = {22--29},
publisher = {Elsevier B.V.},
title = {{Finding rough set reducts with fish swarm algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705115000337},
volume = {81},
year = {2015}
}
@article{Chouchoulas01,
abstract = {The volume of electronically stored information increases exponentially as the state of the art progresses. Automated information filtering (IF) and information retrieval (IR) systems are therefore acquiring rapidly increasing prominence. However, such systems sacrifice efficiency to boost effectiveness. Such systems typically have to cope with sets of vectors of many tens of thousands of dimensions. Rough set (RS) theory can be applied to reducing the dimensionality of data used in IF/IR tasks, by providing a measure of the information content of datasets with respect to a given classification. This can aid IF/IR systems that rely on the acquisition of large numbers of term weights or other measures of relevance. This article investigates the applicability of RS theory to the IF/IR application domain and compares this applicability with respect to various existing TC techniques. The ability of the approach to generalize, given a minimum of training data is also addressed. The background of RS theory is presented, with an illustrative example to demonstrate the operation of the RS-based dimensionality reduction. A modular system is proposed which allows the integration of this technique with a large variety of different IF/IR approaches. The example application, categorization of E-mail messages, is described. Systematic experiments and their results are reported and analyzed.},
author = {Chouchoulas, Alexios and Shen, Qiang},
doi = {10.1080/088395101753210773},
file = {:home/vladimir/Documents/Mendeley Desktop/Chouchoulas, Shen/Applied Artificial Intelligence/Chouchoulas, Shen - 2001 - Rough set-aided keyword reduction for text categorization.pdf:pdf},
isbn = {0883-9514},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
keywords = {088395101753210773,10,1080,15,843-873,9,an,applied artificial intelligence,article,doi,dx,http,international journal,org,please scroll down for,reduction for text categorization,to link to this},
mendeley-groups = {Reducts},
number = {January},
pages = {843--873},
title = {{Rough set-aided keyword reduction for text categorization}},
volume = {15},
year = {2001}
}
@inproceedings{Cumplido06,
author = {Cumplido, Ren\'{e} and Carrasco-Ochoa, Jes\'{u}s Ariel and Feregrino, Claudia},
booktitle = {CIARP 2006},
file = {:home/bill/Documents/Mendeley Desktop/2006/Cumplido, Carrasco-ochoa, Feregrino - 2006 - On the Design and Implementation of a High Performance Configurable Architecture for Testor.pdf:pdf},
isbn = {3540465561},
issn = {03029743},
mendeley-groups = {Reducts},
pages = {665--673},
title = {{On the Design and Implementation of a High Performance Configurable Architecture for Testor Identification LNCS.pdf}},
year = {2006}
}

@article{Davis62,
author = {Davis, Martin and Logemann, George and Loveland, Donald},
file = {:home/vladimir/Documents/Mendeley Desktop/Davis, Logemann, Loveland/Communications of the ACM/Davis, Logemann, Loveland - 1962 - A machine program for theorem-proving.pdf:pdf},
journal = {Communications of the ACM},
mendeley-groups = {Reducts},
number = {7},
pages = {394--397},
publisher = {ACM},
title = {{A machine program for theorem-proving}},
volume = {5},
year = {1962}
}
@article{Feng2012,
author = {Feng, Y a N and Weihua, G U I and Yong, Chen and Yongfang, X I E and Huifeng, R E N},
file = {:home/vladimir/Documents/Mendeley Desktop/Feng et al/Unknown/Feng et al. - 2012 - Rough Set Knowledge Reduction Approach Based on Improving Genetic Algorithm.pdf:pdf},
isbn = {9781479970162},
keywords = {Rough Set, Genetic Algorithm, Support Degree, Impo,dissimilarity degree,genetic algorithm,importance degree,rough set,support degree,粗糙集，遗传算法，支持度，重要度，相异度},
mendeley-groups = {Reducts},
pages = {1967--1971},
title = {{Rough Set Knowledge Reduction Approach Based on Improving Genetic Algorithm}},
year = {2012}
}
@article{Grzes13,
author = {Grze, Tomasz},
file = {:home/vladimir/Documents/Mendeley Desktop/Grze/Unknown/Grze - 2013 - FPGA in Rough Set Based Core and Reduct Computation.pdf:pdf},
keywords = {core,fpga,hardware,reduct,rough sets},
mendeley-groups = {Reducts},
pages = {263--270},
title = {{FPGA in Rough Set Based Core and Reduct Computation}},
year = {2013}
}
@article{Haveraaen01,
abstract = {The advent of computers with a very dynamic run-time behaviour, such as the SGI Origin 2000 series, makes it very difficult to asses the runtime behaviour of programs in order to compare efficiency of implementations. Here we empirically investigate some simple statistical measures for estimat- ing CPU usage under such circumstances, ending up with a very simple and seemingly accurate estimation technique: the minimum run-time of 3-5 executions of the program when the machine is lightly loaded.},
author = {Haveraaen, Magne and Informatikk, Institutt and Bergen, Universitetet},
file = {:home/bill/Documents/Mendeley Desktop/2001/perfor-5d.pdf:pdf},
journal = {Norsk Informatikkonferanse},
mendeley-groups = {Reducts,Reducts/runtime},
pages = {176--185},
title = {{Some Statistical Performance Estimation Techniques for Dynamic Machines}},
url = {http://www.ii.uib.no/saga/papers/perfor-5d.pdf},
year = {2001}
}
@article{Hedar2015,
author = {Hedar, Abdel-rahman and Sewisy, Adel A},
file = {:home/vladimir/Documents/Mendeley Desktop/Hedar, Sewisy/Unknown/Hedar, Sewisy - 2015 - Rough Sets Attribute Reduction Using an Accelerated Genetic Algorithm.pdf:pdf},
isbn = {9781479986767},
keywords = {4,algorithm starts with an,an iterative local search,arbitrary,attribute reduction,for each iteration,genetic algorithm,heuristic knowledge,in that work,it tries to,problem,rough sets,significance are used as,solution for solving a},
mendeley-groups = {Reducts},
title = {{Rough Sets Attribute Reduction Using an Accelerated Genetic Algorithm}},
year = {2015}
}
@article{Jensen01,
abstract = {Most people store ‘bookmarks’ to web pages. These allow the user to return to a web page later on, without having to remember the exact URL address. People attempt to organise their bookmark databases by filing bookmarks under categories, themselves arranged in a hierarchical fashion. As the maintenance of such large repositories is difficult and time-consuming, a tool that automatically categorises bookmarks is required. This paper investigates how rough set theory can help extract information out of this domain, for use in an experimental automatic bookmark classification system. In particular, work on rough set dependency degrees is applied to reduce the otherwise high dimensionality of the feature patterns used to characterize bookmarks. A comparison is made between this approach to data reduction and a conventional entropy-based approach.},
author = {Jensen, Richard and Shen, Qiang},
file = {:home/vladimir/Documents/Mendeley Desktop/Jensen, Shen/Web Intelligence Research and Development/Jensen, Shen - 2001 - A rough set-aided system for sorting WWW bookmarks(2).pdf:pdf;:home/vladimir/Documents/Mendeley Desktop/Jensen, Shen/Web Intelligence Research and Development/Jensen, Shen - 2001 - A rough set-aided system for sorting WWW bookmarks.pdf:pdf},
journal = {Web Intelligence: Research and Development},
mendeley-groups = {Reducts},
pages = {95--105},
publisher = {Springer},
title = {{A rough set-aided system for sorting WWW bookmarks}},
year = {2001}
}
@article{Jensen03,
abstract = {Feature selection refers to the problem of selecting those input features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition and signal processing. In particular, this has found successful application in tasks that involve datasets containing huge numbers of features (in the order of tens of thousands), which would be impossible to process further. Recent examples include text processing and web content classi cation. Rough set theory has been used as such a dataset pre-processor with much success, but current methods are inadequate at nding minimal reductions.},
author = {Jensen, R and Shen, Q},
file = {:home/vladimir/Documents/Mendeley Desktop/Jensen, Shen/Proceedings of the 2003 UK workshop on/Jensen, Shen - 2003 - Finding rough set reducts with ant colony optimization.pdf:pdf},
journal = {Proceedings of the 2003 UK workshop on},
number = {2},
pages = {15--22},
title = {{Finding rough set reducts with ant colony optimization}},
url = {http://users.aber.ac.uk/rkj/pubs/papers/antRoughSets.pdf},
volume = {1},
year = {2003}
}
@article{Jensen04,
abstract = { Semantics-preserving dimensionality reduction refers to the problem of selecting those input features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition, and signal processing. This has found successful application in tasks that involve data sets containing huge numbers of features (in the order of tens of thousands), which would be impossible to process further. Recent examples include text processing and Web content classification. One of the many successful applications of rough set theory has been to this feature selection area. This paper reviews those techniques that preserve the underlying semantics of the data, using crisp and fuzzy rough set-based methodologies. Several approaches to feature selection based on rough set theory are experimentally compared. Additionally, a new area in feature selection, feature grouping, is highlighted and a rough set-based feature grouping technique is detailed.},
author = {Jensen, Richard and Shen, Qiang},
doi = {10.1109/TKDE.2004.96},
file = {:home/vladimir/Documents/Mendeley Desktop/Jensen, Shen/IEEE Transactions on Knowledge and Data Engineering/Jensen, Shen - 2004 - Semantics-preserving dimensionality reduction Rough and fuzzy-rough-based approaches.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Dimensionality reduction,Feature selection,Feature transformation,Fuzzy-rough selection,Rough selection},
mendeley-groups = {Reducts},
number = {12},
pages = {1457--1471},
title = {{Semantics-preserving dimensionality reduction: Rough and fuzzy-rough-based approaches}},
volume = {16},
year = {2004}
}
@article{Jensen14,
abstract = {Feature selection refers to the problem of selecting those input features that are most predictive of a given outcome; a problem encountered in many areas such as machine learning, pattern recognition and signal processing. In particular, solution to this has found successful application in tasks that involve datasets containing huge numbers of features (in the order of tens of thousands), which would otherwise be impossible to process further. Recent examples include text processing and web content classification. Rough set theory has been used as such a dataset pre-processor with much success, but current methods are inadequate at finding globally minimal reductions, the smallest sets of features possible. This paper proposes a technique that considers this problem from a propositional satisfiability perspective. In this framework, globally minimal subsets can be located and verified. ?? 2013 Elsevier Inc. All rights reserved.},
author = {Jensen, Richard and Tuson, Andrew and Shen, Qiang},
doi = {10.1016/j.ins.2013.07.033},
file = {:home/vladimir/Documents/Mendeley Desktop/Jensen, Tuson, Shen/Information Sciences/Jensen, Tuson, Shen - 2014 - Finding rough and fuzzy-rough set reducts with SAT.pdf:pdf},
isbn = {3540286535},
issn = {00200255},
journal = {Information Sciences},
keywords = {Boolean satisfiability,Feature selection,Fuzzy rough set theory,Rough set theory},
mendeley-groups = {Reducts},
pages = {100--120},
title = {{Finding rough and fuzzy-rough set reducts with SAT}},
volume = {255},
year = {2014}
}
@article{Jiang15,
author = {Jiang, Yu and Yu, Yang},
doi = {10.1007/s00500-015-1638-0},
file = {:home/bill/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Yu - 2015 - Minimal attribute reduction with rough set based on compactness discernibility information tree.pdf:pdf},
issn = {1432-7643},
journal = {Soft Computing},
keywords = {compactness discernibility information tree,discernibility matrix,minimal,reduction,rough set},
mendeley-groups = {Reducts},
number = {2009},
pages = {1--11},
title = {{Minimal attribute reduction with rough set based on compactness discernibility information tree}},
url = {http://link.springer.com/10.1007/s00500-015-1638-0},
year = {2015}
}
@article{Jiao10,
abstract = {Feature selection is a key issue in the research on rough set theory. However, when handling large-scale data, many current feature selection methods based on rough set theory are incapable. In this paper, two novel feature selection methods are put forward based on decomposition and composition principles. The idea of decomposition and composition is to break a complex table down into a master-table and several sub-tables that are simpler, more manageable and more solvable by using existing induction methods, then joining them together in order to solve the original table. Compared with some traditional methods, the efficiency of the proposed algorithms can be illustrated by experiments with standard datasets from UCI database. ?? 2010 Elsevier Ltd. All rights reserved.},
author = {Jiao, Na and Miao, Duoqian and Zhou, Jie},
doi = {10.1016/j.eswa.2010.03.039},
file = {:home/vladimir/Documents/Mendeley Desktop/Jiao, Miao, Zhou/Expert Systems with Applications/Jiao, Miao, Zhou - 2010 - Two novel feature selection methods based on decomposition and composition.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Composition,Decomposition,Feature selection,Master-table,Sub-table},
mendeley-groups = {Reducts},
number = {12},
pages = {7419--7426},
publisher = {Elsevier Ltd},
title = {{Two novel feature selection methods based on decomposition and composition}},
url = {http://dx.doi.org/10.1016/j.eswa.2010.03.039},
volume = {37},
year = {2010}
}
@inproceedings{Johnson74,
author = {Johnson, David S},
booktitle = {Proceedings of the fifth annual ACM symposium on Theory of computing},
file = {:home/vladimir/Documents/Mendeley Desktop/Johnson/Proceedings of the fifth annual ACM symposium on Theory of computing/Johnson - 1973 - Approximation algorithms for combinatorial problems.pdf:pdf},
mendeley-groups = {Reducts},
organization = {ACM},
pages = {38--49},
title = {{Approximation algorithms for combinatorial problems}},
year = {1973}
}
@article{Kanazawa11,
abstract = {Formal verification is one of the most important applications of the satisfiability (SAT) problem. WSAT and its variants are one of the best performing stochastic local search algorithms. In this paper, we propose an FPGA solver for SAT-encoded verification problems based on a WSAT algorithm. The size of the verification problems is very large, and most of the data used in the algorithm have to be placed in off-chip DRAMs. The performance of the solver is limited by the throughput and access delay of the DRAMs, not by the parallelism in FPGA. We show how much speed-up is possible under this situation using the memory throughput, memory access delay and the operational frequency of FPGA as parameters.},
author = {Kanazawa, Kenji and Maruyama, Tsutomu},
doi = {10.1109/FPL.2011.18},
file = {:home/vladimir/Documents/Mendeley Desktop/Kanazawa, Maruyama/Proceedings - 21st International Conference on Field Programmable Logic and Applications, FPL 2011/Kanazawa, Maruyama - 2011 - An FPGA solver for SAT-encoded formal verification problems.pdf:pdf},
isbn = {9780769545295},
journal = {Proceedings - 21st International Conference on Field Programmable Logic and Applications, FPL 2011},
keywords = {FPGA,Formal verification,SAT},
mendeley-groups = {Reducts},
pages = {38--43},
title = {{An FPGA solver for SAT-encoded formal verification problems}},
year = {2011}
}
@inproceedings{Kopczynski14,
author = {Kopczynski, Maciej and Grze, Tomasz and Stepaniuk, Jaroslaw},
booktitle = {International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)},
doi = {10.1109/WI-IAT.2014.120},
file = {:home/bill/Documents/Mendeley Desktop/2014/Kopczynski - 2014 - FPGA in Rough-Granular Computing Reduct Generation.pdf:pdf},
isbn = {9781479941438},
mendeley-groups = {Reducts},
pages = {364--370},
title = {{FPGA in Rough-Granular Computing: Reduct Generation}},
year = {2014}
}


@article{Lazo15,
author = {Lazo-Cort\'{e}s, Manuel S and Mart\'{\i}nez-Trinidad, Jos\'{e} Fco and Carrasco-Ochoa, Jes\'{u}s A and Sanchez-Diaz, Guillermo},
file = {:home/vladimir/Documents/Mendeley Desktop/Lazo-Cort\'{e}s et al/Information Sciences/Lazo-Cort\'{e}s et al. - 2015 - On the relation between rough set reducts and typical testors.pdf:pdf},
journal = {Information Sciences},
pages = {152--163},
publisher = {Elsevier},
title = {{On the relation between rough set reducts and typical testors}},
volume = {294},
year = {2015}
}
@article{Lazo01,
abstract = {In this paper, the historical evolution of the concept of testor is presented. Testors in a bivalued logic, in a k-valued logic, and also in a fuzzy logic are considered, particular considerations about each case are expressed. This concept evolution is presented to English readers for the first time. The authors reviewed this history because testor, in each particular formulation, is an interesting tool for feature selection problems, especially when the descriptions of objects are non-classical. ?? 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Lazo-Cort\'{e}s, Manuel and Ruiz-Shulcloper, Jos\'{e} and Alba-Cabrera, Eduardo},
doi = {10.1016/S0031-3203(00)00028-5},
file = {:home/bill/Documents/Mendeley Desktop/2001/lazo01.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Discriminating features,Feature combination,Feature selection,Testor},
mendeley-groups = {Reducts},
number = {4},
pages = {753--762},
title = {{An overview of the evolution of the concept of testor}},
volume = {34},
year = {2001}
}
@inproceedings{Li2015,
author = {Li, Geng and Zaki, Mohammed J.},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-015-0409-y},
file = {:home/vladimir/Documents/Mendeley Desktop/Li, Zaki/Data Mining and Knowledge Discovery/Li, Zaki - 2015 - Sampling frequent and minimal boolean patterns theory and application in classification.pdf:pdf},
issn = {1384-5810},
keywords = {classification,disjunctive patterns,expressions,frequent pattern mining,markov chain,minimal boolean,minimal generators,pattern sampling},
mendeley-groups = {Reducts},
pages = {1--45},
title = {{Sampling frequent and minimal boolean patterns: theory and application in classification}},
url = {http://link.springer.com/10.1007/s10618-015-0409-y},
year = {2015}
}
@article{Liang2014,
author = {Liang, Jiye and Wang, Feng and Dang, Chuangyin and Qian, Yuhua},
doi = {10.1109/TKDE.2012.146},
file = {:home/vladimir/Documents/Mendeley Desktop/Liang et al/Unknown/Liang et al. - 2014 - A Group Incremental Approach to Feature Selection Applying Rough Set Technique.pdf:pdf},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Dynamic data sets,feature selection,incremental algorithm,rough set theory},
mendeley-groups = {Reducts},
number = {2},
pages = {294--308},
title = {{A group incremental approach to feature selection applying rough set technique}},
volume = {26},
year = {2014}
}
@inproceedings{Lias09,
author = {Lias-Rodr\'{\i}guez, Alexsey and Pons-Porrata, Aurora},
booktitle = {CIARP 2009},
doi = {10.1007/978-3-642-10268-4\_50},
file = {:home/bill/Documents/Mendeley Desktop/2009/Lias-Rodr\'{\i}guez, Pons-Porrata - 2009 - BR A new method for computing all typical testors.pdf:pdf},
isbn = {3642102670},
issn = {03029743},
keywords = {Feature selection,Typical testors},
mendeley-groups = {Reducts},
pages = {433--440},
title = {{BR: A new method for computing all typical testors}},
volume = {5856},
year = {2009}
}

@article{Lias13,
author = {Lias-Rodr\'{\i}guez, Alexsey and Sanchez-Diaz, Guillermo},
doi = {10.1142/S0218001413500225},
file = {:home/bill/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lias-Rodr\'{\i}guez, Sanchez-Diaz - 2013 - an Algorithm for Computing Typical Testors Based on Elimination of Gaps and Reduction of Columns.pdf:pdf},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
mendeley-groups = {Reducts},
number = {08},
pages = {1350022},
title = {{An Algorithm for Computing Typical Testors Based on Elimination of Gaps and Reduction of Columns}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001413500225},
volume = {27},
year = {2013}
}
@inproceedings{Lin04,
abstract = {It is well known that finding the shortest reduct is NP hard. In this paper, we present an heuristic fast way of finding shortest rel- ative reducts by exploring the specific nature of the input data. As a byproduct, we can show that the shortest relative reducts can be found in polynomial time, provided that we do know apriori that the lengths of the shortest reducts is bounded by a constant, that is, independent of the column size n. It should be noted that there are heuristic factors in the algorithm (“speeds” are not guaranteed) but the results, namely, the founded shortest reducts, are the precise answers.},
author = {Lin, Tsau Young and Yin, Ping},
booktitle = {Rough Sets and Current Trends in Computing},
file = {:home/vladimir/Documents/Mendeley Desktop/Lin, Yin/Rough Sets and Current Trends in Computing/Lin, Yin - 2004 - Heuristically Fast Finding of the Shortest Reducts.pdf:pdf},
issn = {03029743},
keywords = {bitmap,data mining,granular computing,reduct,rough set},
mendeley-groups = {Reducts},
pages = {465--470},
publisher = {Springer Berlin Heidelberg},
title = {{Heuristically Fast Finding of the Shortest Reducts}},
year = {2004}
}
@article{Luan2015,
author = {Luan, Xin-Yuan and Li, Zhan-Pei and Liu, Ting-Zhang},
doi = {10.1016/j.neucom.2015.06.090},
file = {:home/vladimir/Documents/Mendeley Desktop/Luan, Li, Liu/Neurocomputing/Luan, Li, Liu - 2015 - Neurocomputing A novel attribute reduction algorithm based on rough set and improved arti fi cial fi sh swarm alg.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {AFSA,Attribute reduction,Cauchy distribution,Rough set},
mendeley-groups = {Reducts},
pages = {1--8},
publisher = {Elsevier},
title = {{A novel attribute reduction algorithm based on rough set and improved artificial fish swarm algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215012515},
year = {2015}
}
@article{Martinez01,
author = {Mart\'{\i}nez, Jos\'{e} Francisco and Guzm\'{a}n, Adolfo},
file = {:home/vladimir/Documents/Mendeley Desktop/Mart\'{\i}nez-Trinidad, Guzm\'{a}n-Arenas/Pattern Recognition/Mart\'{\i}nez-Trinidad, Guzm\'{a}n-Arenas - 2001 - The logical combinatorial approach to pattern recognition, an overview through selected work.pdf:pdf},
journal = {Pattern Recognition},
mendeley-groups = {Reducts},
number = {4},
pages = {741--751},
publisher = {Elsevier},
title = {{The logical combinatorial approach to pattern recognition, an overview through selected works}},
volume = {34},
year = {2001}
}
@inproceedings{Mukamakuza2014,
author = {Mukamakuza, Carine Pierrette and Wang, Jiayang},
booktitle = {Natural Computation (ICNC), 2014 10th International Conference on},
doi = {10.1109/ICNC.2014.6975882},
file = {:home/bill/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mukamakuza, Wang - 2014 - Dynamic Reducts Computation Analysis Based on Rough Sets.pdf:pdf},
isbn = {9781479951512},
keywords = {-component,dynamic reduct,information system,knowlegde discovery,optimum reduct},
mendeley-groups = {Reducts},
pages = {480--485},
publisher = {IEEE},
title = {{Dynamic Reducts Computation Analysis Based on Rough Sets}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=6975882\&isnumber=6975799},
year = {2014}
}
@article{Nguyen97,
author = {Nguyen, Hung Son and Skowron, Andrzej},
file = {:home/vladimir/Documents/Mendeley Desktop/Nguyen, Skowron/Foundations of Intelligent Systems/Nguyen, Skowron - 1997 - Boolean Reasoning for Feature Extraction Problems.pdf:pdf},
journal = {Foundations of Intelligent Systems},
mendeley-groups = {Reducts},
pages = {117--126},
title = {{Boolean Reasoning for Feature Extraction Problems}},
year = {1997}
}
@article{Ohrn00,
abstract = {This thesis examines how discernibility-based methods can be equipped to posses several qualities that are needed for analyzing tabular medical data, and how these models can be evaluated according to current standard measures used in the health sciences. To this end, tools have been developed that make this possible, and some novel medical applications have been devised in which the tools are put to use. Rough set theory provides a framework in which discernibility-based methods can be formulated and interpreted, and also forms an appealing foundation for data mining and knowledge discovery. When the medical domain is targeted, several factors be-come important. This thesis examines some of these factors, and holds them up to the current state-of-the-art in discernibility-based empirical modelling. Bringing together pertinent techniques, suitable adaptations of relevant theory for model construction and assessment are presented. Rough set classifiers are brought together with ROC analysis, and it is outlined how attribute costs and semantics can enter the modelling process. ROSETTA, a comprehensive software system for conducting data analyses within the framework of rough set theory, has been developed. Under the hypothesis that the ac-cessibility of such tools lowers the threshold for abstract ideas to migrate into concrete realization, this aids in reducing a gap between theoreticians and practitioners, and enables existing problems to be more easily attacked. The ROSETTA system boasts a set of flexible and powerful algorithms, and sets these in a user-friendly environment designed to support all phases of the discernibility-based modelling methodology. Re-searchers world-wide have already put the system to use in a wide variety of domains. By and large, discernibility-based data analysis can be varied along two main axes: Which objects in the universe of discourse that we deem it necessary to discern be-tween, and how we define that discernibility among these objects is allowed to take place. Using ROSETTA, this thesis has explored various facets of this also in three novel and distinctly different medical applications: A method is proposed for identifying population subgroups for which expen-sive tests may be avoided, and experiments with a real-world database on a cardiological prognostic problem suggest that significant savings are possible. A method is proposed for anonymizing medical databases with sensitive con-tents via cell suppression, thus aiding to preserve patient confidentiality. Very simple rule-based classifiers are employed to diagnose acute appendicitis, and their relative performance is compared to a team of experienced surgeons. The added value of certain biochemical tests is also demonstrated},
author = {\O hrn, a},
doi = {citeulike-article-id:7989427},
file = {:home/vladimir/Documents/Mendeley Desktop/\O hrn/Unknown/\O hrn - 2000 - Discernibility and rough sets in medicine tools and applications.pdf:pdf},
isbn = {8279840141},
mendeley-groups = {Reducts},
pages = {223},
title = {{Discernibility and rough sets in medicine: tools and applications}},
url = {http://ntnu.diva-portal.org/smash/record.jsf?pid=diva2:125243},
year = {2000}
}
@article{Parthalain08,
abstract = {Dataset dimensionality is undoubtedly the single most significant obstacle which exasperates any attempt to apply effective computational intelligence techniques to problem domains. In order to address this problem a technique which reduces dimensionality is employed prior to the application of any classification learning. Such feature selection (FS) techniques attempt to select a subset of the original features of a dataset which are rich in the most useful information. The benefits can include improved data visualisation and transparency, a reduction in training and utilisation times and potentially, improved prediction performance. Methods based on fuzzy-rough set theory have demonstrated this with much success. Such methods have employed the dependency function which is based on the information contained in the lower approximation as an evaluation step in the FS process. This paper presents three novel feature selection techniques employing fuzzy entropy to locate fuzzy-rough reducts. This approach is compared with two other fuzzy-rough feature selection approaches which utilise other measures for the selection of subsets.},
author = {Parthal\'{a}in, Neil Mac and Jensen, Richard and Shen, Qiang},
doi = {10.1109/FUZZY.2008.4630537},
file = {:home/vladimir/Documents/Mendeley Desktop/Parthal\'{a}in, Jensen, Shen/IEEE International Conference on Fuzzy Systems/Parthal\'{a}in, Jensen, Shen - 2008 - Finding fuzzy-rough reducts with fuzzy entropy.pdf:pdf},
isbn = {9781424418190},
issn = {10987584},
journal = {IEEE International Conference on Fuzzy Systems},
mendeley-groups = {Reducts},
pages = {1282--1288},
title = {{Finding fuzzy-rough reducts with fuzzy entropy}},
year = {2008}
}
@book{Pawlak81,
author = {Pawlak, Zdislaw},
mendeley-groups = {Reducts},
publisher = {Polish Academy of Sciences [PAS]. Institute of Computer Science},
title = {{Classification of objects by means of attributes}},
year = {1981}
}
@book{Pawlak81-2,
author = {Pawlak, Zdislaw},
mendeley-groups = {Reducts},
publisher = {Polish Academy of Sciences [PAS]. Institute of Computer Science},
title = {{Rough relations}},
year = {1981}
}
@article{Pawlak82,
author = {Pawlak, Z},
doi = {10.1007/978-94-011-3534-4},
file = {:home/bill/Documents/Mendeley Desktop/1982/roughsetsrep29.pdf:pdf},
isbn = {978-94-010-5564-2},
issn = {1558-0032},
journal = {International Journal of Computer and Information Sciences},
mendeley-groups = {Reducts},
pages = {1--51},
pmid = {19304490},
title = {{Rough sets}},
url = {http://link.springer.com/article/10.1007/BF01001956},
year = {1982}
}
@book{Pawlak91,
author = {Pawlak, Zdzislaw},
mendeley-groups = {Reducts},
publisher = {Springer Science \& Business Media},
title = {{Rough sets: Theoretical aspects of reasoning about data}},
volume = {9},
year = {1991}
}
@article{Pawlak07,
abstract = {In this article, we discuss methods based on the combination of rough sets and Boolean reasoning with applications in pattern recognition, machine learning, data mining and conflict analysis. © 2006 Elsevier Inc. All rights reserved.},
author = {Pawlak, Zdzisław and Skowron, Andrzej},
doi = {10.1016/j.ins.2006.06.007},
file = {:home/vladimir/Documents/Mendeley Desktop/Pawlak, Skowron/Information Sciences/Pawlak, Skowron - 2007 - Rough sets and Boolean reasoning.pdf:pdf},
isbn = {00200255},
issn = {00200255},
journal = {Information Sciences},
keywords = {(In)discernibility,Approximate Boolean reasoning,Association rules,Boolean reasoning,Classifiers,Conflict analysis,Decision rules,Discretization,Reducts,Rough sets,Symbolic value grouping},
mendeley-groups = {Reducts},
number = {1},
pages = {41--73},
title = {{Rough sets and Boolean reasoning}},
volume = {177},
year = {2007}
}
@article{Piza14,
author = {Piza-Davila, Ivan and Sanchez-Diaz, Guillermo and Aguirre-Salado, Carlos A. and Lazo-Cortes, Manuel S.},
doi = {10.1007/s10489-014-0606-1},
file = {:home/bill/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Piza-Davila et al. - 2014 - A parallel hill-climbing algorithm to generate a subset of irreducible testors.pdf:pdf},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {binary trees,feature selection,hill-climbing,irreducible,pattern recognition,testors},
mendeley-groups = {Reducts},
number = {4},
pages = {622--641},
title = {{A parallel hill-climbing algorithm to generate a subset of irreducible testors}},
url = {http://link.springer.com/10.1007/s10489-014-0606-1},
volume = {42},
year = {2014}
}

@book{Polkowski00,
  title={Rough Set Methods and Applications: New Developments in Knowledge Discovery in Information Systems; with 133 Tables;[Adam Mrozek, 1949-1999; this Volume is Dedicated to the Memory of Professor Adam Mr{\'o}zek]},
  author={Polkowski, Lech and Tsumoto, Shusaku and Lin, Tsau Y},
  volume={56},
  year={2000},
  publisher={Springer Science \& Business Media}
}
@inproceedings{Rodriguez14,
abstract = {Feature selection in pattern recognition is a prob- lem whose space complexity grows exponentially regarding the number of attributes in a dataset. There are several hardware implementations of algorithms for overcoming this complexity. These hardware architectures relay on a software component for filtering irreducible features subsets, which is a computationally complex task. In this paper, a new hardware module for the filtering process is presented. The main advantage of this new architecture is that no additional time is required for hardware execution whilst the software component is no longer needed. Experimental results show that the runtime magnitude order for software is the same as for hardware in some cases. The proposed architecture is algorithm independent and may lead to smaller hardware realizations than previous architectures.},
author = {Rodr\'iguez, Vlad\'imir and Mart\'inez, Jos\'e F and Carrasco, Jesus A and Lazo, Manuel S and Cumplido, Ren\'e and {Feregrino Uribe}, Claudia},
booktitle = {ReConFigurable Computing and FPGAs (ReConFig), 2014 International Conference on},
doi = {10.1109/ReConFig.2014.7032526},
file = {:home/vladimir/Documents/Mendeley Desktop/Rodriguez et al/ReConFigurable Computing and FPGAs (ReConFig), 2014 International Conference on/Rodriguez et al. - 2014 - A hardware architecture for filtering irreducible testors.pdf:pdf},
mendeley-groups = {Reducts},
organization = {IEEE},
pages = {1--4},
title = {{A hardware architecture for filtering irreducible testors}},
url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=7032526},
year = {2014}
}
@inproceedings{Rojas07,
author = {Rojas, A and Cumplido, R},
booktitle = {IDEAL 2007},
file = {:home/bill/Documents/Mendeley Desktop/2007/Rojas, Cumplido - 2007 - FPGA-based architecture for computing testors.pdf:pdf},
mendeley-groups = {Reducts},
pages = {188--197},
title = {{FPGA-based architecture for computing testors}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-77226-2\_20},
year = {2007}
}

@article{Rojas12,
abstract = {In pattern recognition, feature selection is a very important task for supervised classification. The problem consists in, given a dataset where each object is described by a set of features, finding a subset of the original features such that a classifier that runs on data containing only these features would reach high classification accuracy. A useful way to find this subset of the original features is through testor theory. A testor is defined as a subset of the original features that allows differentiating objects from different classes. Testors are very useful particularly when object descriptions contain both numeric and non-numeric features. Computing testors for feature selection is a very complex problem due to exponential complexity, with respect to the number of features, of algorithms based on testor theory. Hardware implementation of testor computing algorithms helps to improve their performance taking advantage of parallel processing for verifying if a feature subset is a testor in a single clock cycle. This paper introduces an efficient hardware-software platform for computing irreducible testors for feature selection in pattern recognition. Results of implementing the proposed platform using a FPGA-based prototyping board are presented and discussed. © 2011 Elsevier Ltd. All rights reserved.},
author = {Rojas, Alejandro and Cumplido, Ren\'{e} and {Ariel Carrasco-Ochoa}, J. and Feregrino, Claudia and {Francisco Mart\'{\i}nez-Trinidad}, J.},
doi = {10.1016/j.eswa.2011.07.004},
file = {:home/vladimir/Documents/Mendeley Desktop/Rojas et al/Expert Systems with Applications/Rojas et al. - 2012 - Hardware-software platform for computing irreducible testors.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Custom architectures,FPGAs,Feature selection,Testor theory},
mendeley-groups = {Reducts},
number = {2},
pages = {2203--2210},
publisher = {Elsevier Ltd},
title = {{Hardware-software platform for computing irreducible testors}},
url = {http://dx.doi.org/10.1016/j.eswa.2011.07.004},
volume = {39},
year = {2012}
}
@article{Ruiz85,
author = {Ruiz-Shulcloper, Jos\'{e} and Aguila, L. and Bravo, A.},
journal = {Revista Ciencias Matem\'{a}ticas},
mendeley-groups = {Reducts},
pages = {11--18},
title = {{BT and TB algorithms for computing all irreducible testors.}},
volume = {2},
year = {1985}
}
@article{Ruiz08,
author = {Ruiz-Shulcloper, Jos\'{e}},
file = {:home/vladimir/Documents/Mendeley Desktop/Ruiz-Shulcloper/Pattern Recognition and Image Analysis/Ruiz-Shulcloper - 2008 - Pattern recognition with mixed and incomplete data.pdf:pdf},
journal = {Pattern Recognition and Image Analysis},
mendeley-groups = {Reducts},
number = {4},
pages = {563--576},
publisher = {Springer},
title = {{Pattern recognition with mixed and incomplete data}},
volume = {18},
year = {2008}
}
@book{Shulcloper95a,
address = {M\'{e}xico},
author = {{Ruiz-Shulcloper, J., Alba-Cabrera, E., Lazo-Cort\'{e}s}, M.},
publisher = {CINVESTAV-IPN},
mendeley-groups = {Reducts},
title = {{Introducci\'{o}n al Reconocimiento de Patrones (Enfoque L\'{o}gico-Combinatorio). Serie Verde No. 51.}},
year = {1995}
}
@book{Shulcloper95b,
address = {M\'{e}xico},
author = {{Ruiz-Shulcloper, J., Alba-Cabrera, E., Lazo-Cort\'{e}s}, M.},
mendeley-groups = {Reducts},
publisher = {CINVESTAV-IPN},
title = {{Introducci\'{o}n a la teor\'{i}a de Testotes T\'{i}picos. Serie Verde No. 50.}},
year = {1995}
}
@article{Safar07,
abstract = {The Boolean satisfiability problem (SAT) is a central problem in artificial intelligence, mathematical logic and computing theory with wide range of practical applications. Being an NP-complete problem, the used SAT's solving algorithm execution time influences the performance of SAT-based applications. FPGAs represent a promising technology for accelerating SAT solvers. In this paper, we present an FPGA-based SAT solver based on depth-first search. Our architecture exploits the fine granularity and massive parallelism of FPGAs to evaluate the SAT formula and perform conflict diagnosis. Conflict diagnosis helps pruning the search space by allowing nonchronological conflict directed backtracking. Our architecture modularity enables reflecting a specific SAT instance through memory initialization reducing hardware compilation overhead. The gain in performance is validated through DIMACS benchmarks suite},
author = {Safar, Mona and El-Kharashi, M. Watheq and Salem, Ashraf},
doi = {10.1109/CCECE.2006.277452},
file = {:home/vladimir/Documents/Mendeley Desktop/Safar, El-Kharashi, Salem/Canadian Conference on Electrical and Computer Engineering/Safar, El-Kharashi, Salem - 2007 - FPGA-based SAT solver.pdf:pdf},
isbn = {1424400384},
issn = {08407789},
journal = {Canadian Conference on Electrical and Computer Engineering},
keywords = {Boolean satisfiability,Conflict diagnosis,Hardware acceleration},
mendeley-groups = {Reducts},
number = {May},
pages = {1901--1904},
title = {{FPGA-based SAT solver}},
year = {2007}
}
@inproceedings{Sanchez99,
author = {Sanchez-D\'iaz, G and Lazo-Cort\'es, M and Fuentes-Ch\'avez, O},
booktitle = {Proc. of the Iberoamerican Symposium on Pattern Recognition (SIARP 1999)},
mendeley-groups = {Reducts},
pages = {207--213},
title = {{Genetic algorithm for calculating typical testors of minimal cost}},
year = {1999}
}
@incollection{Sanchez07,
author = {Sanchez, Guillermo and Lazo, Manuel},
booktitle = {Progress in Pattern Recognition, Image Analysis and Applications},
file = {:home/vladimir/Documents/Mendeley Desktop/Sanchez-D\'{\i}az, Lazo-Cort\'{e}s/Progress in Pattern Recognition, Image Analysis and Applications/Sanchez-D\'{\i}az, Lazo-Cort\'{e}s - 2007 - CT-EXT an algorithm for computing typical testor set.pdf:pdf},
mendeley-groups = {Reducts},
pages = {506--514},
publisher = {Springer},
title = {{CT-EXT: an algorithm for computing typical testor set}},
year = {2007}
}
@inproceedings{Sanchez10,
abstract = {Typical testors are a useful tool for both feature selection and for determining feature relevance in supervised classification problems. Nowadays, generating all typical testors of a training matrix is computationally expensive; all reported algorithms have exponential complexity, depending mainly on the number of columns in the training matrix. For this reason, different approaches such as sequential and parallel algorithms, genetic algorithms and hardware implementations techniques have been developed. In this paper, we introduce a fast implementation of the algorithm CT-EXT (which is one of the fastest algorithms reported) based on an accumulative binary tuple, developed for generating all typical testors of a training matrix. The accumulative binary tuple implemented in the CT-EXT algorithm, is a useful way to simplifies the search of feature combinations which fulfill the testor property, because its implementation decreases the number of operations involved in the process of generating all typical testors. In addition, experimental results using the proposed fast implementation of the CT-EXT algorithm and the comparison with other state of the art algorithms that generated typical testors are presented. © 2010 Springer-Verlag.},
author = {S\'{a}nchez-Diaz, Guillermo and Piza-Davila, Ivan and Lazo-Cort\'{e}s, Manuel and Mora-Gonz\'{a}lez, Miguel and Salinas-Luna, Javier},
booktitle = {LNAI 2010},
doi = {10.1007/978-3-642-16773-7\_8},
file = {:home/bill/Documents/Mendeley Desktop/2010/Sanchez-Diaz et al. - 2010 - A fast implementation of the CT-EXT algorithm for the testor property identification.pdf:pdf},
isbn = {3642167721},
issn = {03029743},
keywords = {feature selection,pattern recognition,typical testors},
mendeley-groups = {Reducts},
number = {PART 2},
pages = {92--103},
title = {{A fast implementation of the CT-EXT algorithm for the testor property identification}},
volume = {6438},
year = {2010}
}
@article{Santiesteban03,
author = {Santiesteban, Y and Pons, A},
file = {:home/vladimir/Documents/Mendeley Desktop/Santiesteban, Pons-Porrata/Mathematics Sciences Journal/Santiesteban, Pons-Porrata - 2003 - LEX a new algorithm for the calculus of typical testors.pdf:pdf},
journal = {Mathematics Sciences Journal},
mendeley-groups = {Reducts},
number = {1},
pages = {85--95},
title = {{LEX: a new algorithm for the calculus of typical testors}},
volume = {21},
year = {2003}
}
@incollection{Skowron92,
author = {Skowron, Andrzej and Rauszer, Cecylia},
booktitle = {Intelligent Decision Support},
mendeley-groups = {Reducts},
pages = {331--362},
publisher = {Springer},
title = {{The discernibility matrices and functions in information systems}},
year = {1992}
}
@article{Starzyk99,
author = {Starzyk, Janusz and Nelson, D E and Sturtz, Kirk},
file = {:home/vladimir/Documents/Mendeley Desktop/Starzyk, Nelson, Sturtz/Bulletin of international rough set society/Starzyk, Nelson, Sturtz - 2007 - Reduct generation in information systems.pdf:pdf},
journal = {Bulletin of international rough set society},
mendeley-groups = {Reducts},
number = {May},
pages = {19--22},
title = {{Reduct generation in information systems}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Reduct+Generation+in+Information+Systems\#0},
volume = {3},
year = {1999}
}
@article{Starzyk00,
author = {Starzyk, Janusz a. and Nelson, Dale E. and Sturtz, Kirk},
doi = {10.1007/s101150050007},
file = {:home/vladimir/Documents/Mendeley Desktop/Starzyk, Nelson, Sturtz/Knowledge and Information Systems/Starzyk, Nelson, Sturtz - 2000 - A Mathematical Foundation for Improved Reduct Generation in Information Systems.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {data mining,information systems,rough sets},
mendeley-groups = {Reducts},
number = {2},
pages = {131--146},
title = {{A Mathematical Foundation for Improved Reduct Generation in Information Systems}},
volume = {2},
year = {2000}
}
@article{Strakowski08,
abstract = {Calculating reducts is a very important process. Unfortu- nately, the process of computing all reducts in NP-hard. There are a lot of heuristic solutions for computing reducts, but they do not guarantee achieving complete set of reducts. We propose here three versions of an exact algorithm, designed for parallel processing. We present here how to decompose the problem of calculating reducts, so that parallel calcu- lations are efficient.},
author = {Strakowski, Tomasz and Rybiński, Henryk},
file = {:home/vladimir/Documents/Mendeley Desktop/Strakowski, Rybiński/Transactions on Rough Sets IX/Strakowski, Rybiński - 2008 - A New Approach to Distributed Algorithms for Reduct Calculation.pdf:pdf},
journal = {Transactions on Rough Sets IX},
keywords = {distributed,reducts calculations,rough set theory},
mendeley-groups = {Reducts},
number = {3},
pages = {365--378},
title = {{A New Approach to Distributed Algorithms for Reduct Calculation}},
year = {2008}
}
@inproceedings{Tiwari11,
abstract = {In this paper, a rough set processor for robotics applications is proposed. A lot of research work is carried out in implementing the algorithms of rough set theory using various software tools but hardware implementation of them is an area still unexplored. Hardware approach towards this will reduce the computation overhead on the main processor. We have designed and implemented a Binary Discernibility matrix and a Reduct Calculator Block of rough set processor.},
author = {Tiwari, K.S. and Kothari, A.G.},
booktitle = {2011 International Conference on Computational Intelligence and Communication Networks},
doi = {10.1109/CICN.2011.42},
file = {:home/bill/Documents/Mendeley Desktop/2011/Tiwari, Kothari - 2011 - Architecture and Implementation of Attribute Reduction Algorithm Using Binary Discernibility Matrix.pdf:pdf},
isbn = {978-1-4577-2033-8},
keywords = {Discernibility MATRIX,FPGA,Reduct,Rough set,VHDL},
mendeley-groups = {Reducts},
pages = {212--216},
title = {{Architecture and Implementation of Attribute Reduction Algorithm Using Binary Discernibility Matrix}},
year = {2011}
}

@article{Tiwari12,
author = {Tiwari, KS and Kothari, AG and Keskar, AG},
file = {:home/bill/Documents/Mendeley Desktop/2012/Tiwari, Kothari, Keskar - 2012 - Reduct generation from binary discernibility matrix an hardware approach.pdf:pdf},
journal = {International Journal of Future Computer and Communication},
mendeley-groups = {Reducts},
number = {3},
pages = {270--272},
title = {{Reduct generation from binary discernibility matrix: an hardware approach}},
url = {http://www.ijfcc.org/papers/72-V006.pdf},
volume = {1},
year = {2012}
}
@article{Tiwari13,
author = {Tiwari, Kanchan and Kothari, Ashwin and Shah, Riddhi},
file = {:home/vladimir/Documents/Mendeley Desktop/Tiwari, Kothari, Shah/irdindia.in/Tiwari, Kothari, Shah - Unknown - FPGA Implementation of a Reduct Generation Algorithm based on Rough Set Theory.pdf:pdf},
journal = {International Journal of Advanced Electrical and Electronics Engineering (IJAEEE)},
number = {6},
title = {{FPGA Implementation of a Reduct Generation Algorithm based on Rough Set Theory}},
volume = {2},
year = {2013}
}
@article{Tiwari14,
author = {Tiwari, Kanchan Shailendra},
file = {:home/vladimir/Documents/Mendeley Desktop/Tiwari/Unknown/Tiwari - 2014 - Design and Implementation of Rough Set Algorithms on FPGA A Survey.pdf:pdf},
keywords = {classification,core,discernibility matrix,fpga,reduct,rough set theory},
mendeley-groups = {Reducts},
number = {9},
pages = {14--23},
title = {{Design and Implementation of Rough Set Algorithms on FPGA : A Survey}},
volume = {3},
year = {2014}
}
@article{Torres2014,
author = {Torres, Mar\'{\i}a Dolores and Torres, Aurora and Cuellar, Felipe and Torres, Mar\'{\i}a De La Luz and Le\'{o}n, Eunice Ponce De and Pinales, Francisco},
doi = {10.1016/j.eswa.2013.08.013},
file = {:home/bill/Documents/Mendeley Desktop/2014/7\_- 2013 Evolutionary computation in the identification of risk factors.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
mendeley-groups = {Reducts},
number = {3},
pages = {831--840},
title = {{Evolutionary computation in the identification of risk factors. Case of TRALI}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417413006258},
volume = {41},
year = {2014}
}
@article{Wang01,
author = {Wang, Jue and Wang, Ju},
file = {:home/vladimir/Documents/Mendeley Desktop/Wang, Wang/Journal of computer science and technology/Wang, Wang - 2001 - Reduction algorithms based on discernibility matrix the ordered attributes method.pdf:pdf},
journal = {Journal of computer science and technology},
mendeley-groups = {Reducts},
number = {6},
pages = {489--504},
publisher = {Springer},
title = {{Reduction algorithms based on discernibility matrix: the ordered attributes method}},
volume = {16},
year = {2001}
}
@article{WangP07,
abstract = {Rough set theory is used to represent, analyze, and manipulate knowledge in infor- mation or decision tables. To remove superfluous attributes without changing the origi- nal knowledge, reduction is must in rough set. This paper introduces the pseudo decision table to replace the original table and two algorithms, RGonCRS and SRGonCRS, based on the current rules size, CRS, are presented to generate all reducts which ensure the lower approximation for each instance in the table with a minimal number of attributes. RGonCRS finds reducts by merging candidate attributes and SRGonCRS is a scalable version of RGonCRS which generates reducts for very large tables. Propositions and proofs are presented in this paper. Empirical tests are shown for RGonCRS using simu- lated information tables and UCI benchmark datasets and a preliminary test is generated for SRGonCRS. Results are compared to the well-known rough set software – Rough Set Exploration System (RSES).},
author = {Wang, Pai-Chou},
file = {:home/vladimir/Documents/Mendeley Desktop/Wang/Journal of Information Science and Engineering/Wang - 2007 - Highly Scalable Rough Set Reducts Generation.pdf:pdf},
journal = {Journal of Information Science and Engineering},
keywords = {information reduction,knowledge reduction,reducts generation,rough set theory,scalable reducts generation},
mendeley-groups = {Reducts},
number = {23},
pages = {1281--1298},
title = {{Highly Scalable Rough Set Reducts Generation}},
volume = {4},
year = {2007}
}
@article{Wang07,
author = {Wang, Xiangyang and Yang, Jie and Teng, Xiaolong and Xia, Weijun and Jensen, Richard},
file = {:home/vladimir/Documents/Mendeley Desktop/Wang et al/Pattern Recognition Letters/Wang et al. - 2007 - Feature selection based on rough sets and particle swarm optimization.pdf:pdf},
journal = {Pattern Recognition Letters},
mendeley-groups = {Reducts},
number = {4},
pages = {459--471},
publisher = {Elsevier},
title = {{Feature selection based on rough sets and particle swarm optimization}},
volume = {28},
year = {2007}
}
@inproceedings{Wroblewski95,
author = {Wroblewski, Jakub},
booktitle = {Proccedings of the second annual join conference on infromation science},
mendeley-groups = {Reducts},
pages = {186--189},
title = {{Finding minimal reducts using genetic algorithms}},
year = {1995}
}
@inproceedings{Wroblewski98,
abstract = {A rough set based knowledge discovery system is presented. The sys- tem is based on the decomposition of large data tables into smaller ones in such a way that the approximation of global decision algorithm (related to the whole table) from local ones (related to these smaller tables) can be obtained. The set of these smaller tables can be considered separately and the rules can be calculated in parallel. Then, a set of locally generated rules can be collected for achieving the sufficient approximation of the global de- cision algorithm. The so called ”templates” are presented as the efficient generators of the smaller tables. On the other hand, a rule generator for one table can be implemented in parallel too. A system for efficient rule generating based on the notion of reduct is presented. A fast, parallel algorithm for short reduct finding is described. Moreover, since a genetic algorithm is used as a driving force of the reduct generator, it can be implemented in parallel in natural way. A parallel system of reduct finding is described and discussed. Experi- ment results on large data tables are presented.},
author = {Wroblewski, Jakub},
booktitle = {Proc PARELEC},
file = {:home/vladimir/Documents/Mendeley Desktop/Wroblewski/Proc PARELEC/Wroblewski - 1998 - A parallel algorithm for knowledge discovery system.pdf:pdf},
mendeley-groups = {Reducts},
pages = {228--230},
title = {{A parallel algorithm for knowledge discovery system}},
year = {1998}
}
@inproceedings{Xu2013,
abstract = {Cost-sensitive learning is both hot and difficult in data mining and machine learning applications. Some research considers only one type of cost. Others convert two or more types of cost into the same unit, and then deal with a single-objective optimization problem. However, in many cases different types of cost cannot be converted. In this paper, we define and tackle multi-objective attribute reduct problem with multiple types of test cost. First, we compute all reducts of a decision system. Then, we separately calculate the money cost and time cost of these reducts and compare them according to the two kinds of test cost. Finally, the worse ones are removed. The remaining reducts form a Pareto optimal solution set. We tested our algorithm with three representative cost distributions on four UCI datasets. Experimental results indicate that a Pareto optimal solution set is usually very small compared with the size of all reducts. Hence our approach is effective in filtering out worse solutions and helping users in scheme selection.},
author = {Xu, Bingxin and Chen, Huiping and Zhu, William and Zhu, Xiaozhong},
booktitle = {Proceedings of the 2013 Joint IFSA World Congress and NAFIPS Annual Meeting, IFSA/NAFIPS 2013},
doi = {10.1109/IFSA-NAFIPS.2013.6608602},
file = {:home/bill/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2013 - Multi-objective cost-sensitive attribute reduction.pdf:pdf},
isbn = {9781479903474},
keywords = {Cost-sensitive learning,attribute reduction,money cost,rough sets,time cost},
mendeley-groups = {Reducts},
pages = {1377--1381},
title = {{Multi-objective cost-sensitive attribute reduction}},
year = {2013}
}
@article{Yang08,
abstract = {Attribute reduction is a very important part of rough set theory. However, the cost of reduct computation is highly influenced by the size of object set and attribute set, so enhancing the reduct computation efficiency is one of the major problems. This paper proposes a reduction algorithm capable of reducing the time cost by using the binary discernibility matrix. Based on analyzing the binary discernibility matrix, the concept of attribute significance is defined at two aspects, which is applied to lead the solutions to optimization. The validity and efficiency of the proposed algorithm is verified by comparing with other two reduction algorithms on various standard domains.},
author = {Yang, Ping Yang Ping and Li, Jisheng Li Jisheng and Huang, Yongxuan Huang Yongxuan},
doi = {10.1109/FSKD.2008.355},
file = {:home/vladimir/Documents/Mendeley Desktop/Yang, Li, Huang/2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery/Yang, Li, Huang - 2008 - An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility Matrix.pdf:pdf},
isbn = {978-0-7695-3305-6},
journal = {2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery},
keywords = {Rough Set Theory,attribute reduction,binary discernibility matrix},
mendeley-groups = {Reducts},
title = {{An Attribute Reduction Algorithm by Rough Set Based on Binary Discernibility Matrix}},
volume = {2},
year = {2008}
}
@article{Yao09,
abstract = {This paper proposes a reduct construction method based on discernibility matrix simplification. The method works in a similar way to the classical Gaussian elimination method for solving a system of linear equations. Elementary matrix simplification operations are introduced. Each operation transforms a matrix into a simpler form. By applying these operations a finite number of times, one can transform a discernibility matrix into one of its minimum (i.e., the simplest) forms. Elements of a minimum discernibility matrix are either the empty set or singleton subsets, in which the union derives a reduct. With respect to an ordering of attributes, which is either computed based on a certain measure of attributes or directly given by a user, two heuristic reduct construction algorithms are presented. One algorithm attempts to exclude unimportant attributes from a reduct, and the other attempts to include important attributes in a reduct. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Yao, Yiyu and Zhao, Yan},
doi = {10.1016/j.ins.2008.11.020},
file = {:home/bill/Documents/Mendeley Desktop/2009/10.1.1.375.3157.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Algorithms,Attribute reduction,Discernibility matrix,Matrix simplification,Rough sets},
mendeley-groups = {Reducts},
number = {7},
pages = {867--882},
title = {{Discernibility matrix simplification for constructing attribute reducts}},
volume = {179},
year = {2009}
}
@article{Zheng14,
author = {Zheng, Kai and Hu, Jie and Zhan, Zhenfei and Ma, Jin and Qi, Jin},
doi = {10.1016/j.eswa.2014.04.042},
file = {:home/bill/Documents/Mendeley Desktop/2014/Zheng et al. - 2014 - An enhancement for heuristic attribute reduction algorithm in rough set.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
mendeley-groups = {Reducts},
number = {15},
pages = {6748--6754},
publisher = {Elsevier Ltd},
title = {{An enhancement for heuristic attribute reduction algorithm in rough set}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417414002656},
volume = {41},
year = {2014}
}
@article{Zhong01,
author = {Zhong, Ning and Dong, Juzhen and Ohsuga, Setsuo},
file = {:home/vladimir/Documents/Mendeley Desktop/Zhong, Dong, Ohsuga/Journal of intelligent information systems/Zhong, Dong, Ohsuga - 2001 - Using rough sets with heuristics for feature selection.pdf:pdf},
journal = {Journal of intelligent information systems},
mendeley-groups = {Reducts},
number = {3},
pages = {199--214},
publisher = {Springer},
title = {{Using rough sets with heuristics for feature selection}},
volume = {16},
year = {2001}
}